[
["index.html", "STAT 100 Honors Project Preface Overview Conventions", " STAT 100 Honors Project Ross Drucker Preface Welcome to the STAT 100 Honors Project! Make sure that you’ve gotten your HCLA approved and submitted to your college. Overview This book is meant to run parallel to the STAT 100 course notes. The hope is that STAT 100 students learn how the concepts they learned are put to use in modern statistical settings. Throughout, students will not only gain familiarity with the concepts in the notebook, but will also gain an understanding on how to use the R programming language to solve every-day problems in the real world. Conventions A few things to note throughout this book: Code that is written in monospace font refers to either RStudio keyboard shortcuts or R code, and it will be syntax-highlighted when appropriate 1 + 1 a = 1 b = 2 a + b sqrt(4) Output will begin with ## on the side Vocab words will appear like this throughout the text Helpful hints (“Pro tips”) will appear in this color and be in italics. They are usually shortcuts to use in R to help make code writing and editing easier to do Things to keep in mind or pay attention to will look like this. They’ll usually begin “Note:” "],
["downloading-r-and-rstudio.html", "Chapter 1 Downloading R and RStudio 1.1 Getting Familiar with RStudio 1.2 Console 1.3 Source 1.4 Environment, History, Build, VCS, Presentation 1.5 Files, Plots, Packages, Help, Viewer", " Chapter 1 Downloading R and RStudio The first thing you’ll need to do in order to begin using R is to actually download R itself. You can do this from this link. Make sure that you install it for the type of operating system you have. That is, if you have a Windows computer, don’t download the Mac or Linux versions. This downloads what’s called a terminal, or the bare minimum interface that you need to write and run code in R. You’re not encouraged to use this terminal when writing code. “Wait, I thought you just said that this is how to write and run my code, and that’s what this project is.” — You (probably) This is because you should use RStudio to write, edit, and run your code. RStudio is what’s called an IDE (Integrated Development Environment), and it relies on what you just downloaded to run in the background to run properly. You can download RStudio here. It makes everything in R – from importing, manipulating, vizualizing, and analyzing data, to writing and debugging code, to creating documents (like this book!) – significantly easier to do. 1.1 Getting Familiar with RStudio Once you’ve got R and RStudio downloaded, open RStudio up and click RStudio in the menu across the top. Click Preferences and then click Pane layout so you can see what each pane in RStudio is. On my computer, I’ve got Source on the top left, Console on the top right, Environment, History, Build, VCS, Presentation on the bottom left, and Files, Plots, Packages, Help, Viewer on the bottom right. To me, this layout is easy to follow and understand, but you can set yours up however you want and can always change it around. Here’s what each of these panels actually does for you: 1.2 Console Remember that terminal you downloaded before downloading RStudio? This is what that looks like. Any time you see a &gt; symbol, you can enter commands. R will then run the command, and if output is produced, it will be displayed here. Do a few simple math problems here to see an example: 1 + 1 2 * 9 sqrt(4) While this may seem like a good place to write your code, it’s a better practice to do it in the Source pane in a script (more on that in approximately 2 sentences). Doing quick computations or installing and loading packages in your console is totally fine, but the more code you run, the harder it will be to find earlier lines of code (and more importantly, the harder it will be to edit should you make a mistake). At the end of the day, all of the code you write, whether it be in the Source pane or the Console pane will run in the Console, but for the purposes of legibility and editing you should do everything in your Source pane. 1.3 Source This pane is where you will do a majority of your work in what’s called a script. Think of a script like a Microsoft Word document, but instead of writing essays, you’re writing code. To open a new script, go to File, then select New File, then New R Script. On a Mac, you can press Cmd + Shift + N, or on a Windows/Linux computer, you can press Ctrl + Shift + N. The beauty of using a script is that you can save what you do and easily edit it later, whereas writing directly into the console will be much more difficult to save, edit, or view later. To run code that you write in a script, highlight the line(s) that you’d like to run and click Run at the top of the script. You can also hold Cmd + Enter (Mac) or Ctrl + Enter (PC). In addition to a script, you can also create a markdown document. Markdown documents allow you to combine plain text, code, and equations. We’ll cover this in more detail later, but these allow you to explain your analysis as you do it. Just to give you an idea of how powerful markdown documents are, this entire book was made using markdown documents. Lastly, this pane will be where the results from the View() command will display, and where you can view the actual contents of the datasets you’re working with. Again, more on this in a little bit. 1.4 Environment, History, Build, VCS, Presentation This pane is where you’ll be able to see all of the datasets that you’ve imported, variables that you’ve created, and functions that you’ve written. They are stored in something called your global environment, or environment for short. Think of your environment as a folder on your computer that keeps track of all of the files you’ve created. While none of that will probably mean much to you at the moment, they will in the span of a few chapters. The more code you write and the more variables you create, the harder it will be to remember all of these components. This pane keeps track of them, as well as basic information that you may find useful later. 1.5 Files, Plots, Packages, Help, Viewer Lastly, the Files, Plots, Packages, Help, Viewer pane will display any plots, graphs, or other images you create in R. Plots will display in this pane under the Plots tab. If they don’t pop up right away, try to increase the size of the pane by dragging the boundaries (like you’d resize your web browser). Markdown documents will be available in the Viewer tab, and help files will be in the Help tab. The Files tab shows the files in the directory (folder on your computer) that you’re working in. We’ll cover these in more detail later, but it’s good to become familiar with this pane as well. "],
["functions-and-data-types.html", "Chapter 2 Functions and Data Types 2.1 Functions 2.2 The Types 2.3 Vectors, Lists, and Data Frames 2.4 Coercing To Other Types 2.5 identical()", " Chapter 2 Functions and Data Types 2.1 Functions A function in R is a set of steps, operations, and procedures that are done to data in a specific order. R has some functions that are built into the language (many of which we’ll go through in this book), but you are also able to write your own functions as well. Functions take arguments, which are data points and other items that the function needs to do its job. Think of them like a variable, where you can change the value that they take each time you need to run the function. These can be anything from data points themselves, to colors and sizes for plots, to even other functions if necessary. After performing the steps and calculations that they’re supposed to do, they return, or give out, the information. Note: in R, arguments of functions may have pre-defined values. In this case, unless you specify differently when you call (use) the function, this pre-defined, or default, value will be used instead. To write your own functions, you need to make use of the function() function. You give your function a name, then specify its arguments as the arguments in function(), and include your steps inside of a set of curly braces ({}). To tell R what to return as the output of your new function, you have two options: you can either just leave it as the last line inside of the curly braces, or you can explicitly state it inside the parenthesis of return(). We’ll put a simple example of a user-defined function here to illustrate how simple and useful writing a function can be, although you may not completely understand what’s going on right now. And that’s okay, and honestly that’s expected at this point. We haven’t covered what’s going on here (it’s only Chapter 2!), but if we don’t introduce functions conceptually, we can’t refer to and/or write and teach them them as we go through the book. They’re helpful tools that can save you a ton of time as you get better in R. The function we’re going to build is called doubler. It will take one argument, x, and return whatever the double of x is. See if you can match the parts in this function to the process we just outlined and with the information about doubler() that we just gave you! doubler = function(x){ 2 * x } Then, to call the function, you simply put the name of the function, then without a space, put a set of parenthesis. Inside of these parenthesis, specify the arguments required to make the function run properly. Here’s an example of how to call the doubler() function we just wrote: doubler(x = 2) [1] 4 doubler(4) [1] 8 doubler(100) [1] 200 As you can see, x can be any number, and doubler() just takes the number (x) and doubles it. Pro tip: you don’t always need to specify the name of an argument. In the second example of using doubler(), R interprets 4 to be what x is supposed to be. When there’s more than one argument needed for a function, you can either give them in the same order the function looks for them (which you’ll learn about here), or you can specify them by name. 2.2 The Types Not all data is of the same type, or usage format. What this means is that different kinds of information from a dataset get evaluated differently in R. To check what type a piece of data is, you can use the class() function. Let’s go through a few of the most common types of data: numeric: Numeric data is data that is only numbers. These can be positive, negative, 0, decimals, or even infinity (\\(\\infty\\)) character: Character data is anything that involves a letter or special character. These will be denoted by '' (single quotation marks) or &quot;&quot; (double quotation marks). Characters are also called strings. It’s important to note that 2 is of type numeric, while '2' is of type character. A quick check using the class() function: class(2) [1] &quot;numeric&quot; class(&#39;2&#39;) [1] &quot;character&quot; logical: Logical data, also known as boolean data, is just a series of TRUE or FALSE values. While this may not necessarily seem like the most useful form of data right now, it’s important to know that this type of data exists. R evaluates T to be TRUE and F to be FALSE, so it’s equally valid to use T and F in place of TRUE and FALSE, but it’s better practice to use TRUE and FALSE since we may want to use T and F as variables. More on this in a little factor: Factor data is simply categories. This type of data is really useful for later when we want to split the information on variables such as gender, location, or a variety of other categorical features in the data NA: This isn’t actually its own type of data, but it represents a missing value. These can become pesky, but there are ways to work around them. We can choose to replace them with 0 or any other value we want, we can ignore them in our computations, or we can do something completely different with them altogether. The important thing to remember about NA values is that they exist and should be acknowledged. 2.3 Vectors, Lists, and Data Frames Each of the data types listed above describes a single point of data, called a scalar. However, we usually we don’t have data given to us as one-by-one pieces of information. We’re normally given whole datasets at a time, or at least groups of related data, and they’re much easier to work with. Vectors A vector is a grouped set of data. Think about it as if it were the answer to a single question from a survey from all students in the class, or the heights of all basketball players in the NBA. We’ve actually been working with vectors all along! We’ll discuss it more in chapter 4. R actually treats every value as a vector. That’s why, as you may have noticed, lines of output begin with [1]. This indicates the index (position) in the vector that is at the start of the line. Any time we’ve had any type of data, R has just treated it as a vector of length 1. One important thing to note about vectors is that all members of the vector must be of the same type. If they aren’t, they will be coerced (changed) to be of the same type. To create a vector, we can use the c() function. This function combines the elements (individual data points) and turns them into a vector. Separate the parts with a comma (,). Here’s a few examples: c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10) [1] 1 2 3 4 5 6 7 8 9 10 c(TRUE, FALSE, T, F, T, FALSE) [1] TRUE FALSE TRUE FALSE TRUE FALSE c(&#39;R&#39;, &#39;is&#39;, &#39;fun&#39;) [1] &quot;R&quot; &quot;is&quot; &quot;fun&quot; c(1, &#39;apple&#39;, 2, &#39;banana&#39;) [1] &quot;1&quot; &quot;apple&quot; &quot;2&quot; &quot;banana&quot; Note how in the last example, everything appears inside of double quotation marks. This is an example of coersion in action. 1 and 2 are recognized as type integer, and 'apple' and 'banana' are recognized as type character. Since it’s easier to change a number to a character than it is to go the other way, 1 and 2 become characters. It may seem cumbersome, time-consuming, and tedious to type out numbers in order as we did in the first vector. : to the rescue! Another way that we can create that vector is by putting the first number we’d like in our vector on the left side of the :, and the last number on the right. c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10) [1] 1 2 3 4 5 6 7 8 9 10 1:10 [1] 1 2 3 4 5 6 7 8 9 10 As you can see, they both produce the same output. Let’s say we wanted to only include even numbers. Luckily, there’s a function that allows us to do that as well, and that’s the seq() function. To use it, we start by putting the first number in our vector, then the last number we want in the vector, and finally the amount we want to increment by in each place. To get the even numbers between 1 and 10, we want the following sequence: seq(2, 10, 2) [1] 2 4 6 8 10 Now, let’s say that we want to actually get at the contents of a part of a vector. We can accesss it by using its index. Unlike some other languages (Python, for example), R starts its indexing at 1, not 0. If we want to access the 3\\(^{\\text{rd}}\\) element of the vector we just created, use the index of the part we’re interested in (3) and put it inside of a single set of square brackets ([]) next to the vector. This will return the element in that location. seq(2, 10, 2)[3] [1] 6 To figure out how many elements are contained in a vector, we can use the length() function. This information is also displayed in the Environment tab we configured in chapter 1, assuming that the vector is stored as something in the global environment. The last function we should mention here is the rep() function. Similar to seq(), this function allows you to create a vector of the same number, repeated any number of times. Want the number 10 to be repeated 30 times? rep() makes this easy, as you can write rep(10, 30). The first argument is the number or vector you’d like to repeat, and the second is the number of times you’d like to repeat it. If the first argument is a vector, and you’d like to repeat each element a certain number of times, include the each = argument, with the number of times that you’d like each element to repeat. Let’s see rep() in action: rep(2, 10) [1] 2 2 2 2 2 2 2 2 2 2 rep(c(1, 2, 3), 3) [1] 1 2 3 1 2 3 1 2 3 rep(c(1, 2, 3), 3, each = 2) [1] 1 1 2 2 3 3 1 1 2 2 3 3 1 1 2 2 3 3 Lists On the surface, there’s not much difference between a list and a vector. The biggest difference is that a list can contain different types of data, whereas a vector cannot. To create a list, we can simply use the list() function, again putting all the different parts we want included inside the parenthesis, separated by a comma. list(1:10) [[1]] [1] 1 2 3 4 5 6 7 8 9 10 list(1, &#39;apple&#39;, 2, &#39;banana&#39;) [[1]] [1] 1 [[2]] [1] &quot;apple&quot; [[3]] [1] 2 [[4]] [1] &quot;banana&quot; Lists can be made up of vectors as well. That is, each element of a list is able to be a vector, since a list doesn’t care what type of data each of its elements is. To access a list’s elements, we want to use a double set of square brackets ([[]]) with the index we’d like to access. list(c(&#39;apple&#39;, &#39;banana&#39;), c(1.25, 2.50), 3)[[2]] [1] 1.25 2.50 The above example also illustrates that lists don’t need all list elements to be of the same length. Note that the third element of the list is only of length 1 (it’s just the number 3), but the other two elements are of length 2. Lastly, lists are able to have named elements. To name an element, all you have to do is type NAME OF ELEMENT = before each element, where NAME OF ELEMENT is whatever name you’d like to assign it. In the above example, let’s say we wanted to call the first element fruits, the second element prices, and the third element aisle. Then, our list would look like this: list(fruits = c(&#39;apple&#39;, &#39;banana&#39;), prices = c(1.25, 2.50), aisle = 3) $fruits [1] &quot;apple&quot; &quot;banana&quot; $prices [1] 1.25 2.50 $aisle [1] 3 We can then use a $ to go into the list and “pull out” that element (the vector with the corresponding name). We can then use vector indexing rules to get a particular element from the vector. If we wanted to get 'banana' from our list above, we have two options. list(fruits = c(&#39;apple&#39;, &#39;banana&#39;), prices = c(1.25, 2.50), aisle = 3)$fruits[2] [1] &quot;banana&quot; list(fruits = c(&#39;apple&#39;, &#39;banana&#39;), prices = c(1.25, 2.50), aisle = 3)[[1]][2] [1] &quot;banana&quot; As you can see, both options return 'banana', so these options are equivalent. Data Frames The last major type of combined data storing we need to talk about is a data frame. You can think of a data frame as a big table with the data you’d like. Each row of data is called an observation, and each column represents a feature or a variable. We’ll use a few of our own data frames throughout the semester, but it’s good to know that R comes with some of its own data frames already. This data comes from the crabs data frame in the MASS package (see chapter 3 for more information on packages). The first 6 rows are shown below. sp sex index FL RW CL CW BD B M 1 8.1 6.7 16.1 19.0 7.0 B M 2 8.8 7.7 18.1 20.8 7.4 B M 3 9.2 7.8 19.0 22.4 7.7 B M 4 9.6 7.9 20.1 23.1 8.2 B M 5 9.8 8.0 20.3 23.0 8.2 B M 6 10.8 9.0 23.0 26.5 9.8 To see a data frame, you’ll want to use the View() command. This will open the data frame in the Source pane. A few things about data frames: They’re really just an easy-to-see list. You can access any column (feature) by using the $ operator. The syntax (way to write the code) is: df_name$column_name, where df_name and column_name are the data frame name and column name respectively All columns (or list elements) must be of the same length. They may contain NA values, but their lengths must be the same To get the number of rows of a data frame, use the nrow() function. To get the number of columns, you can either use length() (since, as stated before, it’s just a list of vectors), or ncol(). This information is also in the Environment tab. You can make your own with the data.frame() function. Just put the vectors you’d like to include, separated by commas, inside of the parenthesis. Just like with lists, you can name the columns of a data frame as you create it. As long as the vectors are of the same length, you’ll be making data frames in no time! 2.4 Coercing To Other Types The last point we’ll make about different kinds of data is that you can coerce it yourself to be of another type. There are a lot of functions, the as._() functions, that are helpful here. Have a character string that’s just a number? No problem! we saw before that class('2') was of type character. What about if we wanted it to be of type numeric? class(as.numeric(&#39;2&#39;)) [1] &quot;numeric&quot; Awesome. 2.5 identical() This is as good a time as any to introduce the identical() function. What this does is checks if the things supplied to it are the identical. It returns TRUE if the arguments are identical, and FALSE if they’re different. Examples, with some of the syntax described above, are as follows: identical(1, 1) [1] TRUE x = 1:10 # This has length 10 y = 2:10 # This has length 9 identical(x, y) [1] FALSE identical(x[10], y[9]) [1] TRUE This function is very helpful when you want to check if two vectors or lists have the same information. It’s also particularly useful when you want to check if the outputs or results of different functions are the same if you’re reorganizing/rewriting code. "],
["variables.html", "Chapter 3 Variables 3.1 Naming 3.2 Vectors, Lists and Data Frame Names", " Chapter 3 Variables Pretend that you have a number that you don’t want to keep typing over and over and want R to remember what it means. You can store that value as a variable in R by declaring it (giving it a name) and assigning it a value. To make the assignment, you can use either = or &lt;-. It’s really personal preference as to which one you’d like to use, but for the duration of the book we’ll use = (mostly out of habit). To assign a variable a value, we put the variable name on the left side of the assignment operator, and we put the value on the right side. As an example, if we want R to remember that some variable that we’ll call \\(x\\) should have the value 5, you can do it like this: x = 5 To see what a variable contains, simply type the name of the variable. x [1] 5 Variables don’t only correspond to numeric data, however. We can have variables store any valid type of data. numVar1 = 18 numVar1 [1] 18 numVar2 = 3.14 numVar2 [1] 3.14 stringVar1 = &#39;This is a string&#39; stringVar1 [1] &quot;This is a string&quot; stringVar2 = &quot;I&#39;m learning what a string variable is&quot; stringVar2 [1] &quot;I&#39;m learning what a string variable is&quot; boolVar1 = TRUE boolVar1 [1] TRUE boolVar2 = F boolVar2 [1] FALSE boolVar3 = 3 &lt; 5 boolVar3 [1] TRUE 3.1 Naming You can name any variable you create whatever you’d like, but with this freedom comes great responsibility. Variable names must begin with either a letter. Often times, it’s much more useful to name a variable what it represents, rather than just calling it x. For example, if you’re dealing with a data set of attendance at Cubs games and you’d like to specify the attendance on July 12, you can name a variable attendanceJuly12 and it’s totally valid. You don’t just have to name your variables as a single letter. That’s a relief, because otherwise we’d only have 52 possible names! “52 possible names? I thought there’s only 26 letters in the alphabet…” — You, Again (probably) We’d have 52 possible names since in R, variable names are case sensitive. This means that naming one variable x and another X are interpereted differently. Going back to our example above, we could store x as 5 and X as 9. This is what it looks like: x = 5 X = 9 x [1] 5 X [1] 9 Typically, variables are named in camel case and begin with a lowercase letter (i.e. myFirstVariable as opposed to myfirstvariable, although both are totally valid). Get in the habit of naming variables this way. It’ll make it much easier to identify what variable you’re talking about when you need to access it in your code. Variables can also be operated on together. If you have a variable x and it’s equal to 10 and a variable y that’s equal to 6, and you want to sum them, you can do so like this: x = 10 y = 6 x + y [1] 16 The last thing to beware of when naming variables is overwriting the name of another variable. If you have a variable that’s already been declared, and then you want to declare a new variable, you will want to declare it with a new name. Continuing with our example from above, x currently holds value 10. If we now write x = 100 x [1] 100 We can see that x now holds the value 100, not 10 anymore. This is why we cautioned you earlier about using T and F in place of TRUE or FALSE. If you create a variable named T, or even do something like T = FALSE, that will be used instead of the shorthand T for TRUE. The bottom line of names is this: they’re important, useful, and completely up to you. Make the name short, sweet, and to the point: they shouldn’t be full sentences, but should be more than a letter. A good rule of thumb is the KISS rule: Keep It Simple, Silly. 3.2 Vectors, Lists and Data Frame Names Variables aren’t just useful for one data point, but they’re useful for vectors, lists, and data frames as well. Just as we just saw, you can assign a name to a vector, a list, or a data frame so that you can call on them later. In fact, it’s a really good idea to name all of these types of things, as they’re frequently how you’ll have your data organized anyways. The naming procedures and conventions are exactly the same as described above. Naming List Elements and Data Frame Columns As we saw earlier, it’s possible to name elements of a list or columns in a data frame as you create it. It’s also possible to change the names of already-named parts of these data types by making use of the names() function. Put the name of the list/data frame in the parenthesis, and then assign new names by supplying them as a character vector like we’ve done before. The example we had before was a short list of fruits, prices, and aisles. We’ll save it as a list called groceries: groceries = list(c(&#39;apple&#39;, &#39;banana&#39;), c(1.25, 2.50), 3) Now, we want to rename the elements as fruits, prices, and aisle. Rather than redeclaring our list, we’ll just use the names() function: names(groceries) = c(&#39;fruits&#39;, &#39;prices&#39;, &#39;aisle&#39;) We can also use the names() function to see what the names of the elements are: names(groceries) [1] &quot;fruits&quot; &quot;prices&quot; &quot;aisle&quot; Lastly, we can change individual names so we don’t have to retype/rename every element when we only want to change one. If we wanted to change prices to cost to save a keystroke each time we type it, we can do this: names(groceries)[2] = &#39;cost&#39; As you can see, we did exactly what we wanted to do. Since data frames are really just lists, the same type of renaming applies. Note: R will treat a single value as a vector, even without using the c()function. It’s coercion and vectorization in action. "],
["operators.html", "Chapter 4 Operators 4.1 Mathematical Operators 4.2 Logical (Boolean) Operators 4.3 Vectorization", " Chapter 4 Operators 4.1 Mathematical Operators One thing that R is really good at doing is mathematical computations. Here’s a few of the basic math operations that you’ll want to get familiar with and be very comfortable comfortable using. Symbol Meaning Example + addition 3 + 3 = \\(1 + 1\\) - subtraction 10 - 1 = \\(10 - 1\\) * multiplication 23 * 3 = \\(23 \\cdot 3\\) / division 276 / 4 = \\(\\frac{276}{4}\\) ** or ^ exponent 2 ** 3 = 2 ^ 3 = \\(2^3\\) exp() \\(e\\) exp(2) = \\(\\exp \\left( 2 \\right)\\) log() natural log log(2.71) = \\(\\ln{2.71} = e\\) log10() log\\(_{10}\\) log10(100) = \\(\\log_{10} 100\\) pi \\(\\pi\\) 2 * pi = \\(2 \\cdot \\pi\\) sqrt() square root sqrt(4) = \\(\\sqrt{4}\\) abs() absolute value ( \\(|x|\\) ) abs(-6) = |-6| = 6 R also works in the correct mathematical order of operations: PEMDAS. This stands for Parentheses/brackets Exponents Multiplication Division Addition Subtraction Because of this, using exra parenthesis is never a bad idea. R also automatically closes a set of brackets, parenthesis, or braces ({}) when you open them, but if you’re debugging and deleting make sure that you delete with caution. R will highlight the corresponding parenthesis when you highlight a different one, but these minor details can become major pains. Coder have caution! 4.2 Logical (Boolean) Operators As we talked about before, R can evaluate logical statements in addition to performing math operations. Here’s how this part works: Symbol Meaning Example &gt; greater than 3 &gt; 4 \\(\\implies\\) FALSE &lt; less than 2 &lt; 9 \\(\\implies\\) TRUE &gt;= greater than or equal to (\\(\\geq\\)) 13 &lt;= 13 \\(\\implies\\) TRUE &lt;= less than or equal to (\\(\\leq\\)) 14 &gt;= 12 \\(\\implies\\) FALSE == is exactly equal to 60 == 61 \\(\\implies\\) FALSE != is not equal to (\\(\\ne\\)) 69 != 818 \\(\\implies\\) TRUE &amp; logical AND (used in conjunction with one of the above) x &gt; 12 &amp; y == TRUE \\(\\implies\\) x is TRUE and y is TRUE | logical OR (used in conjunction with one of the above) x == TRUE | y == TRUE \\(\\implies\\) x is TRUE or y is TRUE 4.3 Vectorization Now it’s time to talk about one of the most useful features of R: the fact that it’s a vectorized language. This sounds very complex, but what it really means is that you can operate on entire vectors at a time using the operators we just went through. Let’s see a few examples using a vector of length 25. We’ll create a vector v1 that has the numbers 1 through 25, and we’ll do a few operations on it to really see how powerful vectorized operations are. v1 = 1:25 First, let’s see what simple addition does to the vector. Pretend we want to add 5 to each element? v1 + 5 [1] 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 [24] 29 30 What about doubling each element? v1 * 2 [1] 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 [24] 48 50 Cool! But what about that boolean thing we were talking about? Well, let’s try an example. What if we want to find where all the values are greater than 12? v1 &gt; 12 [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [12] FALSE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE [23] TRUE TRUE TRUE This is the power of vectorization: you can do things to really large amounts of data in a relatively short amount of time. Obviously, the more complex the operation is, the longer it will take, but it still beats the alternative of having to do the manipulations and calculations by hand. There’s one last important point to make about vectorization, and that’s what happens when you add vectors of unequal length. We’ve actually already seen examples of this, but we just didn’t realize it. What’s happening is that R is treating the numbers that we’ve supplied above (5, 2, and 12 respectively) as vectors of length 1, and recycling them over and over in its calculations. If instead we had tried to do v1 + c(5, 10), we’d wind up with something different. v1 + c(5, 10) [1] 6 12 8 14 10 16 12 18 14 20 16 22 18 24 20 26 22 28 24 30 26 32 28 [24] 34 30 This is something that we need to be aware of: even though R can do a vectorized calculation, we need to make sure that it’s performing the calculation we actually want it to do. Again, the length() function is your friend here! "],
["packages-and-getting-help.html", "Chapter 5 Packages and Getting Help 5.1 Packages 5.2 Help Files 5.3 Other Helpful Resources", " Chapter 5 Packages and Getting Help 5.1 Packages A package is just a collection of functions, data, and already-written code that is in a well-defined format. We’ll introduce a few packages as we go, but a lot of what we’ll cover will rely on the base package, which is just the functions that R already comes with. They’re the base of most analyses and other operations you’ll want to do in R, hence the name of the package. To install a new package, you want to use the install.packages() function, with the name of the package quoted and inside of the parenthesis. One package that you’ll want to install is the swirl package, which was developed as a self-guided class to help you learn R. To install this package, copy and paste the following line into your Console. install.packages(&#39;swirl&#39;) Once the package is installed, you’ll want to load the package (tell R that you’ll be using commands, data, and other information from the package). Copy/paste or type the following into your console to do so, and follow the prompts: library(swirl) 5.2 Help Files Another great aspect of R is how well-documented the language is. If you don’t know how to use a particular function, there are a lot of ways to learn how to use it and see it in action. The help files provide the syntax (what order to put things into the function), an explanation of the arguments, as well as information about the output (what the function gives back to you), and examples of the function in use. An Example Let’s say you wanted to learn more about the mean() function. You can type ?mean into your console, and the help file will pull up in the Help tab. From there, you can learn all about the function. The header of the mean() documentation As you can see, the top of the help function starts with the name of the function, with the package it comes from in brackets. Then it provides a brief description of what the function does. This one gets the mean (average) of a list of numbers. Syntax for mean() This part tells us that all the function looks for is an object x, which is just the numbers you’d like to take the average of. Something to note: the trim argument has a default value of 0, and na.rm is set to FALSE. As you hopefully remember from chapter 2, these will be the values that the function uses unless you specify differently. Then it explains the arguments it’s looking for as it does its job in the Arguments section: Arguments for mean() Next is the Value section, which explains what the output of the function actually is. Value for mean() Lastly, the documentation provides examples of it in use. You’ll understand this code by the end of the chapter, but for now trust that it’s doing its job appropriately. Examples of mean() 5.3 Other Helpful Resources If the help file isn’t much help in solving your problem, there’s a plethora of other helpful resources available to you. What do you do when you don’t understand something? What about if you get an error message and your code doesn’t run properly? One of the most important things to remember is that you’re not the first person to struggle with wherever you’re stuck, or get the error message you just got. There may not always be a TA that’s available to help right when you need them, so it’s a good idea to start learning how to help yourself solve your problems. StackOverflow is one of the best resources available to you in terms of help. It’s an open, online forum for people to post and answer questions. Think of it like reddit, but for code. This is a fantastic place to start your search, and the questions range from the very simple beginner questions to incredibly technical and advanced concepts. If a post is too technical for you to understand, there’s almost always another one (or more!) that can help. It may seem odd, but Googling is also a great way to learn. Because of how powerful Google is, copying and pasting error messages will usually result in a good explanation of the error message, why you’re getting it, and multiple ways to correct it. Google is your friend. There’s also plenty of resources online to help you learn R programming as well. In addition to the swirl library we talked about earlier, there are some websites, like codeschool, that allow you to practice writing R code. Feel free to use this one as much or as little as you’d like, but the best way to get better at programming is to practice as much as possible. Lastly, there’s also a variety of books that you can make use of to help learn and understand R, like this book It’s based off a Coursera course, and the author made videos that are linked in the book. You can download it for free by just sliding the price slider to 0 (or you can pay if you want). "],
["loading-data.html", "Chapter 6 Loading Data 6.1 Data Files 6.2 Reading in CSV Files 6.3 Writing Files", " Chapter 6 Loading Data Like we learned before, you’ll be able to see what data, variables, and other things you’ve loaded and stored in R by checking your Environment tab. Right now, yours is most likely empty, and that’s okay! We’ve learned a lot, but haven’t started doing a ton yet. We will soon, but first we need to learn how to get data into R so that we can start exploring it. 6.1 Data Files Most of the time, you’ll want to work with something called a CSV file. CSV stands for Comma Separated Value, which means that all of the data within an observation is separated by a comma. Here’s an example CSV file that you can download. To see the raw file, you can open it in a program like TextEdit on a Mac or Notepad on PC. Usually, a CSV file will default to open in a spreadsheet program like Excel since it’s easy to parse (read in and break up) into columns – this is because of the commas! 6.2 Reading in CSV Files To read a CSV into R, there’s a great function called read.csv(). It will read in your file as a data frame. To read in the file, you just need to put the name of the file as a charachter string (remember your data types?) in between the parenthesis. This is the only required argument, or input to a function, that you need to supply, however there are a few others to note: The stringsAsFactors argument can be either a TRUE or FALSE value. When TRUE (this argument’s default value), it takes any character strings in the data and coerces them to be factors. Sometimes this may be okay, sometimes not The header argument. This indicates whether or not a header row is present in the CSV file, which would contain names for all of the columns. It defaults to TRUE, but it’s a good idea to double check your data and make sure that one’s present. You can check the names and rename if you wish by using the names() function discussed in chapter 3 Directories One common issue is the location on your computer where the CSV is located. If you just type the name of the file as the argument for read.csv(), R will look for it in your working directory, or default file lookup location. However, if your file isn’t present in the working directory, you’ll likely get an error message. If this is the case, you have a few options: You can supply the full filepath to the data as the string. Watch this video to find how to find the full filepath for Mac or this one for PC. While it may be more work to find, it’ll guarantee that you import the right file. You can change your working directory with the setwd() function, supplying the path to the directory as a character string argument. This still requires knowing where your file is located, but if you plan to work with multiple files, this isn’t a bad option. You can check your current working directory with the getwd() function without any arguments. Loading Example Now let’s import that example CSV file. It’s the combined results from Survey 1 of STAT 100 and STAT 200. Remember, we should name the new data frame so that we can look at it and refer back to it. We’ll use the CSV file 'Combined Fall 2017 Survey 1.csv', which does have a header row, and is located in a subdirectory of our working directory called data (this is the directory that holds all of our data files for the book). It’s in a directory called data, has a header row, and we’ll keep character variables as characters. When we import it, we’ll call it survey1: survey1 = read.csv(&#39;data/Combined Fall 2017 Survey 1.csv&#39;, header = TRUE, stringsAsFactors = FALSE) To see the first few observations, you can use the head() function, passing the data frame’s name (in our case, survey1). This will display the observations in your Console, and will look like this: head(survey1) gender genderID height weight shoeSize schoolYear studyHr GPA ACT pets 1 Male Male 66 200 10.5 Sophomore 2.5 3.5 24 1 2 Female Female 66 142 7.5 Freshman 3.0 3.8 26 1 3 Male Male 65 160 10.5 Freshman 3.0 3.9 33 6 4 Female Female 68 118 7.5 Sophomore 3.0 3.9 28 0 5 Female Female 61 173 9.0 Sophomore 0.5 2.8 21 1 6 Female Female 66 125 8.0 Sophomore 2.5 2.3 20 0 siblings speed cash sleep shoeNums ageMother ageFather random love 1 2 25 5 7.0 3 47 49 6 few 2 1 80 11 4.5 21 37 47 4 few 3 0 0 45 7.5 4 43 46 8 few 4 1 105 9 7.5 25 54 53 8 few 5 3 90 7 8.0 13 35 60 7 one 6 0 50 3 6.0 20 40 41 3 dozens charity movie favTV1 favTV2 1 60 Life is Beautiful Narcos Daredevil 2 60 The Giant Como dice el dicho Drake and Josh 3 10 Interstellar none none 4 25 La La Land Friends Parks and Recreation 5 0 Get Out Attack on Titan Rick and Morty 6 50 Hidden Figures The Cosby Show A Different World section 1 Stat100_L1 2 Stat100_L1 3 Stat100_L1 4 Stat100_L1 5 Stat100_L1 6 Stat100_L1 Use the View() function, again passing the name of the data frame as the argument. It’ll display much cleaner and clearer in your Source pane, looking more like this: gender genderID height weight shoeSize schoolYear studyHr GPA ACT pets siblings speed cash sleep shoeNums ageMother ageFather random love charity movie favTV1 favTV2 section Male Male 66 200 10.5 Sophomore 2.5 3.5 24 1 2 25 5 7.0 3 47 49 6 few 60 Life is Beautiful Narcos Daredevil Stat100_L1 Female Female 66 142 7.5 Freshman 3.0 3.8 26 1 1 80 11 4.5 21 37 47 4 few 60 The Giant Como dice el dicho Drake and Josh Stat100_L1 Male Male 65 160 10.5 Freshman 3.0 3.9 33 6 0 0 45 7.5 4 43 46 8 few 10 Interstellar none none Stat100_L1 Female Female 68 118 7.5 Sophomore 3.0 3.9 28 0 1 105 9 7.5 25 54 53 8 few 25 La La Land Friends Parks and Recreation Stat100_L1 Female Female 61 173 9.0 Sophomore 0.5 2.8 21 1 3 90 7 8.0 13 35 60 7 one 0 Get Out Attack on Titan Rick and Morty Stat100_L1 Female Female 66 125 8.0 Sophomore 2.5 2.3 20 0 0 50 3 6.0 20 40 41 3 dozens 50 Hidden Figures The Cosby Show A Different World Stat100_L1 Much better. 6.3 Writing Files After you finish with your analysis, you may wish to save the data frame(s) that you’ve created. Similar to the read.csv() function that allows you to import a CSV, the write.csv() function will allow you to write your own CSV files to your computer to save and send as needed. "],
["coding-style.html", "Chapter 7 Coding Style 7.1 Comments 7.2 Spacing 7.3 Indentation 7.4 Consistency 7.5 Simplifying Coding Style", " Chapter 7 Coding Style It’s almost time to start applying what you’ve learned, but before we get into writing code, we should take a minute to start talking about more good coding practices. It’s often said that the best way to break bad habits is to not fall into them in the first place, so we’ll try to get into good habits right from the get-go. The code that you write will be read by R, which will ignore extra spaces, correct for indentation, and for all intents and purposes run properly, assuming that it’s syntactically correct. However, you will also be reading through your code, both as you write and debug it. That means that it should be easy for you – or anyone else for that matter – to read as well as R. This chapter’s whole purpose is to make this as easy as possible for you to do. 7.1 Comments There’s one important symbol/operator that we left out in chapter 5, and that’s the comment symbol. A comment is just a note for yourself so that you can explain what a block of code does, why you wrote the code a particular way, or really just anything else that you’d like to note at that point in the code. They won’t be evaluated by R as commands, so it may be useful to even comment out parts of your code (make line(s) of code into comments to prevent them from running but save yourself from retyping). To make a comment in R, use #. Pro tip: You can highlight whole lines of code, then go to the Code menu at the top and select Comment/Uncomment Lines to comment out (or uncomment) sections of code at a time. The shortcut on a Mac is Cmd + Shift + C, and on a PC it’s Ctrl + Shift + C. Get in the habit of commenting frequently as you code. As we said before, these are ways for you to remember what you did so that when you revisit your code, you remember what your thought process was. 7.2 Spacing One really good practice is to put a single space before and after any operator you use. While it does lengthen the line of code itself, it makes it much easier to debug. You may find that you’ve used a = to check a condition when you should have used a ==, or you may see that you only put one * when you meant to use the right side as an exponent. Spacing also refers to spacing lines of code out within your script. Have a line that actually takes up 3 lines? No problem, R can handle that, but it’ll be a pain to read. Just find a good breaking point in the line (usually after a comma) and go to the next line. It allows you to see more of your code in an easier format. You may even wish to put each individual argument of a function on its own line in some cases, and that’s encouraged! Keeping with our “script-is-a-MS-Word-Doc” analogy from earlier, related code lines should be grouped together, and separated from other grouped lines of code, just like you’d separate ideas in your essay into paragraphs. They should follow a logical order, be organized into groups, and after each group, you should skip a line to signal the next group is beginning. As we said before, R will ignore empty lines of code, so there’s no harm in skipping a line to organize your thoughts. 7.3 Indentation If you were an absolutely perfect code-writer, this part would take care of itself. However, there’s no such thing as a perfect code-writer, therefore this is worth mentioning. In languages such as Python, indentation. Is. Everything. In R, it’s not as imperative in terms of functionality, but it’s equally imperative in terms of legibility. Once we get to control structures (ADD LINK TO CONTROL STRUCTURES HERE), you’ll be able to see these with much more clarity, but it’s a good idea that any time your code takes up more than one line and you’re working inside of parenthesis, braces, or brackets, you indent your code one tab (four spaces) to the right. Open a new set of parenthesis/braces/brackets after you’ve indented once? Indent again! No harm, only help! 7.4 Consistency In an essay, you wouldn’t switch fonts, colors, or page layouts in the middle, would you? You shouldn’t change much in your code scripts either. Your code should be consistent in as many ways as you can find. This comes up a lot with naming and assignment, so those will be where we’ll turn our focus for now. Name variables in the same way every time you name a variable in a script. If you usually use camel case to name your variables, make sure all of your variables (where applicable) are named with camel case. If you use underscores (_) in names, don’t switch to naming things with periods instead. (Example: if you name something my_variable, don’t name another variable my.new.variable). Names should be unique, concise, and descriptive. In terms of assignment, pick either = or &lt;- and stick with it. It’s good to realize that they do the same thing, and it’s even better to practice using both, but within a script, you should stick to just one. That way, anyone that reads it can clearly identify where you’ve named a variable. 7.5 Simplifying Coding Style This may seem like a lot of very specific things to keep in mind. Luckily, RStudio has a built-in capability to handle this for you and make your life much easier. Simply highlight all of your code that you’d like formatted (it should be all of it), go to the Code menu, and select Reformat Code (Mac: Cmd + Shift + A. PC: Ctrl + Shift + A). Then, go back into the Code menu (with the code still highlighted) and select Reindent Lines (Mac: Cmd + I. PC: Ctrl + I.) While this won’t be necessary right away, this is an excellent, powerful tool to keep in your back pocket for when you do end up needing it. "],
["hists.html", "Chapter 8 Bar Graphs vs. Histograms 8.1 Drawing Bar Graphs and Histograms in R Extracting Information From Histograms", " Chapter 8 Bar Graphs vs. Histograms 8.1 Drawing Bar Graphs and Histograms in R One way to explore data in R is by creating quick vizualizations. We’ll use survey1, the STAT 100 and 200 combined survey data from before, to demonstrate. The dataset has 1628 observations of 24 variables, and a description of the variables in the dataset is available here. Preview of survey1 gender genderID height weight shoeSize schoolYear studyHr GPA ACT pets siblings speed cash sleep shoeNums ageMother ageFather random love charity movie favTV1 favTV2 section Male Male 66 200 10.5 Sophomore 2.5 3.5 24 1 2 25 5 7.0 3 47 49 6 few 60 Life is Beautiful Narcos Daredevil Stat100_L1 Female Female 66 142 7.5 Freshman 3.0 3.8 26 1 1 80 11 4.5 21 37 47 4 few 60 The Giant Como dice el dicho Drake and Josh Stat100_L1 Male Male 65 160 10.5 Freshman 3.0 3.9 33 6 0 0 45 7.5 4 43 46 8 few 10 Interstellar none none Stat100_L1 Female Female 68 118 7.5 Sophomore 3.0 3.9 28 0 1 105 9 7.5 25 54 53 8 few 25 La La Land Friends Parks and Recreation Stat100_L1 Female Female 61 173 9.0 Sophomore 0.5 2.8 21 1 3 90 7 8.0 13 35 60 7 one 0 Get Out Attack on Titan Rick and Morty Stat100_L1 Female Female 66 125 8.0 Sophomore 2.5 2.3 20 0 0 50 3 6.0 20 40 41 3 dozens 50 Hidden Figures The Cosby Show A Different World Stat100_L1 Let’s explore the height variable a little bit. One way that we can do it is by breaking up, or binning, the data into different groups, then plotting what percentage of the data is in each group. This creates what’s called a histogram. To make a histogram in R, we can use the hist() function (see ?hist for more information). All that hist() needs is an argument x, which is what you’d like to make a histogram of. Since we want the densities, we’ll add in the freq = FALSE argument. This results with a histogram that looks like this: hist( survey1$height, freq = FALSE ) Figure 8.1: A basic histogram Let’s add a few extra arguments to make the plot a little clearer: main and xlab create a title and an \\(x\\)-axis label respectively ylim sets the range of \\(y\\)-values that are shown on the \\(y\\)-axis (Note: xlim does the same for the \\(x\\)-axis) breaks controls what numbers are used as part of the binning process. The first number is the smallest value in the data, and the last value is the largest. You can find these by employing min() and max() individually, or you can use the range() function and get both at the same time. Note: the break points we used were arbitrarily selected col changes the colors of the bars. We change these to make them a little easier to identify, but it’s purely cosmetic. Supplying a single value, which can be any named color that R already recognizes, an RGB value while using the rgb() function, or a hexadecimal color value supplied as a character, preceded by a #. We’ll leave it to you to learn about these color formats on your own, but kow that they’re available to you. Supplying a single value will change the color for each bar, making all the bars the same color, while supplying a vecotr of the same length as the number of bars will change each color individually par(mfrow = c(1, 2)) # Puts plots side-by-side # Histogram hist( survey1$height, main = &#39;Histogram of Heights&#39;, xlab = &#39;Heights in Inches&#39;, ylim = c(0, .1), freq = FALSE, breaks = c(49, 62, 65, 68, 70, 73, 95), axes = FALSE, # Removes default axis numbers labels = TRUE, # Put decimals above each bar col = &#39;#0088ce&#39;, border = &#39;#939598&#39; ) axis(2) # Puts y-axis numbers back axis(1, at = c(49, 62, 65, 68, 70, 73, 95)) # Puts x-axis numbers back # Bar plot hist( survey1$height, main = &#39;Bar Plot of Heights&#39;, xlab = &#39;Heights in Inches&#39;, ylim = c(0, 500), freq = TRUE, breaks = c(49, 62, 65, 68, 70, 73, 95), labels = TRUE, # Put counts above each bar axes = FALSE, # Removes default axis numbers col = &#39;#0088ce&#39;, border = &#39;#939598&#39; ) axis(2) # Puts y-axis numbers back axis(1, at = c(49, 62, 65, 68, 70, 73, 95)) # Puts x-axis numbers back Figure 8.2: Well-formatted histogram (left) and density plot (right) While the overall shapes of the two plots seem the same, there are a few important differences to take note of. The biggest one is the significance of each block’s width and height. With a histogram, the height shows the percentage per unit inside of each block, while on a bar graph the heights have no meaning whatsoever. The numbers displayed on top show the total number of people inside each interval. The widths (and the \\(x\\)-axis altogether) of each plot also carry different meanings: barplots just show an interval, while a histogram represents unique heights. That is, even if a number doesn’t appear on the \\(x\\)-axis in a histogram, that height is still represented. Since the heights and widths of each kind of plot differ, so too do the areas, since the areas are the width of the interval times the height of the interval. Histogram areas show percentages within each block, while bar plot areas are – you guessed it – irrelevant. We could also summarize our data as the following table: Range Area Count 49-62 14.8% 241 62-65 25% 407 65-68 27.15% 442 68-70 13.76% 224 70-73 12.71% 207 73-95 6.57% 107 Note: the areas of the blocks on the histogram sum to 100% This isn’t to say that bar plots don’t have their place, this just isn’t it. Sorry bar graphs, you’ll just have to wait until it’s analysis time. We’ll focus on histograms for the rest of this chapter. Extracting Information From Histograms In addition to being able to do visual analysis of a histogram, it may be more useful to use some of the information that the hist() function generates. Usually what we care about is the plot itself, but hist() calculates and stores a lot of information in addition to generating the plot. We’ll store the results from histogram as something called hist1. hist1 = hist( survey1$height, main = &#39;Histogram of Heights&#39;, xlab = &#39;Heights in Inches&#39;, ylim = c(0, .1), freq = FALSE, breaks = c(49, 62, 65, 68, 70, 73, 95), axes = FALSE, # Removes default axis numbers labels = TRUE, # Put decimals above each bar col = &#39;#0088ce&#39;, border = &#39;#939598&#39; ) In addition to the plot, hist1 contains a list with elements breaks, counts, density, mids, xname, and equidist. Check the help file for what these mean, but we should note that the total count of observations and the density of observations in a given range can be accessed by hist1$counts and hist1$density respectively. Check out the use of list element extraction using $ here! These are vectors, and you can use the components as you need to. "],
["measures-of-central-tendency.html", "Chapter 9 Measures of Central Tendency 9.1 The Mean and mean() 9.2 The Median and median() 9.3 Standard Deviation and sd() 9.4 Mean and Standard Deviation After Changing The Data", " Chapter 9 Measures of Central Tendency A measure of central tendency is a way of talking about the common values from a set of data points. In this section, we’ll talk about two: the mean and the median. 9.1 The Mean and mean() The most common measure of central tendency is the average, which is often referred to as the mean. Sometimes, you’ll see it represented as the greek letter mu \\(\\left( \\mu \\right)\\), or as \\(\\bar{x}\\). If we have a set of numbers – let’s say those numbers are 0, 5, -5, 10, -10, 40, and 100 – we compute the average as follows: \\[ \\bar{x} = \\frac{0 + 5 + \\left( -5 \\right) + 10 + \\left( -10 \\right) + 40 + 100}{7} = \\frac{140}{7} = 20 \\] This may seem trivial, so let’s get to the fun part of doing it in R. First thing is first: we need our numbers to get loaded into R. Aha! A perfect time for a vector. Since we called the average \\(\\bar{x}\\), let’s name this vector x. x = c(0, 5, -5, 10, -10, 40, 100) Now we need to go and actually calculate \\(\\bar{x}\\). Well we’re calculating the mean, so we should try the mean() function. mean(x) [1] 20 This returns (gives back) the mean of x, and we can see that it is, in fact, 20. Another way that we could have done this is more like we would have done by hand: first we would have to sum the numbers, then we’d have to divide them by the number of numbers we just added together. Luckily, the sum() and length() functions save us a lot of time and energy. sum(x) / length(x) [1] 20 Again, we get that the average is 20, thus both methods are valid. Handling NA Values What would happen in functions like mean(), sum(), length(), min(), or max() when NAs are present? Well let’s find out. We’ve inserted a few NAs into the same vector as above, but we’ve called it y now so as not to confuse the two. y = c(0, 5, NA, -5, 10, -10, NA, 40, 100) mean(y) [1] NA sum(y) [1] NA They return NA! You may be asking yourself, why? Well, the reason this happens is because you haven’t told R how to deal with a non-existent value (We told you they’d be pesky!). You most likely want R to ignore the NA altogether, so we can take advantage of an argument called na.rm and set it equal to TRUE. mean(y, na.rm = TRUE) [1] 20 sum(y, na.rm = TRUE) [1] 140 Great, everything is working again! Problem solved. 9.1.1 mean() with TRUE/FALSE It’s also good to note that R can take the mean of logical vectors. What happens is that TRUE is coerced to be 1, and FALSE is coerced to be 0. Then, the mean is taken just as before. This may seem boring, but it allows us to determine percentages quickly. Let’s say we want to find the percentage of students that responded to Survey 1 that maintained at least a 3.0 GPA. How would we figure this out if we were doing it by hand? Probably like this: Get an observation of data (that is, go to the first response) Check the GPA for that responder If the GPA is 3.0 or higher, we add 1 to our tally Otherwise, add 0 After going through all observations, take the final tally and divide by the number of observations we observed (that had values) Luckily, there’s a very good way to do this in R, and it involves the mean() function. In addition to evaluating the mean of a vector of numbers, it can compute the percentage of observations that meet a certain condition, given by something called a conditional statement. We talked about them before, so now it’s time to see why we spent the time learning how to write them. We’ll show the “complete” version of the process we just outlined, and then we’ll go through and do it the short way. # Create TRUE/FALSE vector of places where students have GPAs above 3.0 bool_vec = survey1$GPA &gt; 3.0 head(bool_vec) [1] TRUE TRUE TRUE TRUE FALSE FALSE # Coerce the TRUE/FALSE vector to be 1s and 0s bool_as_binary = as.numeric(bool_vec) head(bool_as_binary) [1] 1 1 1 1 0 0 # Calculate the percentage sum(bool_as_binary) / length(bool_as_binary) [1] 0.7800983 This works, but there’s a faster way to do it. We can see that the last step of summing and dividing by the length is the same as the mean() function, so we can just change that to be mean(bool_as_binary) and get the same result. But bool_as_binary is just the numeric coersions of bool_vec, and R knows to do this coersion automatically when using a function like mean() on a vector of type logical. So it’s equally valid to say mean(bool_vec), although we’ll take it one step farther. We’ll put the conditional statement that created bool_vec directly into the mean() function, and we should get the same result. mean(survey1$GPA &gt; 3.0) [1] 0.7800983 Not only did we save four lines of code to get the same result, but we saved a little memory (storage space) in the process. This is because we didn’t need to store the result of the conditional or its coersion as vectors to be used later. R just handled it all internally. This exact procedure is very useful when you want to check how much data is above a threshhold in a quick way. 9.2 The Median and median() The median, or middle number, is the other most common measure of central tendency. It’s the point in which our data gets split in half. To determine the median by hand, we’d follow the following process: Arrange data in numerical order from smallest to greatest Cross off the two endpoints Move in one data point from each end of the range of data Repeat steps 2 and 3 until either: One number remains in the middle, or Two numbers remain in the middle If one number remains, we’ve found our median. If two numbers remain, take the mean of the last two remaining data points. Let’s find the median of this list of numbers: 6, 5, 0, 12, 10, 11 By hand, we’d put them in the following order: 0, 5, 6, 10, 11, 12 Then we’d cross them off, endpoint pair by endpoint pair, until we reach the middle: 0, 5, 6, 10, 11,12 0, 5, 6, 10, 11, 12 We’re down to two numbers, so we just take the average of the remaining numbers, which are 6 and 10. We get 8, so this is our median. As was the case with mean(), R comes with a great function – the median() function – to calculate the median of a list of numbers. We can supply the vector directly to median() and immediately get the median. median(c(6, 5, 0, 12, 10, 11)) [1] 8 Skewed Histograms Sometimes, you’ll get data that’s heavily skewed one way or another. This means that the mean and the median are not in the same place. Let’s take a quick look: Figure 9.1: Skewed and Symmetric Distributions The plot on the left shows a histogram that’s right-skewed. This means that the mean is greater than \\(\\left( \\gt \\right)\\) the median. The middle plot shows a symmetric distribution. This means that the mean is equal to the median. The plot on the right shows a histogram that’s left-skewed. This means that the mean is less than \\(\\left( \\lt \\right)\\) the median. 9.3 Standard Deviation and sd() Lastly, we should talk about the standard deviation. This is how spread out around our average the data is. The standard deviation applies to the whole vector, not just an individual point. We’ll talk about how to use the concept of a standard deviation for just a single data point in the next chapter. Breaking the term down, we can understand a little bit more intuitively what exactly a standard deviation is. A deviation is how far a particular point is from another point (in our case, the average), and standard typically means the average, or divided by how many points there are. So really, a standard deviation is a way to look at the average difference of any data point from the average of the data. Note: The standard deviation can never be negative, since it doesn’t make sense to be a negative distance from the average. The standard deviation is calculated as \\[ \\sqrt{\\frac{1}{n} \\sum_{i = 1}^n \\left( x_i - \\bar{x} \\right)^2} \\] Now that the scary math is out of the way, let’s make it make sense. We’ll define a process to calculate the standard deviation, and rebuild that equation as we go. Calculate the average, \\(\\bar{x}\\), of all of the data points Take each point in the data, which we’ll call \\(x_i\\) (think of \\(i\\) as the \\(i^{th}\\) element of the vector of data points), and subtract off the average. These are the deviations Pro tip: These deviations should always add to 0 Square the difference in step 2. Mathematically, we’ve got \\[\\left( x_i - \\bar{x} \\right)^2\\] Take the average of the squared differences in step 3. That’s where the \\(\\frac{1}{n} \\sum_{i = 1}^n\\) comes from, and it gives us what’s called the variance, and it’s a single number: \\[ \\frac{1}{n} \\sum_{i = 1}^n \\left( x_i - \\bar{x} \\right)^2 \\] Take the square root of the variance we just found in step 4. This is the standard deviation, and it’s given by \\[ \\sqrt{\\frac{1}{n} \\sum_{i = 1}^n \\left( x_i - \\bar{x} \\right)^2} \\] We’ll do a quick example as a table with this list of numbers: 0, 1, 2, 3, 4, 5, 6 . The average of the list is 3. \\(x_i\\) \\(x_i - \\bar{x}\\) \\(\\left( x_i - \\bar{x} \\right)^2\\) 0 -3 9 1 -2 4 2 -1 1 3 0 0 4 1 1 5 2 4 6 3 9 We then average the last column to get 4, and take the square root to get a standard deviation of 2. Now that we see how the process works and can do it by hand, we’re ready to use the R function sd() to speed the process along. Just pass the vector of values to sd() and the standard deviation will be computed. sd(c(0, 1, 2, 3, 4, 5, 6)) [1] 2.160247 Hmmm… That’s not quite right. By hand, we got 4, but with sd(), we got 2.1602469. This is where we need to start being a little careful. Earlier, we defined standard deviation as the standard deviation of the population (frequently referred to as \\(\\sigma\\)), which is given by \\[ \\sigma = \\sqrt{\\frac{1}{n} \\sum_{i = 1}^n \\left(x_i - \\bar{x} \\right)^2} \\] However, in R, the sd() function computes the sample standard deviation, which is slightly different. (We’ll cover it later on in the class, but to help you understand what R is doing, we need to introduce the difference now.) The sample standard deviation, \\(s\\), is given by \\[ s = \\sqrt{\\frac{1}{n-1} \\sum_{i = 1}^n \\left(x_i - \\bar{x} \\right)^2} \\] The the sd() function is actually computing \\(s\\) and not \\(\\sigma\\). That’s okay though! A quick bit of algebra helps us to make the conversion between the sample and the population. To get from \\(s\\) to \\(\\sigma\\), we just need to multiply by \\(\\sqrt{\\frac{n - 1}{n}}\\). The \\(n - 1\\) in the numerator cancels the \\(n - 1\\) in the denominator of \\(s\\), and then the \\(n\\) in the denominator is the \\(n\\) in \\(\\sigma\\). So, to get the population standard deviation, we need to do this: sd(c(0, 1, 2, 3, 4, 5, 6)) * sqrt((length(c(0, 1, 2, 3, 4, 5, 6)) - 1) / length(c(0, 1, 2, 3, 4, 5, 6))) [1] 2 All fixed! Let’s put this into a function so that we can do it on any data set we’d like. We’ll call the function stdv(), which takes one argument (the numeric vector x), and returns the population standard deviation. stdv = function(x){ sd(x) * sqrt((length(x) - 1) / length(x)) } The larger the standard deviation is, the more spread out our data is. The smaller the standard deviation, the less spread the deviation is. As an extreme case, a standard deviation of 0 means that there is no spread whatsoever. This is because all of the data would be located at the average. 9.4 Mean and Standard Deviation After Changing The Data The mean and median change if we add, subtract, multiply, or divide the same number to every point in our data. They will change by the exact amount that we’ve added or subtracted (i.e. if we add 4 to every number, the average and median increase by 4. Dividing by 12 will divide the average and median by 12). The standard deviation changes differently under addition, subtraction, multiplication, and division. Addition and subtraction don’t have any impact on the standard deviation, while multiplication and division change the standard deviation by the same value (i.e. if we multiply every point by 6, the standard deviation is multiplied by 6 as well) If the factor that the data points are multiplied by is negative, the standard deviation is multiplied/divided by the absolute value of the factor. Note: We don’t immediately know how the average, median, or standard deviation change by doing any of these operations to just a single point in our data, but R makes these computations easy. "],
["the-normal-approximation.html", "Chapter 10 The Normal Approximation 10.1 The SD 1-2-3 Rule 10.2 Z-scores 10.3 The _norm() Functions", " Chapter 10 The Normal Approximation The ideal histogram will have a bell shape like the one you see above. While not all histograms will have this shape, many will roughly approximate it. This shape is called a normal curve, also referred to as the normal approximation. There’s two major facts that we need to keep in mind about the normal curve: It’s a symmetric distribution about the center (the average). This fact allows us to apply the logic that whatever we do to one side of the curve, we can safely do to another. We’ll keep coming back to this fact, so it’s really important to keep this in the back of your head. Just like a histogram, the total area underneath the curve adds to 100%. Again, this is something that we’ll keep coming back to. Burn this fact into your brain too. A few more things to note about the normal curve: While on the plot the range of the \\(x\\)-axis goes from -3 to 3, the curve really extends out forever. It’s asymptotic It’s a density plot and not a frequency plot (this is what allows the total area to be 100%) The highest point on our curve occurs at 0. Since the \\(x\\)-axis is how many standard deviations away from the average a point is, we know then that the curve’s highest point (we say “it’s centered”) around 0 10.1 The SD 1-2-3 Rule The SD 1-2-3 Rule tells us how much data is within 1, 2, and 3 standard deviations of the average. The orange area of the above normal curve is &lt;span style = ‘color: #e04e39’; font-weight: bold’&gt;1 standard deviation of the average, or roughly &lt;span style = ‘color: #e04e39; font-weight: bold’‘&gt;68%. Within 2 standard deviations of the average (the &lt;span style = ’color: #13294b; font-weight: bold’‘&gt;blue area, plus the middle &lt;span style = ’color: #e04e39; font-weight: bold’‘&gt;orange area), gives us approximately &lt;span style = ’color: #13294b; font-weight: bold’‘&gt;95% of the data, and the &lt;span style = ’color: #a5acaf; font-weight: bold’‘&gt;grey area, plus the &lt;span style = ’color: #e04e39; font-weight: bold’‘&gt;orange and &lt;span style = ’color: #13294b; font-weight: bold’‘&gt;blue areas, give &lt;span style =’#a5acaf; font-weight: bold’’&gt;99.7% of the data. Quick summary: &lt;span style = ‘color: #e04e39; font-weight: bold’‘&gt;orange = &lt;span style = ’color: #e04e39; font-weight: bold’’&gt;68% &lt;span style = ‘color: #e04e39; font-weight: bold’‘&gt;orange + &lt;span style = ’color: #13294b; font-weight: bold’‘&gt;blue = &lt;span style = ’color: #13294b; font-weight: bold’’&gt;95% &lt;span style = ‘color: #e04e39; font-weight: bold’‘&gt;orange + &lt;span style = ’color: #13294b; font-weight: bold’‘&gt;blue + &lt;span style = ’color: #a5acaf; font-weight: bold’‘&gt;grey = &lt;span style = ’color: #a5acaf; font-weight: bold’’&gt;99% In other words, &lt;span style = ‘color: #e04e39; font-weight: bold’‘&gt;68% of the data is between &lt;span style = ’color: #e04e39; font-weight: bold’‘&gt;-1 and &lt;span style = ’color: #e04e39; font-weight: bold’‘&gt;1, &lt;span style = ’color: #13294b; font-weight: bold’‘&gt;95% of the data is between &lt;span style = ’color: #13294b; font-weight: bold’‘&gt;-2 and &lt;span style = ’color: #13294b; font-weight: bold’‘&gt;2, and &lt;span style = ’color: #a5acaf; font-weight: bold’‘&gt;99% of the data is between &lt;span style = ’color: #a5acaf; font-weight: bold’‘&gt;-3 and &lt;span style = ’color: #a5acaf; font-weight: bold’’&gt;3. Now this is great and all, but what about if the data isn’t exactly 1, 2, or 3 standard deviations away from the average? Glad you asked. That’s where Z-scores make their money. 10.2 Z-scores A Z-score, also known as standard units, is a measure of how many standard deviations away from the average a particular point of data is. Like we just said, every point of data in the data set will correspond to a Z-score. If we want to know how many standard deviations away from the average a point of data is, we should start by figuring out how far the point itself is from the average in whatever units the data’s in (i.e. if we’re talking height, how many inches away from the average is this particular data point?). \\[ \\text{Distance from average} = \\text{Value} - \\text{Average} \\] Then, if we want to figure out how many standard deviations the point (sometimes called \\(x\\)) is away from the average (occasionally called \\(\\mu\\)), we just need to divide by the standard deviation (SD, AKA \\(\\sigma\\)) is. This is how we get the formula for a Z-score. \\[ Z = \\frac{\\text{Distance from average}}{SD} = \\frac{\\text{Value} - \\text{Average}}{\\text{SD}} = \\frac{x - \\mu}{\\sigma} \\] So, all we need to calculate a Z-score is the data point, the average, and the standard deviation. But then how do we know what the corresponding area is? Luckily, we have this handy chart that tells us the middle area, which is the area between -Z and +Z. What a lifesaver! Just be sure that before you go to that chart, you’ve converted everything to a Z-score. We can then compute the middle areas – and therefore the remaining tails (remember our “area under the curve = 100%” fact?) – for any point in our data set. By hand, we can get the middle area from the chart, subtract it from 100% to get the remaining area in both tails, and divide by 2 (thank you curve symmetry!) to get the area of each tail. But that’s not what we’re here to learn: how do we do it in R? 10.3 The _norm() Functions There are four distinct functions that involve the normal approximation in R: dnorm() returns the output of something called a density function, which is the equation that produces the normal curve. It needs one argument (x), and plugs it into the density equation. By default, the function’s mean and sd arguments are set to be 0 and 1 respectively, however you can override these defaults to be accurate to your data as needed. If x is a vector of numbers (i.e. -3:3), it will return the density function’s output for each number in the vector (thank you vectorization!). This function isn’t incredibly useful in computations, but it’s really useful when you need to plot a normal curve in R. pnorm() returns the cumulative probability of the normal curve at a given Z-score (It’s the area to the left of Z). Graphically, at an arbitrary Z-value, it returns the blue shaded area seen here: Graphical Example of pnorm() Output This is the one that we’re going to want to use the most, but we have to modify it a little bit to reproduce both what we’ve learned in class and from the chart. This will appear later (and make your life easier too), but for now we need to adapt it to find middle areas. Luckily, it’s not a hard conversion, and we’ll make use of our two facts from before. To get the middle area, we first need to realize that the upper bound of it (the right side on the normal curve) will always have a positive Z-score. Consequently, the lower bound will always have a negative Z-score. The symmetry of the curve tells us that Z-score on the right has to be the same (but opposite sign) as the Z-score on the left. This makes our calculation of the middle area easy: pnorm(positive z-score) - pnorm(negative z-score). If you want this value to be a percentage and exactly match the chart, take this output and multiply it by 100. Then, to get the tails, you simply take 100% (or 1 if you’re using the direct output) and subtract away the middle area, divide by two, and you’ve got everything you need. qnorm() does the opposite of pnorm(): you supply it an area-to-the-left (out of 1) for which you’d like to know the corresponding Z-score, and it tells you what that Z-score is. For example, if we wanted to know what area gave us 95% to the left (or a 5% tail) quickly, we can find it with one line of code. qnorm(.95) [1] 1.644854 Do a quick check by hand with the chart, and you should see that the middle area of 1.65 does in fact give us an upper (and lower) tail of 5%. rnorm() is basically a random number generator. (The numbers are actually pseudo-random, but the patterns that they come from are not obvious, so we consider them random.) For this class, it won’t be particularly useful, but the more you learn in statistics and R the more useful this function will become. We may use it from time to time going forward to help us do a few things, so it’s worth mentioning now. Also worth mentioning is that this function won’t produce the same output every time you run it since, after all, it is a random number generator. To ensure reproducability of your code, it’s a good habit to set the seed, or starting value, of the random number generator with set.seed(). By supplying it an argument of any integer, it generates that many random numbers from a normal distribution. mean and sd again default to 0 and 1 respectively, but you can override these if you’d like. rnorm(3) [1] -0.2423682 -0.5664037 -0.2053644 rnorm(3) [1] 0.6296777 -1.0787336 -1.7275177 rnorm(3) [1] -1.2508582 1.8306712 0.5032078 Now, if we set a seed, we should get the same results every time as long as we set the seed every time we want the same numbers. set.seed(123456789) rnorm(3) [1] 0.5048723 0.3958758 1.4155378 # This one should give different numbers rnorm(3) [1] -0.7223243 -0.6183570 -1.5626204 # Back to the same numbers set.seed(123456789) rnorm(3) [1] 0.5048723 0.3958758 1.4155378 # One more time set.seed(123456789) rnorm(3) [1] 0.5048723 0.3958758 1.4155378 "],
["percentiles-and-box-plots.html", "Chapter 11 Percentiles and Box Plots 11.1 Percentiles 11.2 Quartiles 11.3 Box Plots and boxplot() 11.4 Summary and summary()", " Chapter 11 Percentiles and Box Plots 11.1 Percentiles Normal curves, averages, standard deviations, and Z-scores may seem like they provide all the in information needed to understand a simple set of data. But what about when it doesn’t? Take for example a standardized test. We may not necessarily care what percentage a particular student got on the exam, but we’re more likely concerned with comparing that student with the rest of their peers. Are they ahead? Are they behind? Luckily, we can make use of percentiles to help answer these types of questions. What is a percentile? It’s the area to the left of a given Z-score, or the percentage of data less than the one you’re examining. In other words, being in the \\(n\\)th percentile means having a Z-score such that \\(n\\)% of the area is to our left. In our standardized test example, it’s the percentage of people you scored better than. To calculate a percentile by hand, we’d first find the Z-score, then get the corresponding middle areas from our chart, compute the tails, and add the area to the left of our calculated Z-score. Graphically, it looks something like this: Wait a second… We’ve seen this graph before! Where? Oh right, when we talked about the pnorm() function and its output. In fact, this is the output of pnorm(), so appropriately using the pnorm() function will quickly calculate the percentile for us. Pro tip about percentiles: the Z-scores for opposite percentiles (i.e. for the 5th and 95th percentiles) are the same sign but opposite magnitude. To check this, we can pick an arbitrary Z-score (say, 1.7), calculate that percentile, and add the percentile of Z = -1.7, and the results should be 100%. Let’s check: 100 * (pnorm(1.7) + pnorm(-1.7)) [1] 100 11.2 Quartiles There’s a few “special” percentiles that we like to use a lot: the 25th, 50th, and 75th percentiles. These are what we call quartiles. As you can see, these quartiles are each quarter of the way across the normal curve. We usually refer to them as Q1, Q2, and Q3 respectively. The interquartile range, or IQR for short, is defined to be Q3 - Q1. We use the IQR to determine if a data point is an outlier. Outliers come in two forms: lower outliers and upper outliers. To check if a point is a lower outlier, it must have a value that is less than Q1 - 1.5 \\(\\cdot\\) IQR. For upper outliers, a point must have a value of Q3 + 1.5 \\(\\cdot\\) IQR. 11.3 Box Plots and boxplot() Quartiles are especially useful when we want to visualize our data in a different way than a histogram or normal curve. We can employ something called a box plot, which visually shows us a summary of our data. Let’s take a look at one and how all its parts fit together. We’ll use the combined survey results from Survey 2 from Fall 2017 to make a histogram of the texts variable. You can download the data here and find the data description here. We could also draw the same plot horizontally. It’s good to be able to analyze a box plot in both ways To draw these box plots, we just make use of the boxplot() function. If you only wish to view a box plot for a given set of numbers (like we did above), you just need to supply the name of the vector that the data is contained in. R takes care of all of the computations and plotting for you, but you’re more than welcome to play with the colors and labels as you see fit. Box plots, however, are also useful at comparing groups. For example, if we wanted to split our data up according to the gender variable in the dataset, box plots will easily illustrate the differences between the genders. The boxplot() function handles this easily by making use of something called formula syntax. It reads as y ~ x, where y is the variable you want on the \\(y\\)-axis, the ~ means “on” or “versus”, and the x is the variable you’d like on the \\(x\\)-axis. Note: the survey data has been imported as a data frame called survey2. boxplot( survey2$socialMedia ~ survey2$gender, xlab = &#39;Gender&#39;, ylab = &#39;Hours Spent on Social Media&#39;, main = &#39;Male vs. Female Social Media Usage&#39; ) As you can see from this plot, females tend to spend more time on social media than males (see the higher Q2 bar?), although the middle 50% – our good friend the IQR – of each gender is within 3 hours of each other. For females, the IQR goes from 2 to 5, and for males, it goes from 1 to 4. 11.4 Summary and summary() We’ve now got a wide variety of statistics – mean, median, minimum, maximum, Q1 and Q3 – we know how to use and calculate both in R and by hand, but wouldn’t it be nice if there was a way to quickly calculate all of these functions for multiple (numeric) variables in our data frames all at once? Well, guess what? There is! It’s the summary() command, and it does exactly that: provides a summary of the data. Here’s the summary of survey2. summary(survey2) gender genderID greek Length:1575 Length:1575 Length:1575 Class :character Class :character Class :character Mode :character Mode :character Mode :character homeTown ethnicity religion religious Length:1575 Length:1575 Length:1575 Min. : 0.000 Class :character Class :character Class :character 1st Qu.: 1.000 Mode :character Mode :character Mode :character Median : 4.000 Mean : 3.882 3rd Qu.: 6.000 Max. :10.000 likeMath calculus ACT GPA Min. : 0.000 Length:1575 Min. :12.00 Min. :1.000 1st Qu.: 3.000 Class :character 1st Qu.:25.00 1st Qu.:3.200 Median : 5.000 Mode :character Median :28.00 Median :3.600 Mean : 5.144 Mean :27.65 Mean :3.446 3rd Qu.: 7.000 3rd Qu.:31.00 3rd Qu.:3.800 Max. :10.000 Max. :36.00 Max. :4.000 partyHr drinks sexPartners relationships Min. : 0.000 Min. : 0.000 Min. : 0.000 Min. : 0.000 1st Qu.: 0.000 1st Qu.: 0.000 1st Qu.: 0.000 1st Qu.: 0.000 Median : 4.000 Median : 2.000 Median : 1.000 Median : 1.000 Mean : 5.426 Mean : 6.554 Mean : 2.924 Mean : 1.241 3rd Qu.: 8.000 3rd Qu.:10.000 3rd Qu.: 3.000 3rd Qu.: 2.000 Max. :50.000 Max. :50.000 Max. :50.000 Max. :25.000 callParents socialMedia texts goodOrWell Min. : 0.000 Min. : 0.000 Min. : 0.000 Min. : 0.000 1st Qu.: 1.000 1st Qu.: 2.000 1st Qu.: 3.000 1st Qu.: 5.000 Median : 3.000 Median : 3.000 Median : 5.000 Median : 5.000 Mean : 4.435 Mean : 3.352 Mean : 5.627 Mean : 5.554 3rd Qu.: 5.500 3rd Qu.: 4.000 3rd Qu.: 7.000 3rd Qu.: 7.000 Max. :50.000 Max. :10.000 Max. :50.000 Max. :10.000 expectedIncome president DACA liberal Min. : 1 Length:1575 Min. : 0.000 Min. : 0.000 1st Qu.: 80 Class :character 1st Qu.: 0.000 1st Qu.: 2.000 Median : 100 Mode :character Median : 0.000 Median : 4.000 Mean : 1815 Mean : 2.029 Mean : 3.963 3rd Qu.: 200 3rd Qu.: 4.500 3rd Qu.: 5.000 Max. :99999 Max. :10.000 Max. :10.000 politicalParty gradeVsLearning parentRelationship workHr Length:1575 Min. : 0.000 Min. : 0.000 Min. : 0.000 Class :character 1st Qu.: 4.000 1st Qu.: 7.000 1st Qu.: 0.000 Mode :character Median : 5.000 Median : 8.000 Median : 0.000 Mean : 4.723 Mean : 7.733 Mean : 5.435 3rd Qu.: 6.000 3rd Qu.: 9.000 3rd Qu.:10.000 Max. :10.000 Max. :10.000 Max. :50.000 tuition career Min. : 0.00 Min. : 0.000 1st Qu.: 20.00 1st Qu.: 4.000 Median : 90.00 Median : 7.000 Mean : 64.93 Mean : 5.988 3rd Qu.:100.00 3rd Qu.: 8.000 Max. :100.00 Max. :10.000 There you have it! A quick, easy way to get the information you need about the data that you care about. "],
["scatter-plots-and-correlation.html", "Chapter 12 Scatter Plots and Correlation 12.1 Scatter Plots 12.2 Correlation 12.3 Calculating \\(r\\) and cor() 12.4 Statistics of the “Cloud” Scatter Plot 12.5 The SD Line 12.6 Subsetting and Ecological Correlations 12.7 Summary of Correlation", " Chapter 12 Scatter Plots and Correlation 12.1 Scatter Plots Everything we’ve done to this point has examined one variable of a data set, or things that could be represented by a single vector. While this helps us to understand that one particular variable, it’s much more interesting to us to examine how variables relate to one another. One way to visualize how they relate is through a scatter plot. A scatter plot puts one variable – an independent variable, or predictor, on the \\(x\\)-axis, and a second variable – the response, or dependent variable, on the \\(y\\)-axis. Usually, we’re trying to show how the independent variable explains the dependent variable. Say, for example, we’re trying to find a relationship between a student’s midterm exam score and their final exam score. Let’s have a small class of seven students, with midterm and final scores according to the following table. Midterm Final Student 1 55 62 Student 2 60 50 Student 3 70 65 Student 4 80 70 Student 5 85 95 Student 6 90 80 Student 7 100 90 It’s kind of hard to tell the general trend of the data from just looking at the table, so let’s plot the points. We’ll make the \\(x\\)-axis the midterm scores and the \\(y\\)-axis as the final scores. Note: we have the midterm and final scores stored in a data frame called test_scores. Pro tip: If you’re ever not sure which variable goes where, think about which variable you’d try in predict. In this example, we’re trying to predict a final score from a midterm score, so the final should go on the \\(y\\)-axis. To make the scatter plot, we’ll use the plot function (see ?plot for more information) and make use of the forumla syntax we discussed before. Now, however, we can write it as Dependent variable ~ independent variable. plot(Final ~ Midterm, data = test_scores, pch = 16, # Makes points into closed dots col = &#39;#0088ce&#39;, xlab = &#39;Midterm&#39;, ylab = &#39;Final&#39;, main = &#39;Final vs. Midterm&#39;, cex.axis = 1.5, # Controls font size of axis numbers cex.main = 1.5, # Controls font size of title labels cex.lab = 1.5, # Controls font size of axis label labels cex = 1.5 # Controls size of points ) Now it’s much easier to see! Typically, the better a student did on the midterm (further to the right on the \\(x\\)), the better they did on the final (higher up on the \\(y\\)-axis). We can conclude that an increase in midterm scores is then associated with an increased final score. This is called a positive association. If instead increasing \\(x\\) meant decreasing \\(y\\), we’d call this a negative association. 12.2 Correlation This is great, but it’s kind of general to just talk about associations. How do we tell if an association is strong or not? This is where the idea of correlation comes in. Correlation measures how closely the points follow a line, and they can be summarized by the correlation coefficient, \\(r\\). It does not measure points that are clustered around a curve. A good rule of thumb is that if the data is roughly football shaped, you can use \\(r\\). If the points fall perfectly on a line and are negatively associated, \\(r\\) = -1. If the points fall perfectly on a line and are positively associated, \\(r\\) = +1. If there’s no association between the independent and dependent variables, the correlation is 0. In other words, a correlation of $r = $1 means that you can know exactly what \\(y\\) is for any given \\(x\\) value. Examples are converting units (i.e. temperature from Fahrenheit to Celsius or vice versa) or anything that’s described by a line (i.e. the \\(x\\) and \\(y\\) values from the equation \\(y = 6x + 9\\)). Things that aren’t related, such as weight of college freshmen and ACT scores or class attendance and number of pets you own, have a correlation coefficient of \\(r\\) = 0. 12.3 Calculating \\(r\\) and cor() To calculate the correlation coefficient, just follow a simple 3-step process. Convert both \\(x\\) and \\(y\\) to Z-scores Multiply the Z-scores of \\(x\\) and \\(y\\) together Take the average of the products you just found in step 2 Like calculating a standard deviation by hand, it’s easy to see (and do) the calculation of a \\(r\\) in a table. Let’s make a small data set of 4 students’ scores on quiz 1 and quiz 2. Quiz 1 Quiz 2 Z\\(_\\text{Quiz 1}\\) Z\\(_\\text{Quiz 2}\\) Z\\(_\\text{Quiz 1} \\cdot\\) Z\\(_\\text{Quiz 2}\\) Student 1 10 10 1.2 1.3 1.5 Student 2 9 7 0.8 -0.6 -0.5 Student 3 5 9 -0.8 0.6 -0.5 Student 4 4 6 -1.2 -1.3 1.5 Averaging the last column, Z\\(_\\text{Quiz 1} \\cdot\\) Z\\(_\\text{Quiz 2}\\), we get that the correlation coefficient is 0.5. Of course, R can calculate this for us. We can get \\(r\\) through the cor() function, which just needs the two vectors for which you’d like to calculate the correlation. quiz1 = c(10, 9, 5, 4) quiz2 = c(10, 7, 9, 6) cor(quiz1, quiz2) [1] 0.5 12.4 Statistics of the “Cloud” Scatter Plot When our data is roughly football-shaped (like we see below), there are five statistics, called the summary statistics, that we’ll want to pay attention to. The Avg\\(_x\\) is the average (mean) of the variable on the \\(x\\)-axis The Avg\\(_y\\) is the average of the variable on the \\(y\\)-axis The point (Avg\\(_x\\), Avg\\(_y\\)) is referred to as the point of averages The SD\\(_x\\) is the standard deviation of the variable on the \\(x\\)-axis The SD\\(_y\\) is the standard deviation of the variable on the \\(y\\)-axis The correlation coefficient, \\(r\\), which describes how closely \\(x\\) and \\(y\\) follow a line These statistics are usually given to you in class, but you’ll have to calculate them yourself or have R calculate them on any data set in practice! We’ll use the data from Survey 1 (loaded as survey1) as we did before, and focus on the height and weight variables, since these are probably correlated. We’ll make use of the stdv() function we wrote before. (means = c(mean(survey1$height), mean(survey1$weight))) [1] 66.89189 147.19165 (stdvs = c(stdv(survey1$height), stdv(survey1$weight))) [1] 4.148278 34.183133 (cor = cor(survey1$height, survey1$weight)) [1] 0.5676085 Pro tip: enclosing code in () while making an assignment both assigns the variable and displays the calculation Reformatting this information into a table, we’d summarize it as Summary Statistics for height and weight from survey1 Average Standard Deviation height 66.9 4.1 \\(r\\) = 0.6 weight 147.2 34.2 One last step before we start using this information: let’s plot what this looks like so we can see what the data looks like. Let’s let our \\(x\\)-axis be height and our \\(y\\)-axis be weight. plot( weight ~ height, data = survey1, pch = 16, xlab = &#39;Height in Inches&#39;, ylab = &#39;Weight in Pounds&#39;, main = &#39;Height vs. Weight&#39;, col = &#39;#0088ce&#39; ) The data is shaped roughly like a football, so we can continue! First, let’s add in the point of averages that we talked about before. We’ll make that point grey, and show the intersecting lines on the plot with &lt;span style = ’color: #ffcc33&gt;gold and maroon. 12.5 The SD Line The SD line is the line that goes through the tips of the football, passing through the point of averages. Its slope is defined by \\[ \\text{Slope of SD Line} = \\begin{cases} \\frac{\\text{SD}_\\text{y}}{\\text{SD}_\\text{x}} &amp; r &gt; 0 \\\\ -\\frac{\\text{SD}_\\text{y}}{\\text{SD}_\\text{x}} &amp; r &lt; 0 \\end{cases} \\] In our example, the slope of the SD line is thus \\(\\frac{34.2}{4.1} =\\) 8.3. We can add this line to our plot with abline() function. abline() looks for arguments a (the intercept of the line) and b (the slope of the line). We’ve got the slope, but we just need to find the intercept. Some quick algebra gives us the following, assuming we know the slope, average in \\(x\\), and average in \\(y\\). y = slope \\(\\cdot\\) x + intercept Plugging in the fact that the SD line has to go through the point of averages (i.e. the point (Avg\\(_\\text{x}\\), Avg\\(_\\text{y}\\)) has to satisfy the equation we’re looking for), we can plug in (Avg\\(_\\text{x}\\), Avg\\(_\\text{y}\\)) for x and y respectively. Avg\\(_\\text{y}\\) = slope \\(\\cdot\\) Avg\\(_\\text{x}\\) + intercept Avg\\(_\\text{y}\\) - slope \\(\\cdot\\) Avg\\(_\\text{x}\\) = intercept In our case, this gives that 147.2 - (8.3 \\(\\cdot\\) 140) = -404.9. So adding the following line to our plot abline(a = -404.9, b = 8.2) will add the SD line to our plot. 12.6 Subsetting and Ecological Correlations Subsetting Sometimes, we may think that it’s useful to summarize data into groups, then talk about the statistics of that group. For example, we may want to compare how different ethnicities in class scored on their midterms. In this case, we don’t necessarily care about how well each student did, but we’re interested in how groups of them did. This is easy to see with this data from Bonus Survey 4. We’ll import it as survey4, and you can find the description of the dataset here. survey4 = read.csv(&#39;data/Combined Fall 2017 Survey 4.csv&#39;) The ethnicity variable is the one that we’d like to group our data by. ethnicity in this data set can be any one of the following: Black East Asian Hispanic South Asian White Other We have a few options of how to group, or subset, the data, some of which are more memory-intensive than others. Method one is to use the subset() function. If, for example, we wanted to only look at East Asian students’ performance on exam 1, we could write something like east_asian = subset(survey4, survey4$ethnicity == &#39;East Asian&#39;) mean(east_asian$Exam1) [1] 89.60363 Note: We used a double equals ( == ) to evaluate the condition, and we passed the condition we were looking for as a string. This takes all of the data from survey4, finds which rows have ethnicity of 'East Asian' (again, matching case), and puts them into their own data frame called east_asian. One great perk of this method is that it’s very easy to keep track of things, since you can then call on parts of the new, subsetted data frame just like you would have on the bigger data frame. Even the names of the features stay the same. However, this method can get fairly memory-intensive since it takes memory to store the data frame we created. The more subsets you’d like to have, the more memory you start to use up, and the slower your code may run. Method 2 is to take advantage of vectorization again. We can subset the data here by using [], $, and even do calculations, all in one step. Let’s try to get the same result as above, but this time we use vectorization. mean(survey4$Exam1[survey4$ethnicity == &#39;East Asian&#39;]) [1] 89.60363 If this is confusing, start from the inside and work your way out. First, we found the rows where ethnicity was 'East Asian'. Then, from these row numbers (indexes), we took the exam1 numbers, and lastly took the mean of them. As you can see, they produce the same result, but this one uses less memory and less lines of code. Pretty cool! The last point we should make about subsetting is that using [] is equivalent to subset(). If we wanted to make the same subset using [], we could do something like this. Just so we don’t overwrite east_asian, we’ll call this one e_a, and use the identical() function to check if they’re the same. east_asian = subset(survey4, survey4$ethnicity == &#39;East Asian&#39;) e_a = survey4[survey4$ethnicity == &#39;East Asian&#39;, ] identical(east_asian, e_a) [1] TRUE Note: in creating e_a , we had to include the , so that we could get all rows. If you wanted only certain columns, you could put something after the , to get only those columns. Ecological Correrlations Now that we know how to restrict data by conditions, we can go back to our original question of representing a bunch of data by the groups they belong to. If we wanted to visualize the original data by ethnicity, one option we have is to color the points accordingly. Remember, we’re interested in the relationship between scores on exams 1 and 2. If we treat each student as a unique data point, the data has correlation 0.7 plot(Exam2 ~ Exam1, data = survey4, xlab = &#39;Exam 1 Score&#39;, ylab = &#39;Exam 2 Score&#39;, main = &#39;Exam 1 and 2 Scores for All Students&#39;, col = survey4$ethnicity, pch = 16, cex = .8 ) legend(x = 60, y = 35, legend = unique(survey4$ethnicity), col = unique(survey4$ethnicity), pch = 16 ) Another option we have, however, is to condense the plot to be the averages of each ethnicity for each exam. exam_1_means = c(mean(survey4$Exam1[survey4$ethnicity == &#39;Black&#39;]), mean(survey4$Exam1[survey4$ethnicity == &#39;East Asian&#39;]), mean(survey4$Exam1[survey4$ethnicity == &#39;Hispanic&#39;]), mean(survey4$Exam1[survey4$ethnicity == &#39;South Asian&#39;]), mean(survey4$Exam1[survey4$ethnicity == &#39;White&#39;]), mean(survey4$Exam1[survey4$ethnicity == &#39;Other&#39;])) exam_2_means = c(mean(survey4$Exam2[survey4$ethnicity == &#39;Black&#39;]), mean(survey4$Exam2[survey4$ethnicity == &#39;East Asian&#39;]), mean(survey4$Exam2[survey4$ethnicity == &#39;Hispanic&#39;]), mean(survey4$Exam2[survey4$ethnicity == &#39;South Asian&#39;]), mean(survey4$Exam2[survey4$ethnicity == &#39;White&#39;]), mean(survey4$Exam2[survey4$ethnicity == &#39;Other&#39;])) These have a higher correlation (\\(r\\) = 1) and visually look like they follow a line much closer. This makes sense: it takes two points to determine a line. The more points we have, the less they’ll fall on a line unless we know that they were generated by some rule or equation that directly ties them together. In subsetting our data, we went from 1518 data points to only 6. It’s much easier for 6 data points to fall on a line than it is for 1518. This is the effect of an ecological correlation. Higher correlation may seem better, but that’s not always true. By reducing each student to their ethnicity, we have lost a lot of data points from our data set (1512 to be exact). This makes it easy to talk about ethnicities, but we may lose the understanding of the performance of an individual in the class. 12.7 Summary of Correlation Some facts about \\(r\\) to keep in mind: Correlation is not causation \\(r\\) is unitless, since when switching to Z-scores, the data loses its units \\(r\\) does not change by doing any of the following: Adding/subtracting the same number from all values of \\(x\\) and/or \\(y\\) Multiplying all values of \\(x\\) and/or \\(y\\) by the same number. Note: multiplying either \\(x\\) or \\(y\\) (but not both) by a negative number changes the sign of \\(r\\) Changing units of the original data (see fact 2) Switching all of the \\(x\\) values with all of the \\(y\\) values If you don’t believe any of these facts, go ahead and prove them to yourself, or come in to office hours and ask a TA to help you! "],
["linear-regression.html", "Chapter 13 Linear Regression 13.1 Regression 13.2 Using \\(r\\) to Make Predictions 13.3 The Regression Line 13.4 lm() 13.5 Using predict() to Make Predictions 13.6 Switching the Predictions", " Chapter 13 Linear Regression Statistics tries to do two things: Explain the trends we observe within data Make predictions on new data based on the trends we see So far, much of what we’ve covered goes to objective #1. Let’s start working on the second part and using the trends we’ve observed to make predictions. 13.1 Regression Regression is the process of using a relationship between two or more variables (the predictor(s) and response variables) to make a better prediction of the response than guessing. We’re focused on what’s called simple linear regression (SLR), which just means doing regression with one predictor. Knowing the correlation between two variables definitely helps make a better prediction since we know the trend between the two variables, but it doesn’t really help us make a prediction… Yet. Now it’s time to put \\(r\\) to use. What the correlation coefficient really tells us is how much, on average, how many standard deviations of the response we move up (or down) when we increase by one standard deviation in the predictor. Admittedly, that’s confusing to type and involves a lot of math words, and it’s probably easier to understand with an example anyways, so we’ll go right to one. Pretend we’re looking at the relationship between your height and your shoe size. This has a positive correlation (think about it: an NBA player will have a bigger foot than a toddler), and since we’re making this example up, let’s say that the correlation is \\(r\\) = .7. The average shoe size (Avg\\(_\\text{y}\\)) is 10, with standard deviation of .5, and the average height (Avg\\(_\\text{x}\\)) in our data set is 69&quot; with standard deviation 3“. Average Standard Deviation Shoe Size 10 0.5 \\(r\\) = 0.7 Height 69 4.0 What \\(r\\) tells us here is that, for every 4 additional inches taller someone is, we’d expect their feet to be .7 \\(\\cdot\\) .5 sizes bigger on average. This should make some sense with other things we know too: if \\(r\\) was \\(\\pm\\) 1, we know exactly what size shoe you’d wear based on your height every time. If the correlation were 0, we’d have no idea what your shoe size would be so we’d guess the average shoe size. When it’s in between, we don’t know for sure, so we can use \\(r\\) to improve our guess to be somewhere in the middle. As you may have noticed, our guess is always going to be moving closer to the average. This effect, where bottom groups are expected to perform better and top groups are expected to perform worse, is called the regression effect. “This is all great, but I just want to know how to use it to make predictions.” You (we’re guessing) 13.2 Using \\(r\\) to Make Predictions One way to make a regression prediction is to start by converting the \\(x\\) value at which you’re trying to predict be a Z-score. Take this Z-score, multiply it by \\(r\\), and boom! You’ve got your Z-score for \\(y\\). Finally, rearrange the Z-score formula to solve for the Value. This is your regression estimate. Going back to our example above, we can illustrate in a table. We’ll see what the predicted shoe size is for someone that’s 73&quot; tall. Height Z\\(_\\text{Height}\\) \\(r\\) Z\\(_\\text{Shoe Size}\\) Shoe Size 73 1 0.7 0.7 10.35 Like we outlined above: take the first column and convert to a Z-score, then put it in column 2. Multiply the Z-score in column 2 and multiply by \\(r\\), which is in column 3. The product is the new Z-score, which we put in column 4, and then we convert back to a value. We’d predict that someone that’s 73&quot; tall would have a size 10.35 shoe. This process does take a while to complete, however. It’s a multi-step procedure with a lot of switching back and forth between Z-scores and values. It’s pretty easy to make a mistake doing this, so let’s introduce another way to do regression. 13.3 The Regression Line Like we said above, \\(r\\) tells us how much \\(y\\) changes (in terms of standard deviations) for a 1-SD change in \\(x\\). Hopefully, this is screaming “IT’S A SLOPE!” loud and in your face, but in case it isn’t, this is the definition of a slope! Slopes are a characteristic of a line, so we can create a regression line to make predictions quickly and easily for every point we could want. We typically describe lines as taking the form \\[ y = m \\cdot x + b \\] where \\(x\\) is the value we’re making the prediction at, \\(m\\) is the slope of the line (how much \\(y\\) changes for each 1-unit increase in \\(x\\)), and \\(b\\) is the intercept (the predicted \\(y\\) value when \\(x\\) is 0). With what we just realized about \\(r\\), we can define the slope of our new friend – the regression line – to be \\[ r \\cdot \\frac{\\text{SD}_\\text{y}}{\\text{SD}_\\text{x}} \\] Great! Now all we need to do is figure out the intercept of the equation, and we’ll be on auto-pilot for predictions. Luckily, this isn’t hard to do. Just like the SD line, the regression line also has to go through the point of averages. Since this point falls on the line, its \\(x\\) and \\(y\\) values are great to plug in. We then know \\(y\\), \\(m\\), and \\(x\\), which leaves us with one equation with one unknown variable. Thus, we can solve for \\(b\\) and be on our merry way. Let’s try it with the example from above. We’re trying to predict shoe size from height, so our equation should take the form \\[ \\text{Shoe size} = \\left( r \\cdot \\frac{\\text{SD}_\\text{Shoe size}}{\\text{SD}_\\text{Height}} \\right) \\cdot \\text{Height} + \\text{intercept} \\] \\(r\\) was .7, and the SDs for shoe size and height were 4 and .5 respectively. \\(.7 \\cdot \\frac{.5}{4} = 0.0875\\), so this is our slope. We can get the intercept by plugging in 10 for shoe size and 69 for height. \\[ 10 = .0875 \\cdot 69 + \\text{intercept} \\\\ 10 - \\left( .0875 \\cdot 69 \\right) = \\text{intercept} \\\\ \\implies \\text{intercept} = 3.9625 \\] Awesome! Now, let’s make our prediction for a 73&quot; tall person. If everything worked out correctly, we should get 10.35 just like before. (.0875 * 73) + 3.9625 [1] 10.35 13.4 lm() While R does make computing the summary statistics for a data set very easy with mean(), sd() (with our modification, of course), and cor(), these can get frustrating to keep typing over and over to get regression equations. This is where the lm() function comes up clutch. lm() stands for linear model, which is exactly what we’ve just created. Essentially, this function allows us to create regression equations quickly and easily, while having additional functionality as well. Part of the reason that we create a model is to be able to use it to make predictions. In order to use it, however, it’s probably a good idea to store the model as a variable. Assignment works the same way it always has. To create a linear model, use formula syntax to specify which variables you’d like as your predictor and response. Note: you could create the model by specifying the predictor (i.e. a variable called predictor) and response (i.e. a variable called response from a data frame called df as lm(df$response ~ df$predictor), however it’s much cleaner and clearer to specify the same thing as lm(response ~ predictor, data = df) . Let’s take our example that we did last chapter one step further. As a refresher, here’s what we were dealing with. Average Standard Deviation height 66.9 4.1 \\(r\\) = 0.6 weight 147.2 34.2 Graphically, we had the following going on. The black line is the SD line, the vertical and horizontal lines are the averages in height and weight respectively, and the grey dot where the lines overlap is the point of averages. As a quick practice round, let’s get the slope and intercept by hand first. \\[ r \\cdot \\frac{\\text{SD}_\\text{weight}}{\\text{SD}_\\text{height}} = 5.00 \\\\ 147.2 = \\left( 5.00 \\cdot 66.9 \\right) + \\text{intercept} \\implies \\text{intercept} = -187.63 \\] Awesome, but annoying to calculate by hand. Let’s make use of lm() and get the same results straight away. We’ll store our model as mod1 so we can refer back to it, and we’ll follow the good coding style we just outlined a minute ago. mod1 = lm(weight ~ height, data = survey1) To see what the model determined for the slope and intercept, we can use the coef() function, and just supply the name of our model to it. coef(mod1) (Intercept) height -165.680120 4.677275 Hmmm… this is close to what we got, but it’s not exactly the same. Why is that? Easy: rounding. We’ve hidden some unecessary code throughout the book that’s rounded many results for us, but if we wanted to really calculate what the values would be, we’d do this. Note: The stdv() is the user-defined function for standard deviation we wrote in chapter 9. # Get summary statistics average_weight = mean(survey1$weight) average_height = mean(survey1$height) sd_weight = stdv(survey1$weight) sd_height = stdv(survey1$height) r = cor(survey1$weight, survey1$height) # Get slope and intercept slope = r * (sd_weight / sd_height) intercept = average_weight - (slope * average_height) c(intercept, slope) [1] -165.680120 4.677275 Since we know now that we’ll get the same result, let’s just keep going with mod1. We can add it to our plot with the abline() function. coef returns a vector, so we can use [] to access its elements. plot(weight ~ height, data = survey1, pch = 16, xlab = &#39;Height in Inches&#39;, ylab = &#39;Weight in Pounds&#39;, main = &#39;Height vs. Weight&#39; ) abline(a = coef(mod1)[1], b = coef(mod1)[2], col = &#39;#e04e39&#39;, lwd = 1.5, lty = 2 ) 13.5 Using predict() to Make Predictions Rather than having to convert to Z-scores to make predictions, we can use the regression line to make predictions. By hand, we’d take our value for \\(x\\) (height in our example), multiply by the slope, add the intercept, and then we’d have our predicted value of \\(y\\). Note: To distinguish the original values in our dataset from our predictions, we often call our predictions \\(\\hat{y}\\). In R, the predict() function allows us to make predictions quickly and easily. To use it, start with the model you’d like to use, followed by passing a data frame of the predictor(s) to predict()’s newdata argument. In the newdata argument, we can either create a data frame with data.frame(), or we can put the name of an already existing data frame. If you’re going to make your own data frame, however, make sure that you specify the names of the values you’re putting in as they were when the model was fit. If we wanted to use mod1 to make a prediction for the weight of a 72&quot; person, we can make use of predict() like this: predict(mod1, newdata = data.frame(height = 72)) 1 171.0837 Note: With SLR like we’re doing, it may seem a little annoying to put it into a data frame, but when the problem involves multiple predictors, this can be much easier than typing out the whole calculation by hand. Don’t worry about that case though; that’s a problem for another day. 13.6 Switching the Predictions In the example above, we predicted someone’s weight from their height using two different methods: by using \\(r\\), and by finding the regression equation. Instead, what if we wanted to predict someone’s height from their weight? Would these methods still apply? In short, kind of. Method 1 (using \\(r\\)) works identically, since it’s just based off of the Z-scores of each variable. Just do the exact same process backwards: convert the weight to a Z-score, multiply by \\(r\\) to get the Z-score for height, then convert back to a value. Easy peasy. However, for Method 2, your \\(x\\) and \\(y\\) variables changed. This means that your slope changed, and your intercept changed as well. We would have to repeat the entire process of finding the slope and intercept, then plug in new values to make our predictions. The thing to remember here is that an equation to predict \\(y\\) from \\(x\\) is not the same as the equation to predict \\(x\\) from \\(y\\). "],
["evaluating-predictions.html", "Chapter 14 Evaluating Predictions 14.1 Residuals 14.2 Root Mean Squared Error (RMSE)", " Chapter 14 Evaluating Predictions 14.1 Residuals Not every prediction is going to be perfect. That’s actually built into the fabric of regression! What regression is really doing is predicting the average \\(y\\) for each value of \\(x\\). Have a quick look at the plot from the last chapter. Each of the horizontal lines represents the average of the bin (defined by the dotted, vertical lines) and the regression line is plotted in grey. As you can see, the line predicts the average value for a group of data (for the majority of it, anyways). However, obviously not every point is located right at that group’s average value. This means we’ll have some error in our prediction, called a residual. We calculate a residual to be the actual value minus the predicted value. Points that are above their predicted value (i.e. people that are heavier than we predict them to be) have positive residuals. Points that are below their predicted value have negative residuals. Points that fall on the line, or are exactly their predicted value, have no residual. Under the hood, when we fit the regression line with lm(), R knows to try and minimize the residual value of each point. In fact, the line finds a way to make the residuals sum (and therefore average) to be 0. 14.2 Root Mean Squared Error (RMSE) So, if the average of each \\(x\\) value is really what the regression line is predicting, there’s probably some spread around that prediction. Exactly right! The standard deviation of the prediction errors, or root mean squared error (RMSE) is what you’re thinking of. Just like a standard deviation does with a normal curve, the RMSE lets us talk about a range of prediction errors and attach wiggle room to our predictions. To get the formula for this new RMSE thing, reverse the order of the words in the name. Start by finding the prediction error. That is, for each \\(y\\) value in our data (we’ll call it \\(y_i\\)), subtract off the predicted value. We called a prediction \\(\\hat{y}\\) earlier, so we’ll call each individual prediction \\(\\hat{y}_i\\) now. \\[ \\text{Prediction Error} = \\text{Actual} - \\text{Predicted} = y_i - \\hat{y}_i \\] Then, square each prediction error: \\[ \\left( y_i - \\hat{y}_i \\right) ^ 2 \\] Take the mean of these squared errors. We’ll assume that there’s \\(n\\) predictions (read: observations) in our data. \\[ \\frac{1}{n} \\sum_{i = 1} ^ n \\left( y_i - \\hat{y}_i \\right) ^ 2 \\] Finally, to get the RMSE, take the square root of this. \\[ \\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i = 1} ^ n \\left( y_i - \\hat{y}_i \\right) ^ 2} \\] While this may seem daunting, this is one way to calculate the RMSE. It’s easy to remember as long as you read the name in reverse order. However, there’s another way to calculate the RMSE that we talk about in class. \\[ \\text{RMSE} = \\sqrt{1 - r ^ 2} \\cdot \\text{SD}_y \\] They should give us the same number, but do they? We’ll find the RMSE of the data we’ve been working with to check. Remember, we’re predicting weight from height in survey1, so weight is our \\(y\\) variable. We’ll make use of R’s vectorization again to do our calculations. Make sure that you pay attention to where the parentheses go! # Method 1 sqrt(mean((survey1$weight - predict(mod1, newdata = survey1)) ^ 2 )) [1] 28.14292 # Method 2 sqrt(1 - cor(survey1$weight, survey1$height) ^ 2) * stdv(survey1$weight) [1] 28.14292 Pam, your thoughts? The same rules apply to the RMSE that applied to standard deviations. 68% of our data will fall within 1 RMSE of its predicted value, 95% will fall within 2 RMSEs of its predicted value, and 99% will fall within 3 RMSEs of what we predict. To see it graphically, look no further. "],
["part-doing-more-with-r.html", "Chapter 15 (PART) Doing More With R", " Chapter 15 (PART) Doing More With R "],
["uses-of-r.html", "Chapter 16 Uses of R", " Chapter 16 Uses of R Now’s a good time to change gears for a little. While we’ve done a lot already, it may seem like R is just good at helping with homework, speeding up calculations involving data. But it’s easy to make use of R when you’re instructed on how to do it and see specific use cases. At its core, R is great at computing statistics and making plots. After all, it was built by statisticians, for statisticians. But like any language, R has grown to be great for so much more than what you’ve seen. It has the power to manipulate entire datasets, create documents (like this entire book, your homework assignments, etc.), and even build websites and applications. We’ll cover how to do some of this in this section of the book. If you want to think of yourself as an R doctor, all of the patients (datasets) you’ve seen so far have come in healthy. You’ve learned how to find the information needed for their charts, and you should be very comfortable with how a healthy check-up should go. But what do you do when a patient comes in sick? They’ve got something wrong with them, and it’s your job to figure out what it is and how to fix it. This diagnosing process, as well as what we do to cure the patient, is known as data cleaning. What we mean is that, in practice, data sets that we’d like to analyze are rarely clean when we first get them. Values can be of the wrong type, be missing altogether, or not be in the formats we want them to be. No worries though! We’ll see how to use R to do all of this for us! Ready? Let’s ride. "]
]
