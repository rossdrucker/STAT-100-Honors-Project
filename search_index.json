[
["index.html", "STAT 100 Honors Project Intro About This Book Conventions For This Book", " STAT 100 Honors Project Ross Drucker Intro The who: This project is available to James Scholars interested in learning the statistical computing language R. Please sign up ASAP because this is a pilot project and enrollment is limited to 50 students. The what: Our James Scholar Honors Project teaches you to use the statistical software, R. Be prepared to spend approximately 2 hours per week on this project. The when: Sign up ASAP by getting a HCLA form from your dean/advisor. As soon as the paperwork goes through, you’re ready to go! The deadline to sign up is Monday, January 29th at 11:59pm. The how: The project consists of weekly notes to read (\\(\\approx\\) 1 hour) and weekly assignments to do on Lon-Capa (\\(\\approx\\) 1 hour). You need to complete all the assignments and turn them in before the due date each week. Late assignments will not be accepted on Lon-Capa, so make sure to keep a close eye on the calendar. All assignments are due on Sundays at 11:59pm. The first one is due Sunday February 4th, 2018. It will be posted on Friday January 19th. The where: No need to leave home and get out of your PJs for this one. All notes and assignments can be found online! The why: Why not!? It’s a great way to get your James Scholar credit, as well as to learn a useful program in the process. Knowledge of R is a very marketable skill. Also, the project is in sync with class. You’ll be able to see how what you learn in class can be done in R. This will help reinforce the concepts that you need to know to succeed in Stat 100 and give you an introduction to programming About This Book This book is meant to run parallel to the STAT 100 course notes. The hope is that STAT 100 students learn how the concepts they learned are put to use in modern statistical settings. Throughout, students will not only gain familiarity with the concepts in the notebook, but will also gain an understanding on how to use the R programming language to solve every-day problems in the real world. Conventions For This Book A few things to note throughout this book: Code that is written in monospace font refers to either RStudio keyboard shortcuts or R code, and it will be syntax-highlighted when appropriate 1 + 1 a = 1 b = 2 a + b sqrt(4) Output will begin with ## on the side Vocab words will appear like this throughout the text Helpful hints (“Pro tips”) will appear in this color and be in italics. They are usually shortcuts to use in R to help make code writing and editing easier to do Things to keep in mind or pay attention to will look like this. They’ll usually begin “Note:” "],
["syllabus.html", "Syllabus", " Syllabus Coordinator: Ross Drucker Email: ross.a.drucker@gmail.com Email Policy: Start your subject with [STAT 100 Honors Project]. Also provide a quick synopsis of what you’re emailing about. For example, a few subject lines could look like: [STAT 100 Honors Project] Homework 1 Problem 2 [STAT 100 Honors Project] Question about histograms Bad subject lines can look like this: HELP ME PLZ idk wut i’m doing 4 hw1 In your email, please include a description of the problem you’re having and any relevant code and/or error messages in the form of scripts or screenshots. Failure to do so may result in longer response times and/or no response altogether. You can send emails from any email address you have. Allow up to 24 hours for a response if you send an email Monday through Thursday. Over the weekend, response times may be longer. Please be patient. Office Hours: MWF from 2:00pm-5:00pm, TR from 3:30pm-5:00pm in 23 Illini Hall (CITES Computer Lab), or by appointment Website(s): General information about the project will be posted on Compass2g. Assignments will be posted on Lon-Capa under the James Scholar folder. The textbook can be found at https://rossdrucker.github.io/STAT-100-Honors-Project/ Project Objective: The goal of this project is to make students comfortable using the R programming language for a variety of uses. By the end, students should be able to perform basic calculations in R for both STAT 100 concepts and personal use. HCLA: In order to receive credit for this project, you must get an HCLA (either paper form or electronic) signed no later than Monday, January 29th at 11:59pm. Since the project is meant to run alongside class material, signups after this date will not be accepted. Topics (not in order; these may be updated throughout the semester): R’s interface and using R as a calculator Data types Descriptive statistics Regression and model creation Plotting, histograms, and visualization Functions and control structures Document rendering and scripting Coding Style Hypothesis testing Probability Grading Policy: - Since this is the first semester for this version of the honors project, there’s not a set number of assignments. There will most likely be around 8-12 assignments (due every 1 to 1.5 weeks) Each week, students should expect to spend 2-3 hours on readings and assignments All assignments will be posted on (and therefore submitted to) Lon-Capa. Point values will vary by assignment In order to receive credit for the project, you must complete all assignments with an (combined) average of 80% or higher Assignments will be available for one week after posting. No late assignments will be accepted Academic Integrity: All students are expected to abide by the campus regulations on academic integrity, which can be found here. These standards will be enforced and infractions of these rules will not be tolerated in this course. Such behavior includes sharing or copying any part of another student’s assignment or allowing another student to copy any part of your assignment. Changes: The coordinator reserves the right to change any of the information in this syllabus. It is the student’s responsibility to refer back to the syllabus to see pending changes. "],
["downloading-r-and-rstudio.html", "Chapter 1 Downloading R and RStudio 1.1 Getting Familiar with RStudio 1.2 Console 1.3 Source 1.4 Environment, History, Build, VCS, Presentation 1.5 Files, Plots, Packages, Help, Viewer", " Chapter 1 Downloading R and RStudio The first thing you’ll need to do in order to begin using R is to actually download R itself. You can do this from this link. It’s open-source, which means anyone can access and use it for free! Make sure that you install it for the type of operating system you have. That is, if you have a Windows computer, don’t download the Mac or Linux versions. This downloads what’s called a terminal, or the bare minimum interface that you need to write and run code in R. You’re not encouraged to use this terminal when writing code. “Wait, I thought you just said that this is how to write and run my code, and that’s what this project is.” — You (probably) This is because you should use RStudio to write, edit, and run your code. RStudio is what’s called an IDE (Integrated Development Environment), and it relies on what you just downloaded to run in the background to run properly. You can download RStudio here. It makes everything in R – from importing, manipulating, vizualizing, and analyzing data, to writing and debugging code, to creating documents (like this book!) – significantly easier to do. 1.1 Getting Familiar with RStudio Once you’ve got R and RStudio downloaded, open RStudio up and click RStudio in the menu across the top. Click Preferences and then click Pane layout so you can see what each pane in RStudio is. On my computer, I’ve got Source on the top left, Console on the top right, Environment, History, Build, VCS, Presentation on the bottom left, and Files, Plots, Packages, Help, Viewer on the bottom right. To me, this layout is easy to follow and understand, but you can set yours up however you want and can always change it around. Here’s what each of these panels actually does for you: 1.2 Console Remember that terminal you downloaded before downloading RStudio? This is what that looks like. Any time you see a &gt; symbol, you can enter commands. R will then run the command, and if output is produced, it will be displayed here. Do a few simple math problems here to see an example: 1 + 1 2 * 9 sqrt(4) While this may seem like a good place to write your code, it’s a better practice to do it in the Source pane in a script (more on that in approximately 2 sentences). Doing quick computations or installing and loading packages in your console is totally fine, but the more code you run, the harder it will be to find earlier lines of code (and more importantly, the harder it will be to edit should you make a mistake). At the end of the day, all of the code you write, whether it be in the Source pane or the Console pane will run in the Console, but for the purposes of legibility and editing you should do everything in your Source pane. 1.3 Source This pane is where you will do a majority of your work in what’s called a script. Think of a script like a Microsoft Word document, but instead of writing essays, you’re writing code. To open a new script, go to File, then select New File, then New R Script. On a Mac, you can press Cmd + Shift + N, or on a Windows/Linux computer, you can press Ctrl + Shift + N. The beauty of using a script is that you can save what you do and easily edit it later, whereas writing directly into the console will be much more difficult to save, edit, or view later. To run code that you write in a script, highlight the line(s) that you’d like to run and click Run at the top of the script. You can also hold Cmd + Enter (Mac) or Ctrl + Enter (PC). In addition to a script, you can also create a markdown document. Markdown documents allow you to combine plain text, code, and equations. We’ll cover this in more detail later, but these allow you to explain your analysis as you do it. Just to give you an idea of how powerful markdown documents are, this entire book was made using markdown documents. Lastly, this pane will be where the results from the View() command will display, and where you can view the actual contents of the datasets you’re working with. Again, more on this in a little bit. 1.4 Environment, History, Build, VCS, Presentation This pane is where you’ll be able to see all of the datasets that you’ve imported, variables that you’ve created, and functions that you’ve written. They are stored in something called your global environment, or environment for short. Think of your environment as a folder on your computer that keeps track of all of the files you’ve created. While none of that will probably mean much to you at the moment, they will in the span of a few chapters. The more code you write and the more variables you create, the harder it will be to remember all of these components. This pane keeps track of them, as well as basic information that you may find useful later. 1.5 Files, Plots, Packages, Help, Viewer Lastly, the Files, Plots, Packages, Help, Viewer pane will display any plots, graphs, or other images you create in R. Plots will display in this pane under the Plots tab. If they don’t pop up right away, try to increase the size of the pane by dragging the boundaries (like you’d resize your web browser). Markdown documents will be available in the Viewer tab, and help files will be in the Help tab. The Files tab shows the files in the directory (folder on your computer) that you’re working in. We’ll cover these in more detail later, but it’s good to become familiar with this pane as well. "],
["functions-and-data-types.html", "Chapter 2 Functions and Data Types 2.1 Functions 2.2 The Types 2.3 Vectors, Lists, and Data Frames 2.4 Coercing To Other Types 2.5 identical()", " Chapter 2 Functions and Data Types 2.1 Functions A function in R is a set of steps, operations, and procedures that are done to data in a specific order. R has some functions that are built into the language (many of which we’ll go through in this book), but you are also able to write your own functions as well. Functions take arguments, which are data points and other items that the function needs to do its job. Think of them like a variable, where you can change the value that they take each time you need to run the function. These can be anything from data points themselves, to colors and sizes for plots, to even other functions if necessary. After performing the steps and calculations that they’re supposed to do, they return, or give out, the information. Note: in R, arguments of functions may have pre-defined values. In this case, unless you specify differently when you call (use) the function, this pre-defined, or default, value will be used instead. To write your own functions, you need to make use of the function() function. You give your function a name, then specify its arguments as the arguments in function(), and include your steps inside of a set of curly braces ({}). To tell R what to return as the output of your new function, you have two options: you can either just leave it as the last line inside of the curly braces, or you can explicitly state it inside the parenthesis of return(). We’ll put a simple example of a user-defined function here to illustrate how simple and useful writing a function can be, although you may not completely understand what’s going on right now. And that’s okay, and honestly that’s expected at this point. We haven’t covered what’s going on here (it’s only Chapter 2!), but if we don’t introduce functions conceptually, we can’t refer to and/or write and teach them them as we go through the book. They’re helpful tools that can save you a ton of time as you get better in R. The function we’re going to build is called doubler. It will take one argument, x, and return whatever the double of x is. See if you can match the parts in this function to the process we just outlined and with the information about doubler() that we just gave you! doubler = function(x){ 2 * x } Then, to call the function, you simply put the name of the function, then without a space, put a set of parenthesis. Inside of these parenthesis, specify the arguments required to make the function run properly. Here’s an example of how to call the doubler() function we just wrote: doubler(x = 2) ## [1] 4 doubler(4) ## [1] 8 doubler(100) ## [1] 200 As you can see, x can be any number, and doubler() just takes the number (x) and doubles it. Pro tip: you don’t always need to specify the name of an argument. In the second example of using doubler(), R interprets 4 to be what x is supposed to be. When there’s more than one argument needed for a function, you can either give them in the same order the function looks for them (which you’ll learn about here), or you can specify them by name. 2.2 The Types Not all data is of the same type, or usage format. What this means is that different kinds of information from a dataset get evaluated differently in R. To check what type a piece of data is, you can use the class() function. Let’s go through a few of the most common types of data: numeric: Numeric data is data that is only numbers. These can be positive, negative, 0, decimals, or even infinity (\\(\\infty\\)) character: Character data is anything that involves a letter or special character. These will be denoted by '' (single quotation marks) or &quot;&quot; (double quotation marks). Characters are also called strings. It’s important to note that 2 is of type numeric, while '2' is of type character. A quick check using the class() function: class(2) ## [1] &quot;numeric&quot; class(&#39;2&#39;) ## [1] &quot;character&quot; logical: Logical data, also known as boolean data, is just a series of TRUE or FALSE values. While this may not necessarily seem like the most useful form of data right now, it’s important to know that this type of data exists. R evaluates T to be TRUE and F to be FALSE, so it’s equally valid to use T and F in place of TRUE and FALSE, but it’s better practice to use TRUE and FALSE since we may want to use T and F as variables. More on this in a little factor: Factor data is simply categories. This type of data is really useful for later when we want to split the information on variables such as gender, location, or a variety of other categorical features in the data NA: This isn’t actually its own type of data, but it represents a missing value. These can become pesky, but there are ways to work around them. We can choose to replace them with 0 or any other value we want, we can ignore them in our computations, or we can do something completely different with them altogether. The important thing to remember about NA values is that they exist and should be acknowledged. 2.3 Vectors, Lists, and Data Frames Each of the data types listed above describes a single point of data, called a scalar. However, we usually we don’t have data given to us as one-by-one pieces of information. We’re normally given whole datasets at a time, or at least groups of related data, and they’re much easier to work with. Vectors A vector is a grouped set of data. Think about it as if it were the answer to a single question from a survey from all students in the class, or the heights of all basketball players in the NBA. We’ve actually been working with vectors all along! We’ll discuss it more in chapter 4. R actually treats every value as a vector. That’s why, as you may have noticed, lines of output begin with [1]. This indicates the index (position) in the vector that is at the start of the line. Any time we’ve had any type of data, R has just treated it as a vector of length 1. One important thing to note about vectors is that all members of the vector must be of the same type. If they aren’t, they will be coerced (changed) to be of the same type. To create a vector, we can use the c() function. This function combines the elements (individual data points) and turns them into a vector. Separate the parts with a comma (,). Here’s a few examples: c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10) ## [1] 1 2 3 4 5 6 7 8 9 10 c(TRUE, FALSE, T, F, T, FALSE) ## [1] TRUE FALSE TRUE FALSE TRUE FALSE c(&#39;R&#39;, &#39;is&#39;, &#39;fun&#39;) ## [1] &quot;R&quot; &quot;is&quot; &quot;fun&quot; c(1, &#39;apple&#39;, 2, &#39;banana&#39;) ## [1] &quot;1&quot; &quot;apple&quot; &quot;2&quot; &quot;banana&quot; Note how in the last example, everything appears inside of double quotation marks. This is an example of coersion in action. 1 and 2 are recognized as type integer, and 'apple' and 'banana' are recognized as type character. Since it’s easier to change a number to a character than it is to go the other way, 1 and 2 become characters. It may seem cumbersome, time-consuming, and tedious to type out numbers in order as we did in the first vector. : to the rescue! Another way that we can create that vector is by putting the first number we’d like in our vector on the left side of the :, and the last number on the right. c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10) ## [1] 1 2 3 4 5 6 7 8 9 10 1:10 ## [1] 1 2 3 4 5 6 7 8 9 10 As you can see, they both produce the same output. Let’s say we wanted to only include even numbers. Luckily, there’s a function that allows us to do that as well, and that’s the seq() function. To use it, we start by putting the first number in our vector, then the last number we want in the vector, and finally the amount we want to increment by in each place. To get the even numbers between 1 and 10, we want the following sequence: seq(2, 10, 2) ## [1] 2 4 6 8 10 Now, let’s say that we want to actually get at the contents of a part of a vector. We can accesss it by using its index. Unlike some other languages (Python, for example), R starts its indexing at 1, not 0. If we want to access the 3\\(^{\\text{rd}}\\) element of the vector we just created, use the index of the part we’re interested in (3) and put it inside of a single set of square brackets ([]) next to the vector. This will return the element in that location. seq(2, 10, 2)[3] ## [1] 6 To figure out how many elements are contained in a vector, we can use the length() function. This information is also displayed in the Environment tab we configured in chapter 1, assuming that the vector is stored as something in the global environment. The last function we should mention here is the rep() function. Similar to seq(), this function allows you to create a vector of the same number, repeated any number of times. Want the number 10 to be repeated 30 times? rep() makes this easy, as you can write rep(10, 30). The first argument is the number or vector you’d like to repeat, and the second is the number of times you’d like to repeat it. If the first argument is a vector, and you’d like to repeat each element a certain number of times, include the each = argument, with the number of times that you’d like each element to repeat. Let’s see rep() in action: rep(2, 10) ## [1] 2 2 2 2 2 2 2 2 2 2 rep(c(1, 2, 3), 3) ## [1] 1 2 3 1 2 3 1 2 3 rep(c(1, 2, 3), 3, each = 2) ## [1] 1 1 2 2 3 3 1 1 2 2 3 3 1 1 2 2 3 3 Lists On the surface, there’s not much difference between a list and a vector. The biggest difference is that a list can contain different types of data, whereas a vector cannot. To create a list, we can simply use the list() function, again putting all the different parts we want included inside the parenthesis, separated by a comma. list(1:10) ## [[1]] ## [1] 1 2 3 4 5 6 7 8 9 10 list(1, &#39;apple&#39;, 2, &#39;banana&#39;) ## [[1]] ## [1] 1 ## ## [[2]] ## [1] &quot;apple&quot; ## ## [[3]] ## [1] 2 ## ## [[4]] ## [1] &quot;banana&quot; Lists can be made up of vectors as well. That is, each element of a list is able to be a vector, since a list doesn’t care what type of data each of its elements is. To access a list’s elements, we want to use a double set of square brackets ([[]]) with the index we’d like to access. list(c(&#39;apple&#39;, &#39;banana&#39;), c(1.25, 2.50), 3)[[2]] ## [1] 1.25 2.50 The above example also illustrates that lists don’t need all list elements to be of the same length. Note that the third element of the list is only of length 1 (it’s just the number 3), but the other two elements are of length 2. Lastly, lists are able to have named elements. To name an element, all you have to do is type NAME OF ELEMENT = before each element, where NAME OF ELEMENT is whatever name you’d like to assign it. In the above example, let’s say we wanted to call the first element fruits, the second element prices, and the third element aisle. Then, our list would look like this: list(fruits = c(&#39;apple&#39;, &#39;banana&#39;), prices = c(1.25, 2.50), aisle = 3) ## $fruits ## [1] &quot;apple&quot; &quot;banana&quot; ## ## $prices ## [1] 1.25 2.50 ## ## $aisle ## [1] 3 We can then use a $ to go into the list and “pull out” that element (the vector with the corresponding name). We can then use vector indexing rules to get a particular element from the vector. If we wanted to get 'banana' from our list above, we have two options. list(fruits = c(&#39;apple&#39;, &#39;banana&#39;), prices = c(1.25, 2.50), aisle = 3)$fruits[2] ## [1] &quot;banana&quot; list(fruits = c(&#39;apple&#39;, &#39;banana&#39;), prices = c(1.25, 2.50), aisle = 3)[[1]][2] ## [1] &quot;banana&quot; As you can see, both options return 'banana', so these options are equivalent. Data Frames The last major type of combined data storing we need to talk about is a data frame. You can think of a data frame as a big table with the data you’d like. Each row of data is called an observation, and each column represents a feature or a variable. We’ll use a few of our own data frames throughout the semester, but it’s good to know that R comes with some of its own data frames already. This data comes from the crabs data frame in the MASS package (see chapter 3 for more information on packages). The first 6 rows are shown below. sp sex index FL RW CL CW BD B M 1 8.1 6.7 16.1 19.0 7.0 B M 2 8.8 7.7 18.1 20.8 7.4 B M 3 9.2 7.8 19.0 22.4 7.7 B M 4 9.6 7.9 20.1 23.1 8.2 B M 5 9.8 8.0 20.3 23.0 8.2 B M 6 10.8 9.0 23.0 26.5 9.8 To see a data frame, you’ll want to use the View() command. This will open the data frame in the Source pane. A few things about data frames: They’re really just an easy-to-see list. You can access any column (feature) by using the $ operator. The syntax (way to write the code) is: df_name$column_name, where df_name and column_name are the data frame name and column name respectively All columns (or list elements) must be of the same length. They may contain NA values, but their lengths must be the same To get the number of rows of a data frame, use the nrow() function. To get the number of columns, you can either use length() (since, as stated before, it’s just a list of vectors), or ncol(). This information is also in the Environment tab. You can make your own with the data.frame() function. Just put the vectors you’d like to include, separated by commas, inside of the parenthesis. Just like with lists, you can name the columns of a data frame as you create it. As long as the vectors are of the same length, you’ll be making data frames in no time! 2.4 Coercing To Other Types The last point we’ll make about different kinds of data is that you can coerce it yourself to be of another type. There are a lot of functions, the as._() functions, that are helpful here. Have a character string that’s just a number? No problem! we saw before that class('2') was of type character. What about if we wanted it to be of type numeric? class(as.numeric(&#39;2&#39;)) ## [1] &quot;numeric&quot; Awesome. 2.5 identical() This is as good a time as any to introduce the identical() function. What this does is checks if the things supplied to it are the identical. It returns TRUE if the arguments are identical, and FALSE if they’re different. Examples, with some of the syntax described above, are as follows: identical(1, 1) ## [1] TRUE x = 1:10 # This has length 10 y = 2:10 # This has length 9 identical(x, y) ## [1] FALSE identical(x[10], y[9]) ## [1] TRUE This function is very helpful when you want to check if two vectors or lists have the same information. It’s also particularly useful when you want to check if the outputs or results of different functions are the same if you’re reorganizing/rewriting code. "],
["variables.html", "Chapter 3 Variables 3.1 Naming 3.2 Vectors, Lists and Data Frame Names", " Chapter 3 Variables Pretend that you have a number that you don’t want to keep typing over and over and want R to remember what it means. You can store that value as a variable in R by declaring it (giving it a name) and assigning it a value. To make the assignment, you can use either = or &lt;-. It’s really personal preference as to which one you’d like to use, but for the duration of the book we’ll use = (mostly out of habit). To assign a variable a value, we put the variable name on the left side of the assignment operator, and we put the value on the right side. As an example, if we want R to remember that some variable that we’ll call \\(x\\) should have the value 5, you can do it like this: x = 5 To see what a variable contains, simply type the name of the variable. x ## [1] 5 Variables don’t only correspond to numeric data, however. We can have variables store any valid type of data. numVar1 = 18 numVar1 ## [1] 18 numVar2 = 3.14 numVar2 ## [1] 3.14 stringVar1 = &#39;This is a string&#39; stringVar1 ## [1] &quot;This is a string&quot; stringVar2 = &quot;I&#39;m learning what a string variable is&quot; stringVar2 ## [1] &quot;I&#39;m learning what a string variable is&quot; boolVar1 = TRUE boolVar1 ## [1] TRUE boolVar2 = F boolVar2 ## [1] FALSE boolVar3 = 3 &lt; 5 boolVar3 ## [1] TRUE 3.1 Naming You can name any variable you create whatever you’d like, but with this freedom comes great responsibility. Variable names must begin with either a letter. Often times, it’s much more useful to name a variable what it represents, rather than just calling it x. For example, if you’re dealing with a data set of attendance at Cubs games and you’d like to specify the attendance on July 12, you can name a variable attendanceJuly12 and it’s totally valid. You don’t just have to name your variables as a single letter. That’s a relief, because otherwise we’d only have 52 possible names! “52 possible names? I thought there’s only 26 letters in the alphabet…” — You, Again (probably) We’d have 52 possible names since in R, variable names are case sensitive. This means that naming one variable x and another X are interpereted differently. Going back to our example above, we could store x as 5 and X as 9. This is what it looks like: x = 5 X = 9 x ## [1] 5 X ## [1] 9 Typically, variables are named in camel case and begin with a lowercase letter (i.e. myFirstVariable as opposed to myfirstvariable, although both are totally valid). Get in the habit of naming variables this way. It’ll make it much easier to identify what variable you’re talking about when you need to access it in your code. Variables can also be operated on together. If you have a variable x and it’s equal to 10 and a variable y that’s equal to 6, and you want to sum them, you can do so like this: x = 10 y = 6 x + y ## [1] 16 The last thing to beware of when naming variables is overwriting the name of another variable. If you have a variable that’s already been declared, and then you want to declare a new variable, you will want to declare it with a new name. Continuing with our example from above, x currently holds value 10. If we now write x = 100 x ## [1] 100 We can see that x now holds the value 100, not 10 anymore. This is why we cautioned you earlier about using T and F in place of TRUE or FALSE. If you create a variable named T, or even do something like T = FALSE, that will be used instead of the shorthand T for TRUE. The bottom line of names is this: they’re important, useful, and completely up to you. Make the name short, sweet, and to the point: they shouldn’t be full sentences, but should be more than a letter. A good rule of thumb is the KISS rule: Keep It Simple, Silly. 3.2 Vectors, Lists and Data Frame Names Variables aren’t just useful for one data point, but they’re useful for vectors, lists, and data frames as well. Just as we just saw, you can assign a name to a vector, a list, or a data frame so that you can call on them later. In fact, it’s a really good idea to name all of these types of things, as they’re frequently how you’ll have your data organized anyways. The naming procedures and conventions are exactly the same as described above. Naming List Elements and Data Frame Columns As we saw earlier, it’s possible to name elements of a list or columns in a data frame as you create it. It’s also possible to change the names of already-named parts of these data types by making use of the names() function. Put the name of the list/data frame in the parenthesis, and then assign new names by supplying them as a character vector like we’ve done before. The example we had before was a short list of fruits, prices, and aisles. We’ll save it as a list called groceries: groceries = list(c(&#39;apple&#39;, &#39;banana&#39;), c(1.25, 2.50), 3) Now, we want to rename the elements as fruits, prices, and aisle. Rather than redeclaring our list, we’ll just use the names() function: names(groceries) = c(&#39;fruits&#39;, &#39;prices&#39;, &#39;aisle&#39;) We can also use the names() function to see what the names of the elements are: names(groceries) ## [1] &quot;fruits&quot; &quot;prices&quot; &quot;aisle&quot; Lastly, we can change individual names so we don’t have to retype/rename every element when we only want to change one. If we wanted to change prices to cost to save a keystroke each time we type it, we can do this: names(groceries)[2] = &#39;cost&#39; As you can see, we did exactly what we wanted to do. Since data frames are really just lists, the same type of renaming applies. Note: R will treat a single value as a vector, even without using the c()function. It’s coercion and vectorization in action. "],
["operators.html", "Chapter 4 Operators 4.1 Mathematical Operators 4.2 Logical (Boolean) Operators 4.3 Vectorization", " Chapter 4 Operators 4.1 Mathematical Operators One thing that R is really good at doing is mathematical computations. Here’s a few of the basic math operations that you’ll want to get familiar with and be very comfortable comfortable using. Symbol Meaning Example + addition 3 + 3 = \\(1 + 1\\) - subtraction 10 - 1 = \\(10 - 1\\) * multiplication 23 * 3 = \\(23 \\cdot 3\\) / division 276 / 4 = \\(\\frac{276}{4}\\) ** or ^ exponent 2 ** 3 = 2 ^ 3 = \\(2^3\\) exp() \\(e\\) exp(2) = \\(\\exp \\left( 2 \\right)\\) log() natural log log(2.71) = \\(\\ln{2.71} = e\\) log10() log\\(_{10}\\) log10(100) = \\(\\log_{10} 100\\) pi \\(\\pi\\) 2 * pi = \\(2 \\cdot \\pi\\) sqrt() square root sqrt(4) = \\(\\sqrt{4}\\) abs() absolute value ( \\(|x|\\) ) abs(-6) = |-6| = 6 R also works in the correct mathematical order of operations: PEMDAS. This stands for Parentheses/brackets Exponents Multiplication Division Addition Subtraction Because of this, using exra parenthesis is never a bad idea. R also automatically closes a set of brackets, parenthesis, or braces ({}) when you open them, but if you’re debugging and deleting make sure that you delete with caution. R will highlight the corresponding parenthesis when you highlight a different one, but these minor details can become major pains. Coder have caution! 4.2 Logical (Boolean) Operators As we talked about before, R can evaluate logical statements in addition to performing math operations. Here’s how this part works: Symbol Meaning Example &gt; greater than 3 &gt; 4 \\(\\implies\\) FALSE &lt; less than 2 &lt; 9 \\(\\implies\\) TRUE &gt;= greater than or equal to (\\(\\geq\\)) 13 &lt;= 13 \\(\\implies\\) TRUE &lt;= less than or equal to (\\(\\leq\\)) 14 &gt;= 12 \\(\\implies\\) FALSE == is exactly equal to 60 == 61 \\(\\implies\\) FALSE != is not equal to (\\(\\ne\\)) 69 != 818 \\(\\implies\\) TRUE &amp; logical AND (used in conjunction with one of the above) x &gt; 12 &amp; y == TRUE \\(\\implies\\) x is TRUE and y is TRUE | logical OR (used in conjunction with one of the above) x == TRUE | y == TRUE \\(\\implies\\) x is TRUE or y is TRUE 4.3 Vectorization Now it’s time to talk about one of the most useful features of R: the fact that it’s a vectorized language. This sounds very complex, but what it really means is that you can operate on entire vectors at a time using the operators we just went through. Let’s see a few examples using a vector of length 25. We’ll create a vector v1 that has the numbers 1 through 25, and we’ll do a few operations on it to really see how powerful vectorized operations are. v1 = 1:25 First, let’s see what simple addition does to the vector. Pretend we want to add 5 to each element? v1 + 5 ## [1] 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 ## [24] 29 30 What about doubling each element? v1 * 2 ## [1] 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 ## [24] 48 50 Cool! But what about that boolean thing we were talking about? Well, let’s try an example. What if we want to find where all the values are greater than 12? v1 &gt; 12 ## [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [12] FALSE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## [23] TRUE TRUE TRUE This is the power of vectorization: you can do things to really large amounts of data in a relatively short amount of time. Obviously, the more complex the operation is, the longer it will take, but it still beats the alternative of having to do the manipulations and calculations by hand. There’s one last important point to make about vectorization, and that’s what happens when you add vectors of unequal length. We’ve actually already seen examples of this, but we just didn’t realize it. What’s happening is that R is treating the numbers that we’ve supplied above (5, 2, and 12 respectively) as vectors of length 1, and recycling them over and over in its calculations. If instead we had tried to do v1 + c(5, 10), we’d wind up with something different. v1 + c(5, 10) ## [1] 6 12 8 14 10 16 12 18 14 20 16 22 18 24 20 26 22 28 24 30 26 32 28 ## [24] 34 30 This is something that we need to be aware of: even though R can do a vectorized calculation, we need to make sure that it’s performing the calculation we actually want it to do. Again, the length() function is your friend here! "],
["packages-and-getting-help.html", "Chapter 5 Packages and Getting Help 5.1 Packages 5.2 Help Files 5.3 Other Helpful Resources", " Chapter 5 Packages and Getting Help 5.1 Packages A package is just a collection of functions, data, and already-written code that is in a well-defined format. We’ll introduce a few packages as we go, but a lot of what we’ll cover will rely on the base package, which is just the functions that R already comes with. They’re the base of most analyses and other operations you’ll want to do in R, hence the name of the package. To install a new package, you want to use the install.packages() function, with the name of the package quoted and inside of the parenthesis. One package that you’ll want to install is the swirl package, which was developed as a self-guided class to help you learn R. To install this package, copy and paste the following line into your Console. install.packages(&#39;swirl&#39;) Once the package is installed, you’ll want to load the package (tell R that you’ll be using commands, data, and other information from the package). Copy/paste or type the following into your console to do so, and follow the prompts: library(swirl) 5.2 Help Files Another great aspect of R is how well-documented the language is. If you don’t know how to use a particular function, there are a lot of ways to learn how to use it and see it in action. The help files provide the syntax (what order to put things into the function), an explanation of the arguments, as well as information about the output (what the function gives back to you), and examples of the function in use. An Example Let’s say you wanted to learn more about the mean() function. You can type ?mean into your console, and the help file will pull up in the Help tab. From there, you can learn all about the function. The header of the mean() documentation As you can see, the top of the help function starts with the name of the function, with the package it comes from in brackets. Then it provides a brief description of what the function does. This one gets the mean (average) of a list of numbers. Syntax for mean() This part tells us that all the function looks for is an object x, which is just the numbers you’d like to take the average of. Something to note: the trim argument has a default value of 0, and na.rm is set to FALSE. As you hopefully remember from chapter 2, these will be the values that the function uses unless you specify differently. Then it explains the arguments it’s looking for as it does its job in the Arguments section: Arguments for mean() Next is the Value section, which explains what the output of the function actually is. Value for mean() Lastly, the documentation provides examples of it in use. You’ll understand this code by the end of the chapter, but for now trust that it’s doing its job appropriately. Examples of mean() 5.3 Other Helpful Resources If the help file isn’t much help in solving your problem, there’s a plethora of other helpful resources available to you. What do you do when you don’t understand something? What about if you get an error message and your code doesn’t run properly? One of the most important things to remember is that you’re not the first person to struggle with wherever you’re stuck, or get the error message you just got. There may not always be a TA that’s available to help right when you need them, so it’s a good idea to start learning how to help yourself solve your problems. StackOverflow is one of the best resources available to you in terms of help. It’s an open, online forum for people to post and answer questions. Think of it like reddit, but for code. This is a fantastic place to start your search, and the questions range from the very simple beginner questions to incredibly technical and advanced concepts. If a post is too technical for you to understand, there’s almost always another one (or more!) that can help. It may seem odd, but Googling is also a great way to learn. Because of how powerful Google is, copying and pasting error messages will usually result in a good explanation of the error message, why you’re getting it, and multiple ways to correct it. Google is your friend. There’s also plenty of resources online to help you learn R programming as well. In addition to the swirl library we talked about earlier, there are some websites, like codeschool, that allow you to practice writing R code. Feel free to use this one as much or as little as you’d like, but the best way to get better at programming is to practice as much as possible. Lastly, there’s also a variety of books that you can make use of to help learn and understand R, like this book It’s based off a Coursera course, and the author made videos that are linked in the book. You can download it for free by just sliding the price slider to 0 (or you can pay if you want). "],
["loading-data.html", "Chapter 6 Loading Data 6.1 Data Files 6.2 Reading in CSV Files 6.3 Writing Files", " Chapter 6 Loading Data Like we learned before, you’ll be able to see what data, variables, and other things you’ve loaded and stored in R by checking your Environment tab. Right now, yours is most likely empty, and that’s okay! We’ve learned a lot, but haven’t started doing a ton yet. We will soon, but first we need to learn how to get data into R so that we can start exploring it. 6.1 Data Files Most of the time, you’ll want to work with something called a CSV file. CSV stands for Comma Separated Value, which means that all of the data within an observation is separated by a comma Here’s an example CSV file that you can download. To see the raw file, you can open it in a program like TextEdit on a Mac or Notepad on PC. Usually, a CSV file will default to open in a spreadsheet program like Excel since it’s easy to parse (read in and break up) into columns – this is because of the commas! 6.2 Reading in CSV Files To read a CSV into R, there’s a great function called read.csv(). It will read in your file as a data frame. To read in the file, you just need to put the name of the file as a charachter string (remember your data types?) in between the parenthesis. This is the only required argument, or input to a function, that you need to supply, however there are a few others to note: The stringsAsFactors argument can be either a TRUE or FALSE value. When TRUE (this argument’s default value), it takes any character strings in the data and coerces them to be factors. Sometimes this may be okay, sometimes not The header argument. This indicates whether or not a header row is present in the CSV file, which would contain names for all of the columns. It defaults to TRUE, but it’s a good idea to double check your data and make sure that one’s present. You can check the names and rename if you wish by using the names() function discussed in chapter 3 Directories One common issue is the location on your computer where the CSV is located. If you just type the name of the file as the argument for read.csv(), R will look for it in your working directory, or default file lookup location. However, if your file isn’t present in the working directory, you’ll likely get an error message. If this is the case, you have a few options: You can supply the full filepath to the data as the string. Watch this video to find how to find the full filepath for Mac or this one for PC. While it may be more work to find, it’ll guarantee that you import the right file. You can change your working directory with the setwd() function, supplying the path to the directory as a character string argument. This still requires knowing where your file is located, but if you plan to work with multiple files, this isn’t a bad option. You can check your current working directory with the getwd() function without any arguments. Loading Example Now let’s import that example CSV file. It’s the combined results from Survey 1 of STAT 100 and STAT 200. Remember, we should name the new data frame so that we can look at it and refer back to it. We’ll use the CSV file 'Combined Fall 2017 Survey 1.csv', which does have a header row, and is located in a subdirectory of our working directory called data (this is the directory that holds all of our data files for the book). It’s in a directory called data, has a header row, and we’ll keep character variables as characters. When we import it, we’ll call it survey1: survey1 = read.csv(&#39;data/Combined Fall 2017 Survey 1.csv&#39;, header = TRUE, stringsAsFactors = FALSE) To see the first few observations, you can use the head() function, passing the data frame’s name (in our case, survey1). This will display the observations in your Console, and will look like this: head(survey1) ## gender genderID height weight shoeSize schoolYear studyHr GPA ACT pets ## 1 Male Male 66 200 10.5 Sophomore 2.5 3.5 24 1 ## 2 Female Female 66 142 7.5 Freshman 3.0 3.8 26 1 ## 3 Male Male 65 160 10.5 Freshman 3.0 3.9 33 6 ## 4 Female Female 68 118 7.5 Sophomore 3.0 3.9 28 0 ## 5 Female Female 61 173 9.0 Sophomore 0.5 2.8 21 1 ## 6 Female Female 66 125 8.0 Sophomore 2.5 2.3 20 0 ## siblings speed cash sleep shoeNums ageMother ageFather random love ## 1 2 25 5 7.0 3 47 49 6 few ## 2 1 80 11 4.5 21 37 47 4 few ## 3 0 0 45 7.5 4 43 46 8 few ## 4 1 105 9 7.5 25 54 53 8 few ## 5 3 90 7 8.0 13 35 60 7 one ## 6 0 50 3 6.0 20 40 41 3 dozens ## charity movie favTV1 favTV2 ## 1 60 Life is Beautiful Narcos Daredevil ## 2 60 The Giant Como dice el dicho Drake and Josh ## 3 10 Interstellar none none ## 4 25 La La Land Friends Parks and Recreation ## 5 0 Get Out Attack on Titan Rick and Morty ## 6 50 Hidden Figures The Cosby Show A Different World ## section ## 1 Stat100_L1 ## 2 Stat100_L1 ## 3 Stat100_L1 ## 4 Stat100_L1 ## 5 Stat100_L1 ## 6 Stat100_L1 Use the View() function, again passing the name of the data frame as the argument. It’ll display much cleaner and clearer in your Source pane, looking more like this: gender genderID height weight shoeSize schoolYear studyHr GPA ACT pets siblings speed cash sleep shoeNums ageMother ageFather random love charity movie favTV1 favTV2 section Male Male 66 200 10.5 Sophomore 2.5 3.5 24 1 2 25 5 7.0 3 47 49 6 few 60 Life is Beautiful Narcos Daredevil Stat100_L1 Female Female 66 142 7.5 Freshman 3.0 3.8 26 1 1 80 11 4.5 21 37 47 4 few 60 The Giant Como dice el dicho Drake and Josh Stat100_L1 Male Male 65 160 10.5 Freshman 3.0 3.9 33 6 0 0 45 7.5 4 43 46 8 few 10 Interstellar none none Stat100_L1 Female Female 68 118 7.5 Sophomore 3.0 3.9 28 0 1 105 9 7.5 25 54 53 8 few 25 La La Land Friends Parks and Recreation Stat100_L1 Female Female 61 173 9.0 Sophomore 0.5 2.8 21 1 3 90 7 8.0 13 35 60 7 one 0 Get Out Attack on Titan Rick and Morty Stat100_L1 Female Female 66 125 8.0 Sophomore 2.5 2.3 20 0 0 50 3 6.0 20 40 41 3 dozens 50 Hidden Figures The Cosby Show A Different World Stat100_L1 Much better. 6.3 Writing Files After you finish with your analysis, you may wish to save the data frame(s) that you’ve created. Similar to the read.csv() function that allows you to import a CSV, the write.csv() function will allow you to write your own CSV files to your computer to save and send as needed. "],
["coding-style.html", "Chapter 7 Coding Style 7.1 Comments 7.2 Spacing 7.3 Indentation 7.4 Consistency 7.5 Simplifying Coding Style", " Chapter 7 Coding Style It’s almost time to start applying what you’ve learned, but before we get into writing code, we should take a minute to start talking about more good coding practices. It’s often said that the best way to break bad habits is to not fall into them in the first place, so we’ll try to get into good habits right from the get-go. The code that you write will be read by R, which will ignore extra spaces, correct for indentation, and for all intents and purposes run properly, assuming that it’s syntactically correct. However, you will also be reading through your code, both as you write and debug it. That means that it should be easy for you – or anyone else for that matter – to read as well as R. This chapter’s whole purpose is to make this as easy as possible for you to do. 7.1 Comments There’s one important symbol/operator that we left out in chapter 5, and that’s the comment symbol. A comment is just a note for yourself so that you can explain what a block of code does, why you wrote the code a particular way, or really just anything else that you’d like to note at that point in the code. They won’t be evaluated by R as commands, so it may be useful to even comment out parts of your code (make line(s) of code into comments to prevent them from running but save yourself from retyping). To make a comment in R, use #. Pro tip: You can highlight whole lines of code, then go to the Code menu at the top and select Comment/Uncomment Lines to comment out (or uncomment) sections of code at a time. The shortcut on a Mac is Cmd + Shift + C, and on a PC it’s Ctrl + Shift + C. Get in the habit of commenting frequently as you code. As we said before, these are ways for you to remember what you did so that when you revisit your code, you remember what your thought process was. 7.2 Spacing One really good practice is to put a single space before and after any operator you use. While it does lengthen the line of code itself, it makes it much easier to debug. You may find that you’ve used a = to check a condition when you should have used a ==, or you may see that you only put one * when you meant to use the right side as an exponent. Spacing also refers to spacing lines of code out within your script. Have a line that actually takes up 3 lines? No problem, R can handle that, but it’ll be a pain to read. Just find a good breaking point in the line (usually after a comma) and go to the next line. It allows you to see more of your code in an easier format. You may even wish to put each individual argument of a function on its own line in some cases, and that’s encouraged! Keeping with our “script-is-a-MS-Word-Doc” analogy from earlier, related code lines should be grouped together, and separated from other grouped lines of code, just like you’d separate ideas in your essay into paragraphs. They should follow a logical order, be organized into groups, and after each group, you should skip a line to signal the next group is beginning. As we said before, R will ignore empty lines of code, so there’s no harm in skipping a line to organize your thoughts. 7.3 Indentation If you were an absolutely perfect code-writer, this part would take care of itself. However, there’s no such thing as a perfect code-writer, therefore this is worth mentioning. In languages such as Python, indentation. Is. Everything. In R, it’s not as imperative in terms of functionality, but it’s equally imperative in terms of legibility. Once we get to control structures, you’ll be able to see these with much more clarity, but it’s a good idea that any time your code takes up more than one line and you’re working inside of parenthesis, braces, or brackets, you indent your code one tab (four spaces) to the right. Open a new set of parenthesis/braces/brackets after you’ve indented once? Indent again! No harm, only help! 7.4 Consistency In an essay, you wouldn’t switch fonts, colors, or page layouts in the middle, would you? You shouldn’t change much in your code scripts either. Your code should be consistent in as many ways as you can find. This comes up a lot with naming and assignment, so those will be where we’ll turn our focus for now. Name variables in the same way every time you name a variable in a script. If you usually use camel case to name your variables, make sure all of your variables (where applicable) are named with camel case. If you use underscores (_) in names, don’t switch to naming things with periods instead. (Example: if you name something my_variable, don’t name another variable my.new.variable). Names should be unique, concise, and descriptive. In terms of assignment, pick either = or &lt;- and stick with it. It’s good to realize that they do the same thing, and it’s even better to practice using both, but within a script, you should stick to just one. That way, anyone that reads it can clearly identify where you’ve named a variable. 7.5 Simplifying Coding Style This may seem like a lot of very specific things to keep in mind. Luckily, RStudio has a built-in capability to handle this for you and make your life much easier. Simply highlight all of your code that you’d like formatted (it should be all of it), go to the Code menu, and select Reformat Code (Mac: Cmd + Shift + A. PC: Ctrl + Shift + A). Then, go back into the Code menu (with the code still highlighted) and select Reindent Lines (Mac: Cmd + I. PC: Ctrl + I.) While this won’t be necessary right away, this is an excellent, powerful tool to keep in your back pocket for when you do end up needing it. "],
["hists.html", "Chapter 8 Bar Graphs vs. Histograms 8.1 Drawing Bar Graphs and Histograms in R Extracting Information From Histograms", " Chapter 8 Bar Graphs vs. Histograms 8.1 Drawing Bar Graphs and Histograms in R One way to explore data in R is by creating quick vizualizations. We’ll use survey1, the STAT 100 and 200 combined survey data from before, to demonstrate. The dataset has 1628 observations of 24 variables, and a description of the variables in the dataset is available here. Preview of survey1 gender genderID height weight shoeSize schoolYear studyHr GPA ACT pets siblings speed cash sleep shoeNums ageMother ageFather random love charity movie favTV1 favTV2 section Male Male 66 200 10.5 Sophomore 2.5 3.5 24 1 2 25 5 7.0 3 47 49 6 few 60 Life is Beautiful Narcos Daredevil Stat100_L1 Female Female 66 142 7.5 Freshman 3.0 3.8 26 1 1 80 11 4.5 21 37 47 4 few 60 The Giant Como dice el dicho Drake and Josh Stat100_L1 Male Male 65 160 10.5 Freshman 3.0 3.9 33 6 0 0 45 7.5 4 43 46 8 few 10 Interstellar none none Stat100_L1 Female Female 68 118 7.5 Sophomore 3.0 3.9 28 0 1 105 9 7.5 25 54 53 8 few 25 La La Land Friends Parks and Recreation Stat100_L1 Female Female 61 173 9.0 Sophomore 0.5 2.8 21 1 3 90 7 8.0 13 35 60 7 one 0 Get Out Attack on Titan Rick and Morty Stat100_L1 Female Female 66 125 8.0 Sophomore 2.5 2.3 20 0 0 50 3 6.0 20 40 41 3 dozens 50 Hidden Figures The Cosby Show A Different World Stat100_L1 Let’s explore the height variable a little bit. One way that we can do it is by breaking up, or binning, the data into different groups, then plotting what percentage of the data is in each group. This creates what’s called a histogram. To make a histogram in R, we can use the hist() function (see ?hist for more information). All that hist() needs is an argument x, which is what you’d like to make a histogram of. Since we want the densities, we’ll add in the freq = FALSE argument. This results with a histogram that looks like this: hist( survey1$height, freq = FALSE ) Figure 8.1: A basic histogram Let’s add a few extra arguments to make the plot a little clearer: main and xlab create a title and an \\(x\\)-axis label respectively ylim sets the range of \\(y\\)-values that are shown on the \\(y\\)-axis (Note: xlim does the same for the \\(x\\)-axis) breaks controls what numbers are used as part of the binning process. The first number is the smallest value in the data, and the last value is the largest. You can find these by employing min() and max() individually, or you can use the range() function and get both at the same time. Note: the break points we used were arbitrarily selected col changes the colors of the bars. We change these to make them a little easier to identify, but it’s purely cosmetic. Supplying a single value, which can be any named color that R already recognizes, an RGB value while using the rgb() function, or a hexadecimal color value supplied as a character, preceded by a #. We’ll leave it to you to learn about these color formats on your own, but kow that they’re available to you. Supplying a single value will change the color for each bar, making all the bars the same color, while supplying a vecotr of the same length as the number of bars will change each color individually par(mfrow = c(1, 2)) # Puts plots side-by-side # Histogram hist( survey1$height, main = &#39;Histogram of Heights&#39;, xlab = &#39;Heights in Inches&#39;, ylim = c(0, .1), freq = FALSE, breaks = c(49, 62, 65, 68, 70, 73, 95), axes = FALSE, # Removes default axis numbers labels = TRUE, # Put decimals above each bar col = &#39;#0088ce&#39;, border = &#39;#939598&#39; ) axis(2) # Puts y-axis numbers back axis(1, at = c(49, 62, 65, 68, 70, 73, 95)) # Puts x-axis numbers back # Bar plot hist( survey1$height, main = &#39;Bar Plot of Heights&#39;, xlab = &#39;Heights in Inches&#39;, ylim = c(0, 500), freq = TRUE, breaks = c(49, 62, 65, 68, 70, 73, 95), labels = TRUE, # Put counts above each bar axes = FALSE, # Removes default axis numbers col = &#39;#0088ce&#39;, border = &#39;#939598&#39; ) axis(2) # Puts y-axis numbers back axis(1, at = c(49, 62, 65, 68, 70, 73, 95)) # Puts x-axis numbers back Figure 8.2: Well-formatted histogram (left) and density plot (right) While the overall shapes of the two plots seem the same, there are a few important differences to take note of. The biggest one is the significance of each block’s width and height. With a histogram, the height shows the percentage per unit inside of each block, while on a bar graph the heights have no meaning whatsoever. The numbers displayed on top show the total number of people inside each interval. The widths (and the \\(x\\)-axis altogether) of each plot also carry different meanings: barplots just show an interval, while a histogram represents unique heights. That is, even if a number doesn’t appear on the \\(x\\)-axis in a histogram, that height is still represented. Since the heights and widths of each kind of plot differ, so too do the areas, since the areas are the width of the interval times the height of the interval. Histogram areas show percentages within each block, while bar plot areas are – you guessed it – irrelevant. We could also summarize our data as the following table: Range Area Count 49-62 14.8% 241 62-65 25% 407 65-68 27.15% 442 68-70 13.76% 224 70-73 12.71% 207 73-95 6.57% 107 Note: the areas of the blocks on the histogram sum to 100% This isn’t to say that bar plots don’t have their place, this just isn’t it. Sorry bar graphs, you’ll just have to wait until it’s analysis time. We’ll focus on histograms for the rest of this chapter. Extracting Information From Histograms In addition to being able to do visual analysis of a histogram, it may be more useful to use some of the information that the hist() function generates. Usually what we care about is the plot itself, but hist() calculates and stores a lot of information in addition to generating the plot. We’ll store the results from histogram as something called hist1. hist1 = hist( survey1$height, main = &#39;Histogram of Heights&#39;, xlab = &#39;Heights in Inches&#39;, ylim = c(0, .1), freq = FALSE, breaks = c(49, 62, 65, 68, 70, 73, 95), axes = FALSE, # Removes default axis numbers labels = TRUE, # Put decimals above each bar col = &#39;#0088ce&#39;, border = &#39;#939598&#39; ) In addition to the plot, hist1 contains a list with elements breaks, counts, density, mids, xname, and equidist. Check the help file for what these mean, but we should note that the total count of observations and the density of observations in a given range can be accessed by hist1$counts and hist1$density respectively. Check out the use of list element extraction using $ here! These are vectors, and you can use the components as you need to. "],
["measures-of-central-tendency.html", "Chapter 9 Measures of Central Tendency 9.1 The Mean and mean() 9.2 The Median and median() 9.3 Standard Deviation and sd() 9.4 Mean and Standard Deviation After Changing The Data", " Chapter 9 Measures of Central Tendency A measure of central tendency is a way of talking about the common values from a set of data points. In this section, we’ll talk about two: the mean and the median. 9.1 The Mean and mean() The most common measure of central tendency is the average, which is often referred to as the mean. Sometimes, you’ll see it represented as the greek letter mu \\(\\left( \\mu \\right)\\), or as \\(\\bar{x}\\). If we have a set of numbers – let’s say those numbers are 0, 5, -5, 10, -10, 40, and 100 – we compute the average as follows: \\[ \\bar{x} = \\frac{0 + 5 + \\left( -5 \\right) + 10 + \\left( -10 \\right) + 40 + 100}{7} = \\frac{140}{7} = 20 \\] This may seem trivial, so let’s get to the fun part of doing it in R. First thing is first: we need our numbers to get loaded into R. Aha! A perfect time for a vector. Since we called the average \\(\\bar{x}\\), let’s name this vector x. x = c(0, 5, -5, 10, -10, 40, 100) Now we need to go and actually calculate \\(\\bar{x}\\). Well we’re calculating the mean, so we should try the mean() function. mean(x) ## [1] 20 This returns (gives back) the mean of x, and we can see that it is, in fact, 20. Another way that we could have done this is more like we would have done by hand: first we would have to sum the numbers, then we’d have to divide them by the number of numbers we just added together. Luckily, the sum() and length() functions save us a lot of time and energy. sum(x) / length(x) ## [1] 20 Again, we get that the average is 20, thus both methods are valid. Handling NA Values What would happen in functions like mean(), sum(), length(), min(), or max() when NAs are present? Well let’s find out. We’ve inserted a few NAs into the same vector as above, but we’ve called it y now so as not to confuse the two. y = c(0, 5, NA, -5, 10, -10, NA, 40, 100) mean(y) ## [1] NA sum(y) ## [1] NA They return NA! You may be asking yourself, why? Well, the reason this happens is because you haven’t told R how to deal with a non-existent value (We told you they’d be pesky!). You most likely want R to ignore the NA altogether, so we can take advantage of an argument called na.rm and set it equal to TRUE. mean(y, na.rm = TRUE) ## [1] 20 sum(y, na.rm = TRUE) ## [1] 140 Great, everything is working again! Problem solved. 9.1.1 mean() with TRUE/FALSE It’s also good to note that R can take the mean of logical vectors. What happens is that TRUE is coerced to be 1, and FALSE is coerced to be 0. Then, the mean is taken just as before. This may seem boring, but it allows us to determine percentages quickly. Let’s say we want to find the percentage of students that responded to Survey 1 that maintained at least a 3.0 GPA. How would we figure this out if we were doing it by hand? Probably like this: Get an observation of data (that is, go to the first response) Check the GPA for that responder If the GPA is 3.0 or higher, we add 1 to our tally Otherwise, add 0 After going through all observations, take the final tally and divide by the number of observations we observed (that had values) Luckily, there’s a very good way to do this in R, and it involves the mean() function. In addition to evaluating the mean of a vector of numbers, it can compute the percentage of observations that meet a certain condition, given by something called a conditional statement. We talked about them before, so now it’s time to see why we spent the time learning how to write them. We’ll show the “complete” version of the process we just outlined, and then we’ll go through and do it the short way. # Create TRUE/FALSE vector of places where students have GPAs above 3.0 bool_vec = survey1$GPA &gt; 3.0 head(bool_vec) ## [1] TRUE TRUE TRUE TRUE FALSE FALSE # Coerce the TRUE/FALSE vector to be 1s and 0s bool_as_binary = as.numeric(bool_vec) head(bool_as_binary) ## [1] 1 1 1 1 0 0 # Calculate the percentage sum(bool_as_binary) / length(bool_as_binary) ## [1] 0.7800983 This works, but there’s a faster way to do it. We can see that the last step of summing and dividing by the length is the same as the mean() function, so we can just change that to be mean(bool_as_binary) and get the same result. But bool_as_binary is just the numeric coersions of bool_vec, and R knows to do this coersion automatically when using a function like mean() on a vector of type logical. So it’s equally valid to say mean(bool_vec), although we’ll take it one step farther. We’ll put the conditional statement that created bool_vec directly into the mean() function, and we should get the same result. mean(survey1$GPA &gt; 3.0) ## [1] 0.7800983 Not only did we save four lines of code to get the same result, but we saved a little memory (storage space) in the process. This is because we didn’t need to store the result of the conditional or its coersion as vectors to be used later. R just handled it all internally. This exact procedure is very useful when you want to check how much data is above a threshhold in a quick way. 9.2 The Median and median() The median, or middle number, is the other most common measure of central tendency. It’s the point in which our data gets split in half. To determine the median by hand, we’d follow the following process: Arrange data in numerical order from smallest to greatest Cross off the two endpoints Move in one data point from each end of the range of data Repeat steps 2 and 3 until either: One number remains in the middle, or Two numbers remain in the middle If one number remains, we’ve found our median. If two numbers remain, take the mean of the last two remaining data points. Let’s find the median of this list of numbers: 6, 5, 0, 12, 10, 11 By hand, we’d put them in the following order: 0, 5, 6, 10, 11, 12 Then we’d cross them off, endpoint pair by endpoint pair, until we reach the middle: 0, 5, 6, 10, 11,12 0, 5, 6, 10, 11, 12 We’re down to two numbers, so we just take the average of the remaining numbers, which are 6 and 10. We get 8, so this is our median. As was the case with mean(), R comes with a great function – the median() function – to calculate the median of a list of numbers. We can supply the vector directly to median() and immediately get the median. median(c(6, 5, 0, 12, 10, 11)) ## [1] 8 Skewed Histograms Sometimes, you’ll get data that’s heavily skewed one way or another. This means that the mean and the median are not in the same place. Let’s take a quick look: Figure 9.1: Skewed and Symmetric Distributions The plot on the left shows a histogram that’s right-skewed. This means that the mean is greater than \\(\\left( \\gt \\right)\\) the median. The middle plot shows a symmetric distribution. This means that the mean is equal to the median. The plot on the right shows a histogram that’s left-skewed. This means that the mean is less than \\(\\left( \\lt \\right)\\) the median. 9.3 Standard Deviation and sd() Lastly, we should talk about the standard deviation. This is how spread out around our average the data is. The standard deviation applies to the whole vector, not just an individual point. We’ll talk about how to use the concept of a standard deviation for just a single data point in the next chapter. Breaking the term down, we can understand a little bit more intuitively what exactly a standard deviation is. A deviation is how far a particular point is from another point (in our case, the average), and standard typically means the average, or divided by how many points there are. So really, a standard deviation is a way to look at the average difference of any data point from the average of the data. Note: The standard deviation can never be negative, since it doesn’t make sense to be a negative distance from the average. The standard deviation is calculated as \\[ \\sqrt{\\frac{1}{n} \\sum_{i = 1}^n \\left( x_i - \\bar{x} \\right)^2} \\] Now that the scary math is out of the way, let’s make it make sense. We’ll define a process to calculate the standard deviation, and rebuild that equation as we go. Calculate the average, \\(\\bar{x}\\), of all of the data points Take each point in the data, which we’ll call \\(x_i\\) (think of \\(i\\) as the \\(i^{th}\\) element of the vector of data points), and subtract off the average. These are the deviations Pro tip: These deviations should always add to 0 Square the difference in step 2. Mathematically, we’ve got \\[\\left( x_i - \\bar{x} \\right)^2\\] Take the average of the squared differences in step 3. That’s where the \\(\\frac{1}{n} \\sum_{i = 1}^n\\) comes from, and it gives us what’s called the variance, and it’s a single number: \\[ \\frac{1}{n} \\sum_{i = 1}^n \\left( x_i - \\bar{x} \\right)^2 \\] Take the square root of the variance we just found in step 4. This is the standard deviation, and it’s given by \\[ \\sqrt{\\frac{1}{n} \\sum_{i = 1}^n \\left( x_i - \\bar{x} \\right)^2} \\] We’ll do a quick example as a table with this list of numbers: 0, 1, 2, 3, 4, 5, 6 . The average of the list is 3. \\(x_i\\) \\(x_i - \\bar{x}\\) \\(\\left( x_i - \\bar{x} \\right)^2\\) 0 -3 9 1 -2 4 2 -1 1 3 0 0 4 1 1 5 2 4 6 3 9 We then average the last column to get 4, and take the square root to get a standard deviation of 2. Now that we see how the process works and can do it by hand, we’re ready to use the R function sd() to speed the process along. Just pass the vector of values to sd() and the standard deviation will be computed. sd(c(0, 1, 2, 3, 4, 5, 6)) ## [1] 2.160247 Hmmm… That’s not quite right. By hand, we got 2, but with sd(), we got 2.1602469. This is where we need to start being a little careful. Earlier, we defined standard deviation as the standard deviation of the population (frequently referred to as \\(\\sigma\\)), which is given by \\[ \\sigma = \\sqrt{\\frac{1}{n} \\sum_{i = 1}^n \\left(x_i - \\bar{x} \\right)^2} \\] However, in R, the sd() function computes the sample standard deviation, which is slightly different. (We’ll cover it later on in the class, but to help you understand what R is doing, we need to introduce the difference now.) The sample standard deviation, \\(s\\), is given by \\[ s = \\sqrt{\\frac{1}{n-1} \\sum_{i = 1}^n \\left(x_i - \\bar{x} \\right)^2} \\] The the sd() function is actually computing \\(s\\) and not \\(\\sigma\\). That’s okay though! A quick bit of algebra helps us to make the conversion between the sample and the population. To get from \\(s\\) to \\(\\sigma\\), we just need to multiply by \\(\\sqrt{\\frac{n - 1}{n}}\\). The \\(n - 1\\) in the numerator cancels the \\(n - 1\\) in the denominator of \\(s\\), and then the \\(n\\) in the denominator is the \\(n\\) in \\(\\sigma\\). So, to get the population standard deviation, we need to do this: sd(c(0, 1, 2, 3, 4, 5, 6)) * sqrt((length(c(0, 1, 2, 3, 4, 5, 6)) - 1) / length(c(0, 1, 2, 3, 4, 5, 6))) ## [1] 2 All fixed! Let’s put this into a function so that we can do it on any data set we’d like. We’ll call the function stdv(), which takes one argument (the numeric vector x), and returns the population standard deviation. stdv = function(x){ sd(x) * sqrt((length(x) - 1) / length(x)) } The larger the standard deviation is, the more spread out our data is. The smaller the standard deviation, the less spread the deviation is. As an extreme case, a standard deviation of 0 means that there is no spread whatsoever. This is because all of the data would be located at the average. 9.4 Mean and Standard Deviation After Changing The Data The mean and median change if we add, subtract, multiply, or divide the same number to every point in our data. They will change by the exact amount that we’ve added or subtracted (i.e. if we add 4 to every number, the average and median increase by 4. Dividing by 12 will divide the average and median by 12). The standard deviation changes differently under addition, subtraction, multiplication, and division. Addition and subtraction don’t have any impact on the standard deviation, while multiplication and division change the standard deviation by the same value (i.e. if we multiply every point by 6, the standard deviation is multiplied by 6 as well) If the factor that the data points are multiplied by is negative, the standard deviation is multiplied/divided by the absolute value of the factor. Note: We don’t immediately know how the average, median, or standard deviation change by doing any of these operations to just a single point in our data, but R makes these computations easy. "],
["the-normal-approximation.html", "Chapter 10 The Normal Approximation 10.1 The SD 1-2-3 Rule 10.2 Z-scores 10.3 The _norm() Functions", " Chapter 10 The Normal Approximation The ideal histogram will have a bell shape like the one you see above. While not all histograms will have this shape, many will roughly approximate it. This shape is called a normal curve, also referred to as the normal approximation. There’s two major facts that we need to keep in mind about the normal curve: It’s a symmetric distribution about the center (the average). This fact allows us to apply the logic that whatever we do to one side of the curve, we can safely do to another. We’ll keep coming back to this fact, so it’s really important to keep this in the back of your head. Just like a histogram, the total area underneath the curve adds to 100%. Again, this is something that we’ll keep coming back to. Burn this fact into your brain too. A few more things to note about the normal curve: While on the plot the range of the \\(x\\)-axis goes from -3 to 3, the curve really extends out forever. It’s asymptotic It’s a density plot and not a frequency plot (this is what allows the total area to be 100%) The highest point on our curve occurs at 0. Since the \\(x\\)-axis is how many standard deviations away from the average a point is, we know then that the curve’s highest point (we say “it’s centered”) around 0 10.1 The SD 1-2-3 Rule The SD 1-2-3 Rule tells us how much data is within 1, 2, and 3 standard deviations of the average. The orange area of the above normal curve is 1 standard deviation of the average, or roughly 68%. Within 2 standard deviations of the average (the blue area, plus the middle orange area), gives us approximately 95% of the data, and the grey area, plus the orange and blue areas, give 99.7% of the data. Quick summary: orange = 68% orange + blue = 95% orange + blue + grey = 99% In other words, 68% of the data is between -1 and 1, 95% of the data is between -2 and 2, and 99% of the data is between -3 and 3. Now this is great and all, but what about if the data isn’t exactly 1, 2, or 3 standard deviations away from the average? Glad you asked. That’s where Z-scores make their money. 10.2 Z-scores A Z-score, also known as standard units, is a measure of how many standard deviations away from the average a particular point of data is. Like we just said, every point of data in the data set will correspond to a Z-score. If we want to know how many standard deviations away from the average a point of data is, we should start by figuring out how far the point itself is from the average in whatever units the data’s in (i.e. if we’re talking height, how many inches away from the average is this particular data point?). \\[ \\text{Distance from average} = \\text{Value} - \\text{Average} \\] Then, if we want to figure out how many standard deviations the point (sometimes called \\(x\\)) is away from the average (occasionally called \\(\\mu\\)), we just need to divide by the standard deviation (SD, AKA \\(\\sigma\\)) is. This is how we get the formula for a Z-score. \\[ Z = \\frac{\\text{Distance from average}}{SD} = \\frac{\\text{Value} - \\text{Average}}{\\text{SD}} = \\frac{x - \\mu}{\\sigma} \\] So, all we need to calculate a Z-score is the data point, the average, and the standard deviation. But then how do we know what the corresponding area is? Luckily, we have this handy chart that tells us the middle area, which is the area between -Z and +Z. What a lifesaver! Just be sure that before you go to that chart, you’ve converted everything to a Z-score. We can then compute the middle areas – and therefore the remaining tails (remember our “area under the curve = 100%” fact?) – for any point in our data set. By hand, we can get the middle area from the chart, subtract it from 100% to get the remaining area in both tails, and divide by 2 (thank you curve symmetry!) to get the area of each tail. But that’s not what we’re here to learn: how do we do it in R? 10.3 The _norm() Functions There are four distinct functions that involve the normal approximation in R: dnorm() returns the output of something called a density function, which is the equation that produces the normal curve. It needs one argument (x), and plugs it into the density equation. By default, the function’s mean and sd arguments are set to be 0 and 1 respectively, however you can override these defaults to be accurate to your data as needed. If x is a vector of numbers (i.e. -3:3), it will return the density function’s output for each number in the vector (thank you vectorization!). This function isn’t incredibly useful in computations, but it’s really useful when you need to plot a normal curve in R. pnorm() returns the cumulative probability of the normal curve at a given Z-score (It’s the area to the left of Z). Graphically, at an arbitrary Z-value, it returns the blue shaded area seen here: Graphical Example of pnorm() Output This is the one that we’re going to want to use the most, but we have to modify it a little bit to reproduce both what we’ve learned in class and from the chart. This will appear later (and make your life easier too), but for now we need to adapt it to find middle areas. Luckily, it’s not a hard conversion, and we’ll make use of our two facts from before. To get the middle area, we first need to realize that the upper bound of it (the right side on the normal curve) will always have a positive Z-score. Consequently, the lower bound will always have a negative Z-score. The symmetry of the curve tells us that Z-score on the right has to be the same (but opposite sign) as the Z-score on the left. This makes our calculation of the middle area easy: pnorm(positive z-score) - pnorm(negative z-score). If you want this value to be a percentage and exactly match the chart, take this output and multiply it by 100. Then, to get the tails, you simply take 100% (or 1 if you’re using the direct output) and subtract away the middle area, divide by two, and you’ve got everything you need. qnorm() does the opposite of pnorm(): you supply it an area-to-the-left (out of 1) for which you’d like to know the corresponding Z-score, and it tells you what that Z-score is. For example, if we wanted to know what area gave us 95% to the left (or a 5% tail) quickly, we can find it with one line of code. qnorm(.95) ## [1] 1.644854 Do a quick check by hand with the chart, and you should see that the middle area of 1.65 does in fact give us an upper (and lower) tail of 5%. rnorm() is basically a random number generator. (The numbers are actually pseudo-random, but the patterns that they come from are not obvious, so we consider them random.) For this class, it won’t be particularly useful, but the more you learn in statistics and R the more useful this function will become. We may use it from time to time going forward to help us do a few things, so it’s worth mentioning now. Also worth mentioning is that this function won’t produce the same output every time you run it since, after all, it is a random number generator. To ensure reproducability of your code, it’s a good habit to set the seed, or starting value, of the random number generator with set.seed(). By supplying it an argument of any integer, it generates that many random numbers from a normal distribution. mean and sd again default to 0 and 1 respectively, but you can override these if you’d like. rnorm(3) ## [1] 0.6456568 -0.4546447 -1.0444065 rnorm(3) ## [1] -0.5182627 0.4242372 0.7175743 rnorm(3) ## [1] 1.2956477 -0.4988964 -1.5136444 Now, if we set a seed, we should get the same results every time as long as we set the seed every time we want the same numbers. set.seed(123456789) rnorm(3) ## [1] 0.5048723 0.3958758 1.4155378 # This one should give different numbers rnorm(3) ## [1] -0.7223243 -0.6183570 -1.5626204 # Back to the same numbers set.seed(123456789) rnorm(3) ## [1] 0.5048723 0.3958758 1.4155378 # One more time set.seed(123456789) rnorm(3) ## [1] 0.5048723 0.3958758 1.4155378 "],
["percentiles-and-box-plots.html", "Chapter 11 Percentiles and Box Plots 11.1 Percentiles 11.2 Quartiles 11.3 Box Plots and boxplot() 11.4 Summary and summary()", " Chapter 11 Percentiles and Box Plots 11.1 Percentiles Normal curves, averages, standard deviations, and Z-scores may seem like they provide all the information needed to understand a simple set of data. But what about when it doesn’t? Take for example a standardized test. We may not necessarily care what percentage a particular student got on the exam, but we’re more likely concerned with comparing that student with the rest of their peers. Are they ahead? Are they behind? Luckily, we can make use of percentiles to help answer these types of questions. What is a percentile? It’s the area to the left of a given Z-score, or the percentage of data less than the one you’re examining. In other words, being in the \\(n\\)th percentile means having a Z-score such that \\(n\\)% of the area is to our left. In our standardized test example, it’s the percentage of people you scored better than. To calculate a percentile by hand, we’d first find the Z-score, then get the corresponding middle areas from our chart, compute the tails, and add the area to the left of our calculated Z-score. Graphically, it looks something like this: Wait a second… We’ve seen this graph before! Where? Oh right, when we talked about the pnorm() function and its output. In fact, this is the output of pnorm(), so appropriately using the pnorm() function will quickly calculate the percentile for us. Pro tip about percentiles: the Z-scores for opposite percentiles (i.e. for the 5th and 95th percentiles) are the same sign but opposite magnitude. To check this, we can pick an arbitrary Z-score (say, 1.7), calculate that percentile, and add the percentile of Z = -1.7, and the results should be 100%. Let’s check: 100 * (pnorm(1.7) + pnorm(-1.7)) ## [1] 100 11.2 Quartiles There’s a few “special” percentiles that we like to use a lot: the 25th, 50th, and 75th percentiles. These are what we call quartiles. As you can see, these quartiles are each quarter of the way across the normal curve. We usually refer to them as Q1, Q2, and Q3 respectively. The interquartile range, or IQR for short, is defined to be Q3 - Q1. We use the IQR to determine if a data point is an outlier. Outliers come in two forms: lower outliers and upper outliers. To check if a point is a lower outlier, it must have a value that is less than Q1 - 1.5 \\(\\cdot\\) IQR. For upper outliers, a point must have a value of Q3 + 1.5 \\(\\cdot\\) IQR. 11.3 Box Plots and boxplot() Quartiles are especially useful when we want to visualize our data in a different way than a histogram or normal curve. We can employ something called a box plot, which visually shows us a summary of our data. Let’s take a look at one and how all its parts fit together. We’ll use the combined survey results from Survey 2 from Fall 2017 to make a histogram of the texts variable. You can download the data here and find the data description here. We could also draw the same plot horizontally. It’s good to be able to analyze a box plot in both ways. To draw these box plots, we just make use of the boxplot() function. If you only wish to view a box plot for a given set of numbers (like we did above), you just need to supply the name of the vector that the data is contained in. R takes care of all of the computations and plotting for you, but you’re more than welcome to play with the colors and labels as you see fit. Box plots, however, are also useful at comparing groups. For example, if we wanted to split our data up according to the gender variable in the dataset, box plots will easily illustrate the differences between the genders. The boxplot() function handles this easily by making use of something called formula syntax. It reads as y ~ x, where y is the variable you want on the \\(y\\)-axis, the ~ means “on” or “versus”, and the x is the variable you’d like on the \\(x\\)-axis. Note: the survey data has been imported as a data frame called survey2. boxplot( survey2$socialMedia ~ survey2$gender, xlab = &#39;Gender&#39;, ylab = &#39;Hours Spent on Social Media&#39;, main = &#39;Male vs. Female Social Media Usage&#39; ) As you can see from this plot, females tend to spend more time on social media than males (see the higher Q2 bar?), although the middle 50% – our good friend the IQR – of each gender is within 3 hours of each other. For females, the IQR goes from 2 to 5, and for males, it goes from 1 to 4. 11.4 Summary and summary() We’ve now got a wide variety of statistics – mean, median, minimum, maximum, Q1 and Q3 – we know how to use and calculate both in R and by hand, but wouldn’t it be nice if there was a way to quickly calculate all of these functions for multiple (numeric) variables in our data frames all at once? Well, guess what? There is! It’s the summary() command, and it does exactly that: provides a summary of the data. Here’s the summary of survey2. summary(survey2) ## gender genderID greek ## Length:1575 Length:1575 Length:1575 ## Class :character Class :character Class :character ## Mode :character Mode :character Mode :character ## ## ## ## homeTown ethnicity religion religious ## Length:1575 Length:1575 Length:1575 Min. : 0.000 ## Class :character Class :character Class :character 1st Qu.: 1.000 ## Mode :character Mode :character Mode :character Median : 4.000 ## Mean : 3.882 ## 3rd Qu.: 6.000 ## Max. :10.000 ## likeMath calculus ACT GPA ## Min. : 0.000 Length:1575 Min. :12.00 Min. :1.000 ## 1st Qu.: 3.000 Class :character 1st Qu.:25.00 1st Qu.:3.200 ## Median : 5.000 Mode :character Median :28.00 Median :3.600 ## Mean : 5.144 Mean :27.65 Mean :3.446 ## 3rd Qu.: 7.000 3rd Qu.:31.00 3rd Qu.:3.800 ## Max. :10.000 Max. :36.00 Max. :4.000 ## partyHr drinks sexPartners relationships ## Min. : 0.000 Min. : 0.000 Min. : 0.000 Min. : 0.000 ## 1st Qu.: 0.000 1st Qu.: 0.000 1st Qu.: 0.000 1st Qu.: 0.000 ## Median : 4.000 Median : 2.000 Median : 1.000 Median : 1.000 ## Mean : 5.426 Mean : 6.554 Mean : 2.924 Mean : 1.241 ## 3rd Qu.: 8.000 3rd Qu.:10.000 3rd Qu.: 3.000 3rd Qu.: 2.000 ## Max. :50.000 Max. :50.000 Max. :50.000 Max. :25.000 ## callParents socialMedia texts goodOrWell ## Min. : 0.000 Min. : 0.000 Min. : 0.000 Min. : 0.000 ## 1st Qu.: 1.000 1st Qu.: 2.000 1st Qu.: 3.000 1st Qu.: 5.000 ## Median : 3.000 Median : 3.000 Median : 5.000 Median : 5.000 ## Mean : 4.435 Mean : 3.352 Mean : 5.627 Mean : 5.554 ## 3rd Qu.: 5.500 3rd Qu.: 4.000 3rd Qu.: 7.000 3rd Qu.: 7.000 ## Max. :50.000 Max. :10.000 Max. :50.000 Max. :10.000 ## expectedIncome president DACA liberal ## Min. : 1 Length:1575 Min. : 0.000 Min. : 0.000 ## 1st Qu.: 80 Class :character 1st Qu.: 0.000 1st Qu.: 2.000 ## Median : 100 Mode :character Median : 0.000 Median : 4.000 ## Mean : 1815 Mean : 2.029 Mean : 3.963 ## 3rd Qu.: 200 3rd Qu.: 4.500 3rd Qu.: 5.000 ## Max. :99999 Max. :10.000 Max. :10.000 ## politicalParty gradeVsLearning parentRelationship workHr ## Length:1575 Min. : 0.000 Min. : 0.000 Min. : 0.000 ## Class :character 1st Qu.: 4.000 1st Qu.: 7.000 1st Qu.: 0.000 ## Mode :character Median : 5.000 Median : 8.000 Median : 0.000 ## Mean : 4.723 Mean : 7.733 Mean : 5.435 ## 3rd Qu.: 6.000 3rd Qu.: 9.000 3rd Qu.:10.000 ## Max. :10.000 Max. :10.000 Max. :50.000 ## tuition career ## Min. : 0.00 Min. : 0.000 ## 1st Qu.: 20.00 1st Qu.: 4.000 ## Median : 90.00 Median : 7.000 ## Mean : 64.93 Mean : 5.988 ## 3rd Qu.:100.00 3rd Qu.: 8.000 ## Max. :100.00 Max. :10.000 There you have it! A quick, easy way to get the information you need about the data that you care about. "],
["scatter-plots-and-correlation.html", "Chapter 12 Scatter Plots and Correlation 12.1 Scatter Plots 12.2 Correlation 12.3 Calculating \\(r\\) and cor() 12.4 Statistics of the “Cloud” Scatter Plot 12.5 The SD Line 12.6 Subsetting and Ecological Correlations 12.7 Summary of Correlation", " Chapter 12 Scatter Plots and Correlation 12.1 Scatter Plots Everything we’ve done to this point has examined one variable of a data set, or things that could be represented by a single vector. While this helps us to understand that one particular variable, it’s much more interesting to us to examine how variables relate to one another. One way to visualize how they relate is through a scatter plot. A scatter plot puts one variable – an independent variable, or predictor, on the \\(x\\)-axis, and a second variable – the response, or dependent variable, on the \\(y\\)-axis. Usually, we’re trying to show how the independent variable explains the dependent variable. Say, for example, we’re trying to find a relationship between a student’s midterm exam score and their final exam score. Let’s have a small class of seven students, with midterm and final scores according to the following table. Midterm Final Student 1 55 62 Student 2 60 50 Student 3 70 65 Student 4 80 70 Student 5 85 95 Student 6 90 80 Student 7 100 90 It’s kind of hard to tell the general trend of the data from just looking at the table, so let’s plot the points. We’ll make the \\(x\\)-axis the midterm scores and the \\(y\\)-axis as the final scores. Note: we have the midterm and final scores stored in a data frame called test_scores. Pro tip: If you’re ever not sure which variable goes where, think about which variable you’d try in predict. In this example, we’re trying to predict a final score from a midterm score, so the final should go on the \\(y\\)-axis. To make the scatter plot, we’ll use the plot function (see ?plot for more information) and make use of the forumla syntax we discussed before. Now, however, we can write it as Dependent variable ~ independent variable. plot(Final ~ Midterm, data = test_scores, pch = 16, # Makes points into closed dots col = &#39;#0088ce&#39;, xlab = &#39;Midterm&#39;, ylab = &#39;Final&#39;, main = &#39;Final vs. Midterm&#39;, cex.axis = 1.5, # Controls font size of axis numbers cex.main = 1.5, # Controls font size of title labels cex.lab = 1.5, # Controls font size of axis label labels cex = 1.5 # Controls size of points ) Now it’s much easier to see! Typically, the better a student did on the midterm (further to the right on the \\(x\\)), the better they did on the final (higher up on the \\(y\\)-axis). We can conclude that an increase in midterm scores is then associated with an increased final score. This is called a positive association. If instead increasing \\(x\\) meant decreasing \\(y\\), we’d call this a negative association. 12.2 Correlation This is great, but it’s kind of general to just talk about associations. How do we tell if an association is strong or not? This is where the idea of correlation comes in. Correlation measures how closely the points follow a line, and they can be summarized by the correlation coefficient, \\(r\\). It does not measure points that are clustered around a curve. A good rule of thumb is that if the data is roughly football shaped, you can use \\(r\\). If the points fall perfectly on a line and are negatively associated, \\(r\\) = -1. If the points fall perfectly on a line and are positively associated, \\(r\\) = +1. If there’s no association between the independent and dependent variables, the correlation is 0. In other words, a correlation of \\(r\\) = \\(\\pm\\) 1 means that you can know exactly what \\(y\\) is for any given \\(x\\) value. Examples are converting units (i.e. temperature from Fahrenheit to Celsius or vice versa) or anything that’s described by a line (i.e. the \\(x\\) and \\(y\\) values from the equation \\(y = 6x + 9\\)). Things that aren’t related, such as weight of college freshmen and ACT scores or class attendance and number of pets you own, have a correlation coefficient of \\(r\\) = 0. 12.3 Calculating \\(r\\) and cor() To calculate the correlation coefficient, just follow a simple 3-step process. Convert both \\(x\\) and \\(y\\) to Z-scores Multiply the Z-scores of \\(x\\) and \\(y\\) together Take the average of the products you just found in step 2 Like calculating a standard deviation by hand, it’s easy to see (and do) the calculation of a \\(r\\) in a table. Let’s make a small data set of 4 students’ scores on quiz 1 and quiz 2. Quiz 1 Quiz 2 Z\\(_\\text{Quiz 1}\\) Z\\(_\\text{Quiz 2}\\) Z\\(_\\text{Quiz 1} \\cdot\\) Z\\(_\\text{Quiz 2}\\) Student 1 10 10 1.2 1.3 1.5 Student 2 9 7 0.8 -0.6 -0.5 Student 3 5 9 -0.8 0.6 -0.5 Student 4 4 6 -1.2 -1.3 1.5 Averaging the last column, Z\\(_\\text{Quiz 1} \\cdot\\) Z\\(_\\text{Quiz 2}\\), we get that the correlation coefficient is 0.5. Of course, R can calculate this for us. We can get \\(r\\) through the cor() function, which just needs the two vectors for which you’d like to calculate the correlation. quiz1 = c(10, 9, 5, 4) quiz2 = c(10, 7, 9, 6) cor(quiz1, quiz2) ## [1] 0.5 12.4 Statistics of the “Cloud” Scatter Plot When our data is roughly football-shaped (like we see below), there are five statistics, called the summary statistics, that we’ll want to pay attention to. The Avg\\(_x\\) is the average (mean) of the variable on the \\(x\\)-axis The Avg\\(_y\\) is the average of the variable on the \\(y\\)-axis The point (Avg\\(_x\\), Avg\\(_y\\)) is referred to as the point of averages The SD\\(_x\\) is the standard deviation of the variable on the \\(x\\)-axis The SD\\(_y\\) is the standard deviation of the variable on the \\(y\\)-axis The correlation coefficient, \\(r\\), which describes how closely \\(x\\) and \\(y\\) follow a line These statistics are usually given to you in class, but you’ll have to calculate them yourself or have R calculate them on any data set in practice! We’ll use the data from Survey 1 (loaded as survey1) as we did before, and focus on the height and weight variables, since these are probably correlated. We’ll make use of the stdv() function we wrote before. (means = c(mean(survey1$height), mean(survey1$weight))) ## [1] 66.89189 147.19165 (stdvs = c(stdv(survey1$height), stdv(survey1$weight))) ## [1] 4.148278 34.183133 (cor = cor(survey1$height, survey1$weight)) ## [1] 0.5676085 Pro tip: enclosing code in () while making an assignment both assigns the variable and displays the calculation Reformatting this information into a table, we’d summarize it as Summary Statistics for height and weight from survey1 Average Standard Deviation height 66.9 4.1 \\(r\\) = 0.6 weight 147.2 34.2 One last step before we start using this information: let’s plot what this looks like so we can see what the data looks like. Let’s let our \\(x\\)-axis be height and our \\(y\\)-axis be weight. plot( weight ~ height, data = survey1, pch = 16, xlab = &#39;Height in Inches&#39;, ylab = &#39;Weight in Pounds&#39;, main = &#39;Height vs. Weight&#39;, col = &#39;#0088ce&#39; ) The data is shaped roughly like a football, so we can continue! First, let’s add in the point of averages that we talked about before. We’ll make that point grey, and show the intersecting lines on the plot with &lt;span style = ’color: #ffcc33&gt;gold and maroon. 12.5 The SD Line The SD line is the line that goes through the tips of the football, passing through the point of averages. Its slope is defined by \\[ \\text{Slope of SD Line} = \\begin{cases} \\frac{\\text{SD}_\\text{y}}{\\text{SD}_\\text{x}} &amp; r &gt; 0 \\\\ -\\frac{\\text{SD}_\\text{y}}{\\text{SD}_\\text{x}} &amp; r &lt; 0 \\end{cases} \\] In our example, the slope of the SD line is thus \\(\\frac{34.2}{4.1} =\\) 8.3. We can add this line to our plot with abline() function. abline() looks for arguments a (the intercept of the line) and b (the slope of the line). We’ve got the slope, but we just need to find the intercept. Some quick algebra gives us the following, assuming we know the slope, average in \\(x\\), and average in \\(y\\). y = slope \\(\\cdot\\) x + intercept Plugging in the fact that the SD line has to go through the point of averages (i.e. the point (Avg\\(_\\text{x}\\), Avg\\(_\\text{y}\\)) has to satisfy the equation we’re looking for), we can plug in (Avg\\(_\\text{x}\\), Avg\\(_\\text{y}\\)) for x and y respectively. Avg\\(_\\text{y}\\) = slope \\(\\cdot\\) Avg\\(_\\text{x}\\) + intercept Avg\\(_\\text{y}\\) - slope \\(\\cdot\\) Avg\\(_\\text{x}\\) = intercept In our case, this gives that 147.2 - (8.3 \\(\\cdot\\) 140) = -404.9. So adding the following line to our plot abline(a = -404.9, b = 8.2) will add the SD line to our plot. 12.6 Subsetting and Ecological Correlations Subsetting Sometimes, we may think that it’s useful to summarize data into groups, then talk about the statistics of that group. For example, we may want to compare how different ethnicities in class scored on their midterms. In this case, we don’t necessarily care about how well each student did, but we’re interested in how groups of them did. This is easy to see with this data from Bonus Survey 4. We’ll import it as survey4, and you can find the description of the dataset here. survey4 = read.csv(&#39;data/Combined Fall 2017 Survey 4.csv&#39;) The ethnicity variable is the one that we’d like to group our data by. ethnicity in this data set can be any one of the following: Black East Asian Hispanic South Asian White Other We have a few options of how to group, or subset, the data, some of which are more memory-intensive than others. Method one is to use the subset() function. If, for example, we wanted to only look at East Asian students’ performance on exam 1, we could write something like east_asian = subset(survey4, survey4$ethnicity == &#39;East Asian&#39;) mean(east_asian$Exam1) ## [1] 89.60363 Note: We used a double equals ( == ) to evaluate the condition, and we passed the condition we were looking for as a string. This takes all of the data from survey4, finds which rows have ethnicity of 'East Asian' (again, matching case), and puts them into their own data frame called east_asian. One great perk of this method is that it’s very easy to keep track of things, since you can then call on parts of the new, subsetted data frame just like you would have on the bigger data frame. Even the names of the features stay the same. However, this method can get fairly memory-intensive since it takes memory to store the data frame we created. The more subsets you’d like to have, the more memory you start to use up, and the slower your code may run. Method 2 is to take advantage of vectorization again. We can subset the data here by using [], $, and even do calculations, all in one step. Let’s try to get the same result as above, but this time we use vectorization. mean(survey4$Exam1[survey4$ethnicity == &#39;East Asian&#39;]) ## [1] 89.60363 If this is confusing, start from the inside and work your way out. First, we found the rows where ethnicity was 'East Asian'. Then, from these row numbers (indexes), we took the exam1 numbers, and lastly took the mean of them. As you can see, they produce the same result, but this one uses less memory and less lines of code. Pretty cool! The last point we should make about subsetting is that using [] is equivalent to subset(). If we wanted to make the same subset using [], we could do something like this. Just so we don’t overwrite east_asian, we’ll call this one e_a, and use the identical() function to check if they’re the same. east_asian = subset(survey4, survey4$ethnicity == &#39;East Asian&#39;) e_a = survey4[survey4$ethnicity == &#39;East Asian&#39;, ] identical(east_asian, e_a) ## [1] TRUE Note: in creating e_a , we had to include the , so that we could get all rows. If you wanted only certain columns, you could put something after the , to get only those columns. Ecological Correrlations Now that we know how to restrict data by conditions, we can go back to our original question of representing a bunch of data by the groups they belong to. If we wanted to visualize the original data by ethnicity, one option we have is to color the points accordingly. Remember, we’re interested in the relationship between scores on exams 1 and 2. If we treat each student as a unique data point, the data has correlation 0.7 plot(Exam2 ~ Exam1, data = survey4, xlab = &#39;Exam 1 Score&#39;, ylab = &#39;Exam 2 Score&#39;, main = &#39;Exam 1 and 2 Scores for All Students&#39;, col = survey4$ethnicity, pch = 16, cex = .8 ) legend(x = 60, y = 35, legend = unique(survey4$ethnicity), col = unique(survey4$ethnicity), pch = 16 ) Another option we have, however, is to condense the plot to be the averages of each ethnicity for each exam. exam_1_means = c(mean(survey4$Exam1[survey4$ethnicity == &#39;Black&#39;]), mean(survey4$Exam1[survey4$ethnicity == &#39;East Asian&#39;]), mean(survey4$Exam1[survey4$ethnicity == &#39;Hispanic&#39;]), mean(survey4$Exam1[survey4$ethnicity == &#39;South Asian&#39;]), mean(survey4$Exam1[survey4$ethnicity == &#39;White&#39;]), mean(survey4$Exam1[survey4$ethnicity == &#39;Other&#39;])) exam_2_means = c(mean(survey4$Exam2[survey4$ethnicity == &#39;Black&#39;]), mean(survey4$Exam2[survey4$ethnicity == &#39;East Asian&#39;]), mean(survey4$Exam2[survey4$ethnicity == &#39;Hispanic&#39;]), mean(survey4$Exam2[survey4$ethnicity == &#39;South Asian&#39;]), mean(survey4$Exam2[survey4$ethnicity == &#39;White&#39;]), mean(survey4$Exam2[survey4$ethnicity == &#39;Other&#39;])) These have a higher correlation (\\(r\\) = 1) and visually look like they follow a line much closer. This makes sense: it takes two points to determine a line. The more points we have, the less they’ll fall on a line unless we know that they were generated by some rule or equation that directly ties them together. In subsetting our data, we went from 1518 data points to only 6. It’s much easier for 6 data points to fall on a line than it is for 1518. This is the effect of an ecological correlation. Higher correlation may seem better, but that’s not always true. By reducing each student to their ethnicity, we have lost a lot of data points from our data set (1512 to be exact). This makes it easy to talk about ethnicities, but we may lose the understanding of the performance of an individual in the class. 12.7 Summary of Correlation Some facts about \\(r\\) to keep in mind: Correlation is not causation \\(r\\) is unitless, since when switching to Z-scores, the data loses its units \\(r\\) does not change by doing any of the following: Adding/subtracting the same number from all values of \\(x\\) and/or \\(y\\) Multiplying all values of \\(x\\) and/or \\(y\\) by the same number. Note: multiplying either \\(x\\) or \\(y\\) (but not both) by a negative number changes the sign of \\(r\\) Changing units of the original data (see fact 2) Switching all of the \\(x\\) values with all of the \\(y\\) values If you don’t believe any of these facts, go ahead and prove them to yourself, or come in to office hours and ask a TA to help you! "],
["linear-regression.html", "Chapter 13 Linear Regression 13.1 Regression 13.2 Using \\(r\\) to Make Predictions 13.3 The Regression Line 13.4 lm() 13.5 Using predict() to Make Predictions 13.6 Switching the Predictions", " Chapter 13 Linear Regression Statistics tries to do two things: Explain the trends we observe within data Make predictions on new data based on the trends we see So far, much of what we’ve covered goes to objective #1. Let’s start working on the second part and using the trends we’ve observed to make predictions. 13.1 Regression Regression is the process of using a relationship between two or more variables (the predictor(s) and response variables) to make a better prediction of the response than guessing. We’re focused on what’s called simple linear regression (SLR), which just means doing regression with one predictor. Knowing the correlation between two variables definitely helps make a better prediction since we know the trend between the two variables, but it doesn’t really help us make a prediction… Yet. Now it’s time to put \\(r\\) to use. What the correlation coefficient really tells us is how much, on average, how many standard deviations of the response we move up (or down) when we increase by one standard deviation in the predictor. Admittedly, that’s confusing to type and involves a lot of math words, and it’s probably easier to understand with an example anyways, so we’ll go right to one. Pretend we’re looking at the relationship between your height and your shoe size. This has a positive correlation (think about it: an NBA player will have a bigger foot than a toddler), and since we’re making this example up, let’s say that the correlation is \\(r\\) = .7. The average shoe size (Avg\\(_\\text{y}\\)) is 10, with standard deviation of .5, and the average height (Avg\\(_\\text{x}\\)) in our data set is 69&quot; with standard deviation 3“. Average Standard Deviation Shoe Size 10 0.5 \\(r\\) = 0.7 Height 69 4.0 What \\(r\\) tells us here is that, for every 4 additional inches taller someone is, we’d expect their feet to be .7 \\(\\cdot\\) .5 sizes bigger on average. This should make some sense with other things we know too: if \\(r\\) was \\(\\pm\\) 1, we know exactly what size shoe you’d wear based on your height every time. If the correlation were 0, we’d have no idea what your shoe size would be so we’d guess the average shoe size. When it’s in between, we don’t know for sure, so we can use \\(r\\) to improve our guess to be somewhere in the middle. As you may have noticed, our guess is always going to be moving closer to the average. This effect, where bottom groups are expected to perform better and top groups are expected to perform worse, is called the regression effect. “This is all great, but I just want to know how to use it to make predictions.” You (we’re guessing) 13.2 Using \\(r\\) to Make Predictions One way to make a regression prediction is to start by converting the \\(x\\) value at which you’re trying to predict be a Z-score. Take this Z-score, multiply it by \\(r\\), and boom! You’ve got your Z-score for \\(y\\). Finally, rearrange the Z-score formula to solve for the Value. This is your regression estimate. Going back to our example above, we can illustrate in a table. We’ll see what the predicted shoe size is for someone that’s 73&quot; tall. Height Z\\(_\\text{Height}\\) \\(r\\) Z\\(_\\text{Shoe Size}\\) Shoe Size 73 1 0.7 0.7 10.35 Like we outlined above: take the first column and convert to a Z-score, then put it in column 2. Multiply the Z-score in column 2 and multiply by \\(r\\), which is in column 3. The product is the new Z-score, which we put in column 4, and then we convert back to a value. We’d predict that someone that’s 73&quot; tall would have a size 10.35 shoe. This process does take a while to complete, however. It’s a multi-step procedure with a lot of switching back and forth between Z-scores and values. It’s pretty easy to make a mistake doing this, so let’s introduce another way to do regression. 13.3 The Regression Line Like we said above, \\(r\\) tells us how much \\(y\\) changes (in terms of standard deviations) for a 1-SD change in \\(x\\). Hopefully, this is screaming “IT’S A SLOPE!” loud and in your face, but in case it isn’t, this is the definition of a slope! Slopes are a characteristic of a line, so we can create a regression line to make predictions quickly and easily for every point we could want. We typically describe lines as taking the form \\[ y = m \\cdot x + b \\] where \\(x\\) is the value we’re making the prediction at, \\(m\\) is the slope of the line (how much \\(y\\) changes for each 1-unit increase in \\(x\\)), and \\(b\\) is the intercept (the predicted \\(y\\) value when \\(x\\) is 0). With what we just realized about \\(r\\), we can define the slope of our new friend – the regression line – to be \\[ r \\cdot \\frac{\\text{SD}_\\text{y}}{\\text{SD}_\\text{x}} \\] Great! Now all we need to do is figure out the intercept of the equation, and we’ll be on auto-pilot for predictions. Luckily, this isn’t hard to do. Just like the SD line, the regression line also has to go through the point of averages. Since this point falls on the line, its \\(x\\) and \\(y\\) values are great to plug in. We then know \\(y\\), \\(m\\), and \\(x\\), which leaves us with one equation with one unknown variable. Thus, we can solve for \\(b\\) and be on our merry way. Let’s try it with the example from above. We’re trying to predict shoe size from height, so our equation should take the form \\[ \\text{Shoe size} = \\left( r \\cdot \\frac{\\text{SD}_\\text{Shoe size}}{\\text{SD}_\\text{Height}} \\right) \\cdot \\text{Height} + \\text{intercept} \\] \\(r\\) was .7, and the SDs for shoe size and height were 4 and .5 respectively. \\(.7 \\cdot \\frac{.5}{4} = 0.0875\\), so this is our slope. We can get the intercept by plugging in 10 for shoe size and 69 for height. \\[ 10 = .0875 \\cdot 69 + \\text{intercept} \\\\ 10 - \\left( .0875 \\cdot 69 \\right) = \\text{intercept} \\\\ \\implies \\text{intercept} = 3.9625 \\] Awesome! Now, let’s make our prediction for a 73&quot; tall person. If everything worked out correctly, we should get 10.35 just like before. (.0875 * 73) + 3.9625 ## [1] 10.35 13.4 lm() While R does make computing the summary statistics for a data set very easy with mean(), sd() (with our modification, of course), and cor(), these can get frustrating to keep typing over and over to get regression equations. This is where the lm() function comes up clutch. lm() stands for linear model, which is exactly what we’ve just created. Essentially, this function allows us to create regression equations quickly and easily, while having additional functionality as well. Part of the reason that we create a model is to be able to use it to make predictions. In order to use it, however, it’s probably a good idea to store the model as a variable. Assignment works the same way it always has. To create a linear model, use formula syntax to specify which variables you’d like as your predictor and response. Note: you could create the model by specifying the predictor (i.e. a variable called predictor) and response (i.e. a variable called response from a data frame called df as lm(df$response ~ df$predictor), however it’s much cleaner and clearer to specify the same thing as lm(response ~ predictor, data = df) . Let’s take our example that we did last chapter one step further. As a refresher, here’s what we were dealing with. Average Standard Deviation height 66.9 4.1 \\(r\\) = 0.6 weight 147.2 34.2 Graphically, we had the following going on. The black line is the SD line, the vertical and horizontal lines are the averages in height and weight respectively, and the grey dot where the lines overlap is the point of averages. As a quick practice round, let’s get the slope and intercept by hand first. \\[ r \\cdot \\frac{\\text{SD}_\\text{weight}}{\\text{SD}_\\text{height}} = 5.00 \\\\ 147.2 = \\left( 5.00 \\cdot 66.9 \\right) + \\text{intercept} \\implies \\text{intercept} = -187.63 \\] Awesome, but annoying to calculate by hand. Let’s make use of lm() and get the same results straight away. We’ll store our model as mod1 so we can refer back to it, and we’ll follow the good coding style we just outlined a minute ago. mod1 = lm(weight ~ height, data = survey1) To see what the model determined for the slope and intercept, we can use the coef() function, and just supply the name of our model to it. coef(mod1) ## (Intercept) height ## -165.680120 4.677275 Hmmm… this is close to what we got, but it’s not exactly the same. Why is that? Easy: rounding. We’ve hidden some unecessary code throughout the book that’s rounded many results for us, but if we wanted to really calculate what the values would be, we’d do this. Note: The stdv() is the user-defined function for standard deviation we wrote in chapter 9. # Get summary statistics average_weight = mean(survey1$weight) average_height = mean(survey1$height) sd_weight = stdv(survey1$weight) sd_height = stdv(survey1$height) r = cor(survey1$weight, survey1$height) # Get slope and intercept slope = r * (sd_weight / sd_height) intercept = average_weight - (slope * average_height) c(intercept, slope) ## [1] -165.680120 4.677275 Since we know now that we’ll get the same result, let’s just keep going with mod1. We can add it to our plot with the abline() function. coef returns a vector, so we can use [] to access its elements. plot(weight ~ height, data = survey1, pch = 16, xlab = &#39;Height in Inches&#39;, ylab = &#39;Weight in Pounds&#39;, main = &#39;Height vs. Weight&#39; ) abline(a = coef(mod1)[1], b = coef(mod1)[2], col = &#39;#e04e39&#39;, lwd = 1.5, lty = 2 ) 13.5 Using predict() to Make Predictions Rather than having to convert to Z-scores to make predictions, we can use the regression line to make predictions. By hand, we’d take our value for \\(x\\) (height in our example), multiply by the slope, add the intercept, and then we’d have our predicted value of \\(y\\). Note: To distinguish the original values in our dataset from our predictions, we often call our predictions \\(\\hat{y}\\). In R, the predict() function allows us to make predictions quickly and easily. To use it, start with the model you’d like to use, followed by passing a data frame of the predictor(s) to predict()’s newdata argument. In the newdata argument, we can either create a data frame with data.frame(), or we can put the name of an already existing data frame. If you’re going to make your own data frame, however, make sure that you specify the names of the values you’re putting in as they were when the model was fit. If we wanted to use mod1 to make a prediction for the weight of a 72&quot; person, we can make use of predict() like this: predict(mod1, newdata = data.frame(height = 72)) ## 1 ## 171.0837 Note: With SLR like we’re doing, it may seem a little annoying to put it into a data frame, but when the problem involves multiple predictors, this can be much easier than typing out the whole calculation by hand. Don’t worry about that case though; that’s a problem for another day. 13.6 Switching the Predictions In the example above, we predicted someone’s weight from their height using two different methods: by using \\(r\\), and by finding the regression equation. Instead, what if we wanted to predict someone’s height from their weight? Would these methods still apply? In short, kind of. Method 1 (using \\(r\\)) works identically, since it’s just based off of the Z-scores of each variable. Just do the exact same process backwards: convert the weight to a Z-score, multiply by \\(r\\) to get the Z-score for height, then convert back to a value. Easy peasy. However, for Method 2, your \\(x\\) and \\(y\\) variables changed. This means that your slope changed, and your intercept changed as well. We would have to repeat the entire process of finding the slope and intercept, then plug in new values to make our predictions. The thing to remember here is that an equation to predict \\(y\\) from \\(x\\) is not the same as the equation to predict \\(x\\) from \\(y\\). "],
["evaluating-predictions.html", "Chapter 14 Evaluating Predictions 14.1 Residuals 14.2 Root Mean Squared Error (RMSE)", " Chapter 14 Evaluating Predictions 14.1 Residuals Not every prediction is going to be perfect. That’s actually built into the fabric of regression! What regression is really doing is predicting the average \\(y\\) for each value of \\(x\\). Have a quick look at the plot from the last chapter. Each of the horizontal lines represents the average of the bin (defined by the dotted, vertical lines) and the regression line is plotted in grey. As you can see, the line predicts the average value for a group of data (for the majority of it, anyways). However, obviously not every point is located right at that group’s average value. This means we’ll have some error in our prediction, called a residual. We calculate a residual to be the actual value minus the predicted value. Points that are above their predicted value (i.e. people that are heavier than we predict them to be) have positive residuals. Points that are below their predicted value have negative residuals. Points that fall on the line, or are exactly their predicted value, have no residual. Under the hood, when we fit the regression line with lm(), R knows to try and minimize the residual value of each point. In fact, the line finds a way to make the residuals sum (and therefore average) to be 0. 14.2 Root Mean Squared Error (RMSE) So, if the average of each \\(x\\) value is really what the regression line is predicting, there’s probably some spread around that prediction. Exactly right! The standard deviation of the prediction errors, or root mean squared error (RMSE) is what you’re thinking of. Just like a standard deviation does with a normal curve, the RMSE lets us talk about a range of prediction errors and attach wiggle room to our predictions. To get the formula for this new RMSE thing, reverse the order of the words in the name. Start by finding the prediction error. That is, for each \\(y\\) value in our data (we’ll call it \\(y_i\\)), subtract off the predicted value. We called a prediction \\(\\hat{y}\\) earlier, so we’ll call each individual prediction \\(\\hat{y}_i\\) now. \\[ \\text{Prediction Error} = \\text{Actual} - \\text{Predicted} = y_i - \\hat{y}_i \\] Then, square each prediction error: \\[ \\left( y_i - \\hat{y}_i \\right) ^ 2 \\] Take the mean of these squared errors. We’ll assume that there’s \\(n\\) predictions (read: observations) in our data. \\[ \\frac{1}{n} \\sum_{i = 1} ^ n \\left( y_i - \\hat{y}_i \\right) ^ 2 \\] Finally, to get the RMSE, take the square root of this. \\[ \\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i = 1} ^ n \\left( y_i - \\hat{y}_i \\right) ^ 2} \\] While this may seem daunting, this is one way to calculate the RMSE. It’s easy to remember as long as you read the name in reverse order. However, there’s another way to calculate the RMSE that we talk about in class. \\[ \\text{RMSE} = \\sqrt{1 - r ^ 2} \\cdot \\text{SD}_y \\] They should give us the same number, but do they? We’ll find the RMSE of the data we’ve been working with to check. Remember, we’re predicting weight from height in survey1, so weight is our \\(y\\) variable. We’ll make use of R’s vectorization again to do our calculations. Make sure that you pay attention to where the parentheses go! # Method 1 sqrt(mean((survey1$weight - predict(mod1, newdata = survey1)) ^ 2 )) ## [1] 28.14292 # Method 2 sqrt(1 - cor(survey1$weight, survey1$height) ^ 2) * stdv(survey1$weight) ## [1] 28.14292 Pam, your thoughts? The same rules apply to the RMSE that applied to standard deviations. 68% of our data will fall within 1 RMSE of its predicted value, 95% will fall within 2 RMSEs of its predicted value, and 99% will fall within 3 RMSEs of what we predict. To see it graphically, look no further. "],
["uses-of-r.html", "Chapter 15 Uses of R", " Chapter 15 Uses of R Now’s a good time to change gears for a little. While we’ve done a lot already, it may seem like R is just good at helping with homework, speeding up calculations involving data. But it’s easy to make use of R when you’re instructed on how to do it and see specific use cases. At its core, R is great at computing statistics and making plots. After all, it was built by statisticians, for statisticians. But like any language, R has grown to be great for so much more than what you’ve seen. It has the power to manipulate entire datasets, create documents (like this entire book, your homework assignments, etc.), and even build websites and applications. We’ll cover how to do some of this in this section of the book. If you want to think of yourself as an R doctor, all of the patients (datasets) you’ve seen so far have come in healthy. You’ve learned how to find the information needed for their charts, and you should be very comfortable with how a healthy check-up should go. But what do you do when a patient comes in sick? They’ve got something wrong with them, and it’s your job to figure out what it is and how to fix it. This diagnosing process, as well as what we do to cure the patient, is known as data cleaning. What we mean is that, in practice, data sets that we’d like to analyze are rarely clean when we first get them. Values can be of the wrong type, be missing altogether, or not be in the formats we want them to be. No worries though! We’ll see how to use R to do all of this for us! Ready? Let’s ride. "],
["for-loops.html", "Chapter 16 for Loops 16.1 Algorithms 16.2 Syntax 16.3 Nesting", " Chapter 16 for Loops In order to clean up our datasets, we’ll have to make use of control structures. This sounds super technical, but really what they help us do is navigate our way through the data in an organized, controlled fashion. They can do anything from helping us perform a series of operations on each observation to doing different operations based on criteria that we determine. There’s two main components to control structures: for loops and conditional statements. for loops allow us to iterate, or cycle through, our data set. Inside of a for loop, we can create variables, do calculations, or even put other for loops (this is called nesting, and we’ll come back to it in a little bit). We’ll do a few simple examples of for loops just to demonstrate their usefulness. Let’s take a vector of numbers called v1 and use a for loop to do something simple: add 1 to each of the numbers. 16.1 Algorithms Now it’s time to think like a computer and devise an algorithm, or set of strict steps. “Legal” operations are anything you’d be able to do by hand, such as: “Getting” a number, which just means seeing what the number is “Saving” a number, since you could write it down on a piece of paper to remember it “Repeating” any step, since this is what for loops allow us to do Any mathematical operation Checking a condition (but more on that later) Our algorithm for this process looks like this: Get the first number in the vector v1 Add 1 to this number Save this new number in the same place as the original number Repeat for all remaining numbers Note: While using vectorization is the faster, more elegant way to do this same task, this is still possible (and easy to understand) in a for loop. (v1 = seq(1, 100, by = 7)) ## [1] 1 8 15 22 29 36 43 50 57 64 71 78 85 92 99 16.2 Syntax To declare or initialize (make use of) a for loop, start the line with the word for, followed immediately by parentheses (()). for() Inside of the parentheses, we need to make use of an iterable index. You should be familiar with indexes from before, but as a refresher, the index is just the location in the vector/data frame of the point that we’re talking about. We’ve been using indicies that we knew we wanted (i.e. the 3rd location we used [3]), but now we want this iterable index to be “generally specific.” That is, we want it to be specific enough to know what location we’re at, but general enough so we don’t have to write the same line of code for each different index. Essentially, this iterable index is just a variable, and each time we go through the for loop, the variable will increase by 1. We’ll call this iterable index i. The other thing we need to supply is the boundaries (that are inclusive) we want to iterate over. If we want to go over all of the elements of v1, for example, we’d want to specify that by putting in 1:15, since there are 15 elements in v1. Pro tip: it’s a better idea to fill in the boundries as 1:length(v1) in case we need to change the number of elements in v1 (maybe we meant to put 10 or 1000 instead of 100?). Writing the boundaries in this way is more flexible in the code while still providing the same desired output, so that’s what we’ll do. To finish off this line and specify the exact operations we’d like to do to v1, we need to enclose the operations in curly braces ({}). Each time R hits that last, closing brace, it will increase i by 1 and go back to the top. As long as i stays within the boundaries, R will keep running the code inside the loop. All together, we’ve got this as our for loop declaration: for(i in 1:length(v1)){} Inside of the curly braces is where the metaphorical magic happens. We’ll put our operations in there, and let R handle the rest. We want to add 1 to each element of v1 and save it in its same location. Step-by-step, we’re saying to replace the ith element of v1 with 1 + the ith element of v1. for(i in 1:length(v1)){v1[i] = v1[i] + 1} v1 ## [1] 2 9 16 23 30 37 44 51 58 65 72 79 86 93 100 This is gross-looking, so let’s practice good coding style like we talked about earlier and show good spacing and indentation. Since we just acted on the initial v1 and made our replacements in-place, we need to recreate the original v1 vector. v1 = seq(1, 100, by = 7) for(i in 1:length(v1)){ v1[i] = v1[i] + 1 } v1 ## [1] 2 9 16 23 30 37 44 51 58 65 72 79 86 93 100 Another quick example following: let’s reverse the order of a set of numbers. We’ll create a vector called numbers, which will have the numbers 1 through 10. Our goal is to reverse the order. Think like a computer and see if you agree with our algorithm: Get the first number from numbers Get the last number from numbers (since this is the value we want to switch the first value with) and save it in a variable called temp (since if we just put the first value in here, we’d lose the value it originally had) Replace the last item with the first item Put the value from temp into the current location Repeat steps 1-4 until we get to the halfway point of numbers (since then we’d start switching everything back) All together, this is the resulting for loop. Match it up to make sure you agree! (numbers = 1:10) ## [1] 1 2 3 4 5 6 7 8 9 10 for(i in 1:(length(numbers) / 2)){ temp = numbers[length(numbers) - i + 1] numbers[length(numbers) - i + 1] = numbers[i] numbers[i] = temp } numbers ## [1] 10 9 8 7 6 5 4 3 2 1 Note: We used the index length(numbers) - i + 1 since the first element we wanted to replace was the last element, or at position length(numbers). Since i started at 1, and we needed it to change each time, we subtracted i and then added the 1 back in to take care of the first case. You can trace (follow) it if you write out the values that i takes in each iteration. 16.3 Nesting The last point we’ll make about loops is that you can nest them, or put a for loop inside of another for loop. Just be sure that your indexes are different! Some example of when you may want to nest two for loops would be if you wanted to count the number of matching values in two different vectors, or if you wanted to go through each feature of each observation (i.e. go through each row, and within each row, go through each value) of a data frame. Again, be careful with your indentation. Check out this example, where we check go through the rows and columns of a data frame and add the row number plus the column number to it. That is, the data in the ith row and the jth column will have i + j added to it. First, we need to create all of the vectors and merge them together as a data frame: multiplication_table = data.frame( ones = seq(1, 12, by = 1), twos = seq(2, 24, by = 2), threes = seq(3, 36, by = 3), fours = seq(4, 48, by = 4), fives = seq(5, 60, by = 5), sixes = seq(6, 72, by = 6), sevens = seq(7, 84, by = 7), eights = seq(8, 96, by = 8), nines = seq(9, 108, by = 9), tens = seq(10, 120, by = 10), elevens = seq(11, 132, by = 11), twelves = seq(12, 144, by = 12) ) Multiplication Table for Numbers 1 through 12 1 2 3 4 5 6 7 8 9 10 11 12 1 1 2 3 4 5 6 7 8 9 10 11 12 2 2 4 6 8 10 12 14 16 18 20 22 24 3 3 6 9 12 15 18 21 24 27 30 33 36 4 4 8 12 16 20 24 28 32 36 40 44 48 5 5 10 15 20 25 30 35 40 45 50 55 60 6 6 12 18 24 30 36 42 48 54 60 66 72 7 7 14 21 28 35 42 49 56 63 70 77 84 8 8 16 24 32 40 48 56 64 72 80 88 96 9 9 18 27 36 45 54 63 72 81 90 99 108 10 10 20 30 40 50 60 70 80 90 100 110 120 11 11 22 33 44 55 66 77 88 99 110 121 132 12 12 24 36 48 60 72 84 96 108 120 132 144 Now, our nested loops: for(i in 1:nrow(multiplication_table)){ for(j in 1:ncol(multiplication_table)){ multiplication_table[i, j] = multiplication_table[i, j] + i + j } } After Nested for Loops 1 2 3 4 5 6 7 8 9 10 11 12 1 3 5 7 9 11 13 15 17 19 21 23 25 2 5 8 11 14 17 20 23 26 29 32 35 38 3 7 11 15 19 23 27 31 35 39 43 47 51 4 9 14 19 24 29 34 39 44 49 54 59 64 5 11 17 23 29 35 41 47 53 59 65 71 77 6 13 20 27 34 41 48 55 62 69 76 83 90 7 15 23 31 39 47 55 63 71 79 87 95 103 8 17 26 35 44 53 62 71 80 89 98 107 116 9 19 29 39 49 59 69 79 89 99 109 119 129 10 21 32 43 54 65 76 87 98 109 120 131 142 11 23 35 47 59 71 83 95 107 119 131 143 155 12 25 38 51 64 77 90 103 116 129 142 155 168 Great, everything worked! To check if it did, just take any number in the table, and subtract off the row and column numbers. This may seem like a silly example, and it admittedly is, but nested for loops work even better when we want to only operate on data that meet certain conditions. You’ll learn about this in the next chapter. "],
["conditional-statements.html", "Chapter 17 Conditional Statements 17.1 Simple Conditions 17.2 Compound Conditions 17.3 else and else if 17.4 Extra Things for Conditionals", " Chapter 17 Conditional Statements A programmer is going to the grocery store and his wife tells him, “Buy a gallon of milk, and if there are eggs, buy a dozen.” So the programmer goes, buys everything, and drives back to his house. Upon arrival, his wife angrily asks him, “Why did you get 13 gallons of milk?” The programmer says, “There were eggs!” 17.1 Simple Conditions Going hand in hand with for loops are what we call conditional statements, or statements that evaluate to TRUE or FALSE. These are really important, especially inside of for loops (and nested for loops too!), because what they allow us to do is operate only on certain observations of data that meet our criteria. You can think of conditionals like flow charts, where the statement is the node (splitting point), and the operations and commands are what R does down the corresponding path. They’re essentially a bunch of statements that say “If my condition is true, then do this set of operations.” We’ve been using a few conditional statements all along, we just didn’t realize it. For example, after each iteration of our for loops, R checks the boundaries. If the next index is still within those boundaries, then R goes through the loop. Otherwise, it exits out of the loop and goes to the next line of code. Conditional statements are very similar to for loops. One way to use a conditional statement is by using the ifelse() function. The three arguments you’ll need (in order) are the condition you’d like to check, what to do if the condition is met, and what to do if the condition isn’t met. As a quick vectorized example, let’s say we have a vector x that has the numbers 1 through 10. We want to change all the numbers that are greater than (but not equal to) 6 to just be 6, otherwise we want to keep the value that’s in x. ifelse() is great at this! Take a look. x = 1:10 ifelse(x &gt; 6, 6, x) ## [1] 1 2 3 4 5 6 6 6 6 6 Another way to insert a conditional statementis to use if() and put your condition(s) inside the parenthesis. Then, to specify what to do when that condition is met, use a set of curly braces ({}) and put the code you’d like to run inside of those braces. Let’s go back to the example from the last chapter with that multiplication table. 1 2 3 4 5 6 7 8 9 10 11 12 1 1 2 3 4 5 6 7 8 9 10 11 12 2 2 4 6 8 10 12 14 16 18 20 22 24 3 3 6 9 12 15 18 21 24 27 30 33 36 4 4 8 12 16 20 24 28 32 36 40 44 48 5 5 10 15 20 25 30 35 40 45 50 55 60 6 6 12 18 24 30 36 42 48 54 60 66 72 7 7 14 21 28 35 42 49 56 63 70 77 84 8 8 16 24 32 40 48 56 64 72 80 88 96 9 9 18 27 36 45 54 63 72 81 90 99 108 10 10 20 30 40 50 60 70 80 90 100 110 120 11 11 22 33 44 55 66 77 88 99 110 121 132 12 12 24 36 48 60 72 84 96 108 120 132 144 This time, however, let’s get more creative with our operation. Let’s replace the square numbers in the table with their square root. That is, if i and j are the same number, put i in that place instead of the squared value. We’ll have to use a nested loop again, but now we can check the condition and act accordingly. for(i in 1:nrow(multiplication_table)){ for(j in 1:ncol(multiplication_table)){ if(i == j){ multiplication_table[i, j] = i } } } 1 2 3 4 5 6 7 8 9 10 11 12 1 1 2 3 4 5 6 7 8 9 10 11 12 2 2 2 6 8 10 12 14 16 18 20 22 24 3 3 6 3 12 15 18 21 24 27 30 33 36 4 4 8 12 4 20 24 28 32 36 40 44 48 5 5 10 15 20 5 30 35 40 45 50 55 60 6 6 12 18 24 30 6 42 48 54 60 66 72 7 7 14 21 28 35 42 7 56 63 70 77 84 8 8 16 24 32 40 48 56 8 72 80 88 96 9 9 18 27 36 45 54 63 72 9 90 99 108 10 10 20 30 40 50 60 70 80 90 10 110 120 11 11 22 33 44 55 66 77 88 99 110 11 132 12 12 24 36 48 60 72 84 96 108 120 132 12 Cool, it worked flawlessly! Also, make sure you see how the code is styled. The closing braces all line up vertically with what they open, and after each opening brace, we went to the next line and indented one tab. This makes it easy to see what happens where in the code, and allows us to easily edit it as we need to. Conditinoal statements are sometimes a little tricky though. For example, let’s say you want to check if the \\(i\\)th element of a vector called x is NA. You may think that you be tempted to write something like if(x[i] == NA), but this is WRONG! Instead, you want to use the is.na() function. The line should read if(is.na(x[i])). 17.2 Compound Conditions Compound conditions are just combining conditions with &amp; or | (“and” and “or” in R). You can make conditions as compounded as you want, but just be careful that they don’t get to be too complex. If you’re trying to hone in on one particular point in your data, it may be a better idea to just manipulate that one point. It’s also a good idea to use parentheses extensively when writing your conditions, that way there’s no confusion as to which compounds go together. As an example, if you want to execute code if condition a is met and either b or conditon c are also met, you should write this condition as if(a &amp; (b | c)) where a, b, and c are the conditions that you want to be met. Compounding conditions can help you to cover a lot of cases in your data, but the expressions can get confusing quick. Since conditional statements can be nested, it may be better to take the bigger conditions and then nest another conditional statement inside of the broader one. One very important thing about conditional statements (and really all code in general) is that they’re evaluated and executed in the exact order that you specify them in. That’s why it’s important to import a dataset before trying to do any kind of manipulation on it, or why you should have a variable declared before you try and use it. Conditional statements are no different. If we write the following pseudocode (code syntax but regular words), for(i in 1:10){ if(condition A){ Do something here } if(condition B){ Do something different here } } R will start by evaluating condition A, and then move to condition B. If we flipped them, R would evaluate condition B first. Be careful when you specify the conditions! If what you want to do where it says Do something different here depends on something you did where condition A was met, you may not get the result you wanted at the end. In this example, it’s also entirely possible we could meet condition A and condition B, but really we’d like to handle those conditions differently. That’s where else if comes in handy. 17.3 else and else if When we write these conditional statements, we may have more than two possible scenarios that we want to address. This is where else if makes itself useful. This basically says “If the condition we just checked isn’t met, but this new condition is met, do this thing I’m going to tell you to do.” else alone says “If none of the other conditions are met, do this last thing.” We can deploy them (in pseudocode) as follows: for(i in 1:10){ if(condition A){ Do stuff } else if(condition B){ Do other stuff } else if(condition C){ Do something different } else{ If conditions A, B, and C are not met, execute whatever is here } } 17.4 Extra Things for Conditionals A few quick points things to remember: ! is called the negation operator. It’s the R equivalent of the word “not.” That’s why != is “does not equal.” However, it works with functions like is.na() as well. So if you wanted to do some calculations only if the \\(i\\)th element of x is not NA, you can count on if(!is.na(x[i])) to do the job. %in% is a shortcut way to check if a value is in another vector. If we have a variable called temp, and we want to know if temp is in a vector v1, writing if(temp %in% v1) will do the trick. You can negate this by using ! and enclosing your conditions "],
["functions-1.html", "Chapter 18 Functions 18.1 When to Write A Function 18.2 Parts of a Function 18.3 Scope 18.4 Arguments 18.5 Return Statements 18.6 Writing Your Own Functions", " Chapter 18 Functions We’ve been using a ton of functions throughout the text, and we were very quickly introduced to them in chapter 2, but as promised, we’re going to take a deep dive into functions so that we can write and use our own. We’ll go into more depth about things that we’ve already talked about and show you how to combine everything to make a function to do anything you’d like it to. 18.1 When to Write A Function Functions are super useful whenever there’s a calculation that we’re going to want to repeat on a variety of different things. Remember that stdv() function we wrote before? That’s a perfect example of a time to write a function, since that equation/conversion is something that we’re going to want to do over and over again. The thing to be careful of, however, is that functions only work when they’re either imported from a package or if you write them in the same file that you’re working in. If you write a function in a file called script1.R, and you want to use it in script2.R, you have to redefine the function in script2.R. 18.2 Parts of a Function There’s three main parts of a user-defined function. The first part we’ll talk about is the function’s declaration, or how specifically we create it. We do this by using the function() function, assiging the function a name (just like we do with variables and data frames), and declaring what its arguments will be. Just like a for loop or a conditional statement, end the line with a set of curly braces ({}) The next part of a function is its body. It’s what goes in the {} and is what you want the function to do. This is where you’ll want to put any manipulations, calculations, loops, conditionals, or anything else that your function needs to do its job. It can be as long or short as you want it to be, but if you don’t specify what you’d like the function to do in its body, you can bet that it won’t do it. The last thing that every function needs is something to return, or some kind of output. This may seem kind of obvious, but the reason we want to write functions is to give us something back. This can be anything: a list, a data frame, a plot, a model, or any number of other things we want it to give us. 18.3 Scope If you want to think of a function as a black box, the only things that the function can use are anything that’s already inside of the box (the function’s body), and anything that we add into the box (the function’s arguments). Anything that’s inside the box never gets to see anything outside the box, and anything that’s outside the box never gets to see the inside of it. And unless we allow something to leave the box, anything inside of it will stay inside of it. This is the idea of scope in a nutshell. It’s important to realize that any variable you create inside of a function will only be accessible outside of it if you allow it to come out. And anything that you create outside the function will not be used inside the function unless you specifically tell the function to use it. This is true of for loops and conditional statements as well. Here’s the brief pseudocoded skeleton so you can see it and not just read it: someVariable = Some variable is defined here someFunction = function(arguments get put here){ This is where the body of the function goes temp = Some temporary variable we wanted to create Now we decide what we want to return and put it here } temp is not accessible here The variable temp is only accessible inside the body of someFunction since that’s where it was declared. We say that it’s inside the scope of someFunction, but unless we make reference to it inside of someFunction, we say that someVariable is outside the scope of someFunction (since it was declared outside of the body of someFunction). 18.4 Arguments We’ve been talking a lot about arguments throughout the book, but what exactly are they? And how can we use them to help us write our own functions? Well, arguments are the parts of the function that can change each time you run it, but the operations that you do to them will be the same. In a sense, they’re their own variables. Each time you call the function, you’ll most likely want to specify what some (if not all) of the arguments are. Everything about an argument in a function you write – the name, the order, how they’re used, what type of data it can be, etc. – is completely up to you! Just make sure that they do what you want them to do. 18.5 Return Statements The easiest way to specify the return of your function is to simply make it the last line of the function’s body. When you do this, you don’t have to save it as a variable. Simply put whatever you want the output to be, followed by a closing brace }. (If you’re using good coding style, that brace should be on the next line.) Another option you have is to use return(). This just explicitly tells R to return whatever you put inside of the parenthesis. Most of the time, however, going with the first method will be easier and faster. 18.6 Writing Your Own Functions Woo! Now that we understand what a function is and what it’s made up of, it’s time to start writing your own functions. To write your own function, the procedure to follow is this: Write out your algorithm. This will save you so much time if you have a clear roadmap of what your function needs to do, and the order in which it needs to do it. Declare your function by giving it a name and using the function() function. If there’s any arguments you know from the get-go you’ll need, put those in. Writing the body of the function. If you realize as you’re writing that you need to add an argument, make sure you add it to your arguments inside of function(). After you finish writing the body, make sure it returns what you’d like it to return (i.e. if you want it to return a number, make sure it’s not returning a character) Test it out. This step allows you to go through and find mistakes in your function. Functions are great because they can (or at least should) work on small and large sets of data, so using a handful of small testing scenarios to make sure there’s no bugs, or problems, in your code is always a good idea. If there are, make sure you address them! Let’s start with a simple example: a function that takes a set of numbers and returns the average of those numbers. Sure, the mean() function does this already, but let’s write a simple function that can do the same thing. First thing’s first: we need to come up with our algorithm: Get set of numbers Add up all the numbers in the set Divide the sum from step 2 by how many numbers there are in our set Give back the result of step 3 as our answer. Now, we need a name to make our declaration. How about averageCalc()? Great. Declare it. averageCalc = function(){ } Step 1 of our algorithm tells us we need to get a set of numbers to work with. This is going to be an argument of the function, since we want averageCalc() to work on any set of numbers we pass to it. Remember, names of arguments should be descriptive, but concise. Since it’s just a set of numbers, why not call it nums? Sounds good to us! averageCalc = function(nums){ } Step 2 says we need to add up the numbers in our set, nums. And we know that R has a function, sum() that adds up all of the numbers in a vector! We’ll want to use this sum later (in step 3), so we should probably store it as something. We’ll call it total, since it’s the total sum of the numbers in nums. averageCalc = function(nums){ total = sum(nums) } Now, we need to divide total by how many numbers there are in nums, but how many numbers are in nums? We don’t know, so we should be generally specific: general enough where we know that it will work whether there’s 10 or 10,000 numbers in the set, but specific enough where we know what exactly what we’re doing with it. Luckily, the length() function tells us how many numbers there are in a vector, so we can use it here. We’ll store this length in a variable called length_nums. averageCalc = function(nums){ total = sum(nums) length_nums = length(nums) } Lastly, we need to return total divided by length_nums. We know that if we want to return something, we can just put it as the last line of our function’s body and not assign it a variable name. This gives us the complete function definition: averageCalc = function(nums){ total = sum(nums) length_nums = length(nums) total / length_nums } Now let’s make sure it worked. We’ll use a vector of length 1 (averageCalc should just return the number) and the numbers 1, 2, and 3 (we should get 2, since 1 + 2 + 3 = 6, and 6 \\(\\div\\) 3 = 2). Just call your function, and pass the numbers in place of nums. To call your function, just type the name, immediately followed by a set of parentheses, and put the arguments you need inside the parentheses averageCalc(12) ## [1] 12 averageCalc(c(1, 2, 3)) ## [1] 2 The more variables you create inside of your function, however, the slower they run and the more memory they use up. That’s not to say that you should never create variables inside of your functions. You should! That’s what they’re there for! But an equally good function we could have written would be averageCalc2.0: averageCalc2.0 = function(nums){ sum(nums) / length(nums) } averageCalc2.0(12) ## [1] 12 averageCalc2.0(c(1, 2, 3)) ## [1] 2 Beautiful, it all works! Functions are meant to make your life a lot easier, so the better you get at writing functions, the more the world of R will open up for you. You’ll get plenty of pratice writing functions in the homework for this section. "],
["document-creation.html", "Chapter 19 Document Creation 19.1 Knitting 19.2 Document Elements 19.3 Text Formatting, Lists Links, and Images 19.4 Code Chunks 19.5 \\(\\LaTeX\\) and Equation Formatting 19.6 HTML Tags and CSS 19.7 Cheat Sheets", " Chapter 19 Document Creation Another great feature of R is the ability to make nicely-formatted documents through the use of the rmarkdown package. This allows you to become a literate programmer, where you can write code and explain what you’re doing, why you’re doing it, and explain the results all in the same place. Before we can really start showing how powerful these documents are, you need to install rmarkdown. install.packages(&#39;rmarkdown&#39;) This package allows you to create an R markdown document. (They’ll have the file extension .Rmd). We’ll refer to them as markdown documents since they make use of markdown syntax (we’ll go over this; hang tight for now!). They allow you to weave together code and commentary/explanation into a single file, allowing you to explain your work as you go. To create a markdown document, go to File &gt; New File &gt; R Markdown.... R Markdown documents can be rendered, or put together, into 3 formats: HTML, PDF, and Word. When you create your document, you’ll be asked which format you’d like it to render as. For our purposes, we’ll use HTML, since they open very nicely in a web browser. You’ll be asked to pick a format when you create a new markdown file. You’ll also be asked to title your document. Don’t freak out about naming it; you can always change it later. Just to give you an idea of how powerful R Markdown is, it’s what was used to create this entire textbook. Seriously! If you want to see them, you can find the source files by clicking this link. To download them, click any file that ends in .Rmd, then right click the Raw button, and click Save Link As.... This will save the file for you to see how the whole document is put together. Note: the markdown files that make up this book are actually from a package called bookdown , which you can read more about here if you want to learn more. While bookdown is way beyond the focus of this project, it’s a really cool way to combine markdown files. Since this is going to be one of the longer chapters in the book, we’ve provided a downloadable example for you to look at as you go. While the content of the document itself doesn’t exactly mean anything, we’ve tried to demonstrate as much of the formatting we cover and usefulness of R Markdown as we possibly can within it. 19.1 Knitting We’re going to start a little bit backwards and teach you how to knit a document first before we teach you how to write them and make good use of them. In R, knitting a document means knitting together the code and the context that the document contains and rendering the document in the format that you specified when you created it. You can render it in other formats after you create it too! To knit your document, you can use the knit button . Pro tip: you can also knit your documents using keyboard shortcuts. On a Mac, use Cmd + Shift + K, or on a PC, use Ctrl + Shift + K. We have one more point to make before we go and start with the components of a markdown document, and that is where the markdown file looks for files that it needs to run (like data files, images, etc.). When knitting your document, R will look for any files in the same directory that the document is contained in unless you tell it otherwise. What this means is that if the markdown file is contained in the directory /User/Desktop/STAT 100, R will automatically look for any files just specified by name in the /User/Desktop/STAT 100 directory. If inside of this directory, there’s a folder called data that we want to get data from (the data is saved as data-to-use.csv), we can specify a relative filepath, or a filepath that’s relative to the one that we’re working in. We could say data = read.csv('data/data-to-use.csv') in our file, and it would work identically to data = read.csv('User/Desktop/STAT 100/data/data-to-use.csv'). It’s preferred to use the relative filepath, since the full one may be more specific to your computer. Now that we know how to create and render a markdown file, let’s take a closer look at its parts and how to make the best use of them. 19.2 Document Elements When you create a document, the first thing that gets put at the top is what’s called a YAML header. This header contains some very basic information about the document that follows, such as the author, the title of the document, the date it was created, and what kind of output should be created. You’re welcome to edit this header as you need to. For example, if you’d like to change the title from the one you originally gave, just change what goes inside the quotation marks next to title: in the YAML header. This header is always enclosed by three dashes on each side of it (---). In your YAML header, you can also create a table of contents, like so: --- title: &quot;Your title here&quot; author: &quot;Your name here&quot; output: html_document: toc: yes --- Any first-level section headers (which you’ll read about in a minute) will be used as the indexes, with second-level headers being nested inside of those, and so on. Next, we can put (section) headers to create different sections of our document. To create a header, we start the line with a pound sign (#), then follow it with the header you’d like to use. Headers of the same level nest inside each other. Depending on the number of pound signs you use, your header will be a different level, or size. More #s means a smaller size header, or a lower header level. Play around with them to see what we mean! # This is a first-level header ## This is a second-level header ## A nested second-level header here ### This is a third-level header # Now we&#39;re back to a first-level header After a header, we may want to give a little overview of what we’re going to be talking about throughout the rest of the document. There’s nothing really special about this body of text, but we know that it’s not meant to be interpreted as a section header or as code. We can just type regular text in the section, skipping 2 lines to do so. For example, if our header is “Overview”, we could put this into our markdown document and start writing our problem up. # Overview In this section, we&#39;ll provide a brief overview of the problem. We used methods x, y, and z to solve it, and we&#39;re having a lot of fun writing up this markdown document to explain what we did as we did it! 19.3 Text Formatting, Lists Links, and Images There’s also a few special characters in markdown that you may find helpful as you write your documents. If you use reddit, these should all be fairly familiar, as this is how you can create and format a text post. One of the most common is the asterisk (*). Surrounding any regular text in a single set of asterisks italicizes the text. For example, *this text will be italicized* will appear as this text will be italicized in your final document. Using two asterisks will keep the text in a regular font, but will make it bold. **Bolded Phrase** will become Bolded Phrase. Using a triple set of asterisks will both italicize and bold the enclosed text: ***bolded and italicized*** becomes bolded and italicized. Putting three (or more) asterisks on their own line will create a horizontal line on the page, like the one right below this paragraph. Note: instead of using asterisks, you could also use the same number of underscores (_) to accomplish the same stylization. Another common markdown character is the caret (^). Enclosing text in a single set of carets will superscript the text. As an example, i^th^ will become ith when we’re all done here. The last common character is the tilde (~) character. A single set will subscript the enclosed text (X~i~ will look like Xi), and a double set of tildes strikes through whatever it encloses. To see an example, include ~~strikethrough~~ in your document and it should appear as strikethrough. Starting a line with &gt; will make it look like a block quote, and will appear like this line in your text. Should you want to provide a hyperlink to something in your document, put the text you’d like displayed inside a single set of square brackets ([]), followed immediately by a set of parentheses that contain the link you’d like it to go to. If we wanted a link that says “Click Here” and have it take us to Google, for example, we’d put [Click Here](www.google.com) into our document. We can also link to files using the same syntax, but instead of putting a website in the parentheses, we’d put the filepath. Images work the same way as links, we just lead with a !. So, if we have an image file called image-to-use.png in a subdirectory called img that’s inside our working directory, we’d want to put ![](img/image-to-use.png) in our text where we want the image to be displayed. If you want your image to have a caption, you can put it inside of the brackets. Items in a bulleted, unnumbered/unordered list should begin with - and each be put on their own line. To create a sublisted item, move down two lines, tab twice, and put another -, like so: - Main item - Sub-item 1 Numbered lists work the same way, but instead of starting the item with a -, start the item with the number it should take in the list. 19.4 Code Chunks Great! Now we can write as much text as we want, but how’s that better than a Word document? Well, Word documents can’t process R code like R markdown documents can. To include code, you need to make use of a code chunk. You can create them by creating a fence, which is just three backticks (`) in a row. Chunks need a chunk header, which just tells the document that whatever’s inside the fence should be processed as R code. The header goes next to the opening fence and inside a set of curly braces ({}), and should begin with r. Inside this header, you can include a chunk label and other chunk options that help control output (these should be separated by a comma). While only the r is necessary, the chunk label helps you to remember what that chunk does, and the chunk options allow you to format the code and output accordingly. You can learn more about chunk options here. A template of a code chunk with a label and chunk options While chunks can contain as much or as little code as you’d like them to, it’s a better practice to keep chunks short in your document and explain the parts as you go. Part of the beauty of R Markdown documents is that the code/results from a previous chunk is saved and can be accessed/used later in the document. Again, they’re great at allowing you to be a literate programmer, explaining code and results as you go. In addition to having code chunks like the one illustrated above, you can also have code run directly in your document. Instead of having to calculate a value or type out the full value of a variable, you can use an in-line code chunk to do the dirty work for you. It will just display that value in your document as if it were the values you typed yourself! Just type `r `, but before that closing backtick, just put the code you’d like to run (example: mean(x)). Another quick note about using backticks: if you want to just have text in monospace font within a paragraph (like we’ve done to show the difference between code and regular commentary), you can just enclose it in a set of backticks. As long as you don’t start it with `r` followed by a space, it will just appear as plain text. One last thing: in a script, you’ve probably been using the View() command to look at the data. DO NOT DO THIS IN MARKDOWN. Instead, use the kable() function from the knitr package. Everything formats nicely with it. To make one, call knitr::kable(DATA FRAME GOES HERE) in a code chunk, and set echo = FALSE in that chunk’s header. While hiding the code is arbitrary, we don’t necessarily need to see every line of code you wrote. We just need to see how you got to your final answer. Being able to see the raw data most likely didn’t help you like a for loop did, so we’d rather see the for loop and save the space on the document. Again, markdown allows you to be a literate programmer. This means that you get to decide what the important parts of your code are, and what code the reader sees. R Markdown will still run all of the code you wrote in the background, but adding lines of output or extra code actually weaken the strength of your document (and cause it to take longer to render). 19.5 \\(\\LaTeX\\) and Equation Formatting The final stylistic element of an R Markdown document is being able to quickly and easily include \\(\\LaTeX\\) (pronounced as LAY-tek) formatting. What is \\(\\LaTeX\\) formatting? Glad you asked! Essentially \\(\\LaTeX\\) formatting allows us to include equations, other special characters (i.e. characters from other languages), mathematical expressions, constants, and other operations easily into our document. To format something with \\(\\LaTeX\\) in the line you’re working on, just enclose it in a set of $s. For example, if you wanted to make the letter \\(\\pi\\) appear, you’d use the \\(\\LaTeX\\) command $\\pi$. Note the \\: this tells \\(\\LaTeX\\) that the following part (the pi) is actually a code word for something that \\(\\LaTeX\\) understands. Some other common ones you may see are as follows: {} essentially means “Whatever’s enclosed in here, treat together and do the \\(\\LaTeX\\) command to it” \\sqrt{} puts everything inside of the curly braces under a radical. $\\sqrt{4}$ will appear as \\(\\sqrt{4}\\) \\frac{}{} nicely formats fractions. The numerator goes in the first set of braces, and the denominator goes in the second set. If you wanted to express 0.5 as a fraction, you could write in in \\(\\LaTeX\\) as $\\frac{1}{2}$ \\cdot puts a multiplication operator (\\(\\cdot\\)) into your equation \\text{} reformats anything inside of the braces to appear as plain text. For example, using the \\(\\LaTeX\\) command $Z-score = \\frac{Val - Avg}{SD}$ will appear as \\(Z-score = \\frac{Val - Avg}{SD}\\), however enclosing the phrases in the equation in the \\text{} command (i.e. using $\\text{Z-score} = \\frac{\\text{Val} - \\text{Avg}}{\\text{SD}}$) we get \\(\\text{Z-score} = \\frac{\\text{Val} - \\text{Avg}}{\\text{SD}}\\), which is how we’re used to seeing it. ^ and _ are superscript and subscript respectively. Without using {}, these will only take the first character that follows them and super/subscript it. Remember: {} means “treat as a group” If instead of enclosing the \\(\\LaTeX\\) commands in one $, and instead they were enclosed by $$, the \\(\\LaTeX\\) would still work fine, but would actually format to be centered and on it’s own line. 19.6 HTML Tags and CSS If you’re familiar with HTML and/or CSS, you can use these to customize your R Markdown documents. We won’t go over how to do this, but you can feel free to look it up and use it as you want to. We’ve used both to help style the textbook, so if you see something in a .Rmd file that looks like &lt;a name/href = 'something in here'&gt;&lt;/a&gt; or &lt;span class = 'class name here'&gt;&lt;/span&gt;, that’s what these are. 19.7 Cheat Sheets This chapter contained a TON of information. Feel free to refer back to it as you need to, but (as always in R) there’s a good resource or two to use. Specifically for R Markdown, there are cheat sheets that have pretty much everything that we’ve covered in this chapter, plus a few extra bits, for you to refer back to. You can find it here. It’s a really good idea to get comfortable in markdown, since we’ll be doing more and more with it now that you’ve learned about it. "],
["probability-and-chance.html", "Chapter 20 Probability And Chance 20.1 Probability 20.2 Replacement 20.3 The Multiplication Rule 20.4 The Addition Rule 20.5 All and None 20.6 Box Models 20.7 sample()", " Chapter 20 Probability And Chance In the last few chapters, we’ve been talking about how to make predictions using features in data, such as predicting weight from height, GPA from hours studied, or how many pairs of shoes you own based on how big your foot is. Now, let’s change what we’re predicting. Rather than predicting what a particular value is going to be, let’s talk about chance. No, not that Chance… We meant chance as in probability, or how likely we are to observe a particular number in our data. But maybe Chance the Rapper can still help us out. Whaddaya say, Chance? 20.1 Probability We’ll define probability according to this definition: \\[\\text{Probability} = \\frac{\\text{Outcomes we want to happen}}{\\text{All possible outcomes that could happen}}\\] One of the easiest probability scenarios to think about is a rolling a die. Let’s pretend we’re playing a game against each other with a fair, six-sided die. If we roll an odd number, we win the game. If we roll an even number, Chance wins the game. What’s the probability, or what’s the chance, that Chance wins the game? Start by listing out all of the possible outcomes of our roll: 1, 2, 3, 4, 5, or a 6. Of those possible outcomes, if the die comes up a 2, a 4, or a 6, Chance wins. So that gives 3 outcomes out of 6 total outcomes. We write this as \\(P(\\text{You win}) = \\frac{3}{6} = \\frac{1}{2}\\). Simple stuff. Let’s move on to something a little more challenging: casino games. We’ll start with a standard deck of 52 playing cards. For the unfamiliar, a deck of cards has 4 different suits, or pictures, and each suit is made up of 13 unique cards. They look like this: We’ll represent them in the following table to make them easier to visualize. Clubs Spades Hearts Diamonds A A A A 2 2 2 2 3 3 3 3 4 4 4 4 5 5 5 5 6 6 6 6 7 7 7 7 8 8 8 8 9 9 9 9 10 10 10 10 J J J J Q Q Q Q K K K K Awesome, now let’s start having some fun with them. We’ll start with a simple example. Let’s pretend that we want to know the probability of drawing randomly from our deck of cards and getting a card that’s 9 or higher (we’ll say the ace is the lowest card). No problem, we just count up how many cards are a 9 or higher. Clubs Spades Hearts Diamonds A A A A 2 2 2 2 3 3 3 3 4 4 4 4 5 5 5 5 6 6 6 6 7 7 7 7 8 8 8 8 9 9 9 9 10 10 10 10 J J J J Q Q Q Q K K K K If you count up how many numbers we highlighted, you’ll see that there’s 20 possible cards that are 9 or higher. This means that the probability of getting a card of 9 or higher is \\(P(\\text{9 or higher}) = \\frac{20}{52}\\). So far, so good. 20.2 Replacement So far, we’ve only played our game or repeated our experiment one time and found the probability. But what about if we want to get the probability of a series of events, or repetitions? No problem! We’ll cover that in just a minute, but before we do we need to talk about replacement. What we mean by replacement is what we do after each time we play the game. Do we put the card back in the deck? Is it possible to get the exact same outcome? If yes, we’re doing our experiment with replacement. Otherwise, we say that we’re doing it without replacement. When we do something with replacement, the total number of possible outcomes doesn’t change. When we do the same thing without replacement, the number of possible outcomes decreases by one (since we made one of those outcomes happen). 20.3 The Multiplication Rule Now that we understand some basics about probability, we can start to talk about probability over multiple , or repetitions of our experiment/plays of our game. The idea here is that we need things to happen together, like drawing twice from our deck and getting a King and then getting a Queen. To do this, we find the probability of each and use the multiplication rule, where we multiply the probability of each event together. The key word that should trigger us to use it is and. If we wanted to play our die game twice and know what the chance is we win each time. Because of the multiplcation rule, this is easy! \\[P(\\text{Win first time AND win second time}) = P(\\text{Win first time}) \\cdot P(\\text{Win second time}) = \\frac{1}{2} \\cdot \\frac{1}{2} = \\frac{1}{4}\\] Notice that the game is played with replacement, since we can’t get rid of a number on a die after it turns up on a roll. If instead we wanted to draw two cards from a deck with replacement and get the probability we get a number bigger than 9 on our first draw and a Spade on our second draw, we could calculate it as follows: \\[P(\\text{Bigger than 9 AND Spade}) = P(\\text{Bigger than 9}) \\cdot P(\\text{Spade}) = \\frac{20}{52} \\cdot \\frac{13}{52}\\] 20.4 The Addition Rule The other rule to remember is the addition rule, which we use when we want the probability of event A OR event B, but not both. We define the addition rule as \\[P(\\text{Event A OR Event B}) = P(\\text{Event A}) + P(\\text{Event B}) - P(\\text{Event A AND Event B})\\] That last term may seem confusing, but it should make sense with an example. Let’s say we want the probability we draw one card out of our deck, and we want it to either be a King or a Spade. Here’s all of the cards that meet the criteria: Clubs Spades Hearts Diamonds A A A A 2 2 2 2 3 3 3 3 4 4 4 4 5 5 5 5 6 6 6 6 7 7 7 7 8 8 8 8 9 9 9 9 10 10 10 10 J J J J Q Q Q Q K K K K We can see that there’s 16 unique cards that are highlighted: 3 unique Kings, 12 unique Spades, and 1 King of Spades. This means that the probability of getting a King or a Spade is \\(\\frac{16}{52}\\). But let’s see the addition rule in action! First, we know that the probability of a King is given by \\(P(\\text{King}) = \\frac{4}{52}\\), and the probability of a spade is \\(P(\\text{Spade}) = \\frac{13}{52}\\). Lastly, we know that there’s one card that’s both a King and a Spade: the King of Spades, which we can draw with probability \\(P(\\text{King AND Spade}) = \\frac{1}{52}\\). All together, we have: \\[P(\\text{King OR Spade}) = P(\\text{King}) + P(\\text{Spade}) - P(\\text{King AND Spade}) = \\frac{4}{52} + \\frac{13}{52} - \\frac{1}{52} = \\frac{16}{52}\\] 20.5 All and None One point we haven’t explicitly made yet about probability is that the total probability, or the probability of events we want to have happen plus the probability of all the other possible events, needs to add up to 1, or 100%. This usually doesn’t matter, but it’s a really important fact that we’ll take advantage of when we want to calculate the probability that at least one event is what we want, or the probability that not all events are what we want. The rules are as follows: \\[P(\\text{All}) + P(\\text{Not all}) = 1\\] \\[P(\\text{None}) + P(\\text{At least one}) = 1\\] It’s easier to find the probability that all events or no events meet our criteria than it is to calculate the probability that not all or at least one does. As an example, if we roll a die three times we can easily calculate the probability that all three rolls are a 2, since there’s only one scenario in which all three rolls are a 2. If we wanted to calculate the probability that not all three rolls are 2s, we’d have to find the probability that 0 rolls, 1 roll, or 2 rolls were a 2 and combine them. 20.6 Box Models Another way (and a better way in terms of R) to conceptulaize probability is in the form of a box model. What’s a box model, you ask? It’s a way to represent one play of a game, or trial of an experiment, or roll of a die as if we were randomly picking a number written on a ticket out of a hat. Let’s switch to roulette. Roulette is played by spinning a wheel with 38 spaces, dropping a ball into the wheel, and finding out where the ball lands. There are 18 red spaces, 18 black spaces, and 2 green spaces. What’s the chance of the roulette ball landing on a red space? Let’s use a box model to represent a single play of the game. There’s 18 ways for the ball to land on a red space, and there’s 20 ways for it to land on a black or green space. The box model looks like this. Now it’s easy to find the probability of the ball landing on a red space! There’s 18 tickets that are red, and there’s 38 tickets overall. Any time we play the game, we’d have to replace the ticket in the box (see why it’s called “with/without replacement” now?), and it’s easy to conceptualize the scenario graphically. \\(P(\\text{Red}) = \\frac{18}{38}\\). Want the probability of landing on red twice in a row? Easy! Pick a red on your first draw, replace the ticket, and then pick a red on your next draw. Use the multiplication rule to combine them together. \\(P(\\text{Red AND Red}) = \\frac{18}{38} \\cdot \\frac{18}{38}\\). Your thoughts, Chance the Rapper? 20.7 sample() Now, why is this a better way to think about probability for R? It’s because of the way that R can randomly sample things for you. To do this, you want to use the sample() function. The first argument you should pass to it (it’s called x in the help file) should be the vector of things you’d like to sample, whether that’s numbers, colors, TRUE/FALSE values, or whatever else it may be. The second argument, n, is the number of items to pull out, or the number of times to draw out of the box. replace is the third argument, which can be either TRUE or FALSE, indicating whether or not to do the draws with replacement. Lastly, you can change how likely each item you passed to x is via the prob argument, but usually you’ll want to leave this argument alone. It defaults to have each item be equally likely. A few examples before we move on to sampling. set.seed(42) # Set seed for reproducibility # Rolling a die 3 times sample(1:6, 3, replace = TRUE) ## [1] 6 6 2 # Draw 5 cards without replacement # 1-13 is A-K of Clubs # 14-26 is A-K of Spades # 27-39 is A-K of Hearts # 40-52 is A-K of Diamonds sample(1:52, 5, replace = FALSE) ## [1] 44 33 26 37 7 # Help me decide what to eat for dinner sample(c(&#39;Hot Dog&#39;, &#39;Hamburger&#39;, &#39;Quesadilla&#39;, &#39;Ribs&#39;, &#39;Soup&#39;), 1) ## [1] &quot;Ribs&quot; "]
]
